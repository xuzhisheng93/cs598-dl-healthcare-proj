{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcn.inits import *\n",
    "from gcn.utils import *\n",
    "import tensorflow as tf\n",
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()\n",
    "# flags = tf.compat.v1.flags\n",
    "# FLAGS = flags.FLAGS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Layers Class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global unique layer ID dictionary for layer name assignment\n",
    "_LAYER_UIDS = {}\n",
    "\n",
    "\n",
    "def get_layer_uid(layer_name=''):\n",
    "    \"\"\"Helper function, assigns unique layer IDs.\"\"\"\n",
    "    if layer_name not in _LAYER_UIDS:\n",
    "        _LAYER_UIDS[layer_name] = 1\n",
    "        return 1\n",
    "    else:\n",
    "        _LAYER_UIDS[layer_name] += 1\n",
    "        return _LAYER_UIDS[layer_name]\n",
    "\n",
    "\n",
    "def sparse_dropout(x, keep_prob, noise_shape):\n",
    "    \"\"\"Dropout for sparse tensors.\"\"\"\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += tf.random_uniform(noise_shape)\n",
    "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
    "    pre_out = tf.sparse_retain(x, dropout_mask)\n",
    "    return pre_out * (1./keep_prob)\n",
    "\n",
    "\n",
    "def dot(x, y, sparse=False):\n",
    "    \"\"\"Wrapper for tf.matmul (sparse vs dense).\"\"\"\n",
    "    if sparse:\n",
    "        res = tf.sparse_tensor_dense_matmul(x, y)\n",
    "    else:\n",
    "        res = tf.matmul(x, y)\n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Layer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    \"\"\"Base layer class. Defines basic API for all layer objects.\n",
    "    Implementation inspired by keras (http://keras.io).\n",
    "\n",
    "    # Properties\n",
    "        name: String, defines the variable scope of the layer.\n",
    "        logging: Boolean, switches Tensorflow histogram logging on/off\n",
    "\n",
    "    # Methods\n",
    "        _call(inputs): Defines computation graph of layer\n",
    "            (i.e. takes input, returns output)\n",
    "        __call__(inputs): Wrapper for _call()\n",
    "        _log_vars(): Log all variables\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        allowed_kwargs = {'name', 'logging'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            layer = self.__class__.__name__.lower()\n",
    "            name = layer + '_' + str(get_layer_uid(layer))\n",
    "        self.name = name\n",
    "        self.vars = {}\n",
    "        logging = kwargs.get('logging', False)\n",
    "        self.logging = logging\n",
    "        self.sparse_inputs = False\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        return inputs\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.name_scope(self.name):\n",
    "            if self.logging and not self.sparse_inputs:\n",
    "                tf.summary.histogram(self.name + '/inputs', inputs)\n",
    "            outputs = self._call(inputs)\n",
    "            if self.logging:\n",
    "                tf.summary.histogram(self.name + '/outputs', outputs)\n",
    "            return outputs\n",
    "\n",
    "    def _log_vars(self):\n",
    "        for var in self.vars:\n",
    "            tf.summary.histogram(self.name + '/vars/' + var, self.vars[var])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    \"\"\"Dense layer.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, placeholders, dropout=0., sparse_inputs=False,\n",
    "                 act=tf.nn.relu, bias=False, featureless=False, **kwargs):\n",
    "        super(Dense, self).__init__(**kwargs)\n",
    "\n",
    "        if dropout:\n",
    "            self.dropout = placeholders['dropout']\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "\n",
    "        self.act = act\n",
    "        self.sparse_inputs = sparse_inputs\n",
    "        self.featureless = featureless\n",
    "        self.bias = bias\n",
    "\n",
    "        # helper variable for sparse dropout\n",
    "        self.num_features_nonzero = placeholders['num_features_nonzero']\n",
    "\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            self.vars['weights'] = glorot([input_dim, output_dim],\n",
    "                                          name='weights')\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([output_dim], name='bias')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        # dropout\n",
    "        if self.sparse_inputs:\n",
    "            x = sparse_dropout(x, 1-self.dropout, self.num_features_nonzero)\n",
    "        else:\n",
    "            x = tf.nn.dropout(x, 1-self.dropout)\n",
    "\n",
    "        # transform\n",
    "        output = dot(x, self.vars['weights'], sparse=self.sparse_inputs)\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "\n",
    "        return self.act(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Graph Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(Layer):\n",
    "    \"\"\"Graph convolution layer.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, placeholders, dropout=0.,\n",
    "                 sparse_inputs=False, act=tf.nn.relu, bias=False,\n",
    "                 featureless=False, **kwargs):\n",
    "        super(GraphConvolution, self).__init__(**kwargs)\n",
    "\n",
    "        if dropout:\n",
    "            self.dropout = placeholders['dropout']\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "\n",
    "        self.act = act\n",
    "        self.support = placeholders['support']\n",
    "        self.sparse_inputs = sparse_inputs\n",
    "        self.featureless = featureless\n",
    "        self.bias = bias\n",
    "\n",
    "        # helper variable for sparse dropout\n",
    "        self.num_features_nonzero = placeholders['num_features_nonzero']\n",
    "\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            for i in range(len(self.support)):\n",
    "                self.vars['weights_' + str(i)] = glorot([input_dim, output_dim],\n",
    "                                                        name='weights_' + str(i))\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([output_dim], name='bias')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        # dropout\n",
    "        if self.sparse_inputs:\n",
    "            x = sparse_dropout(x, 1-self.dropout, self.num_features_nonzero)\n",
    "        else:\n",
    "            x = tf.nn.dropout(x, 1-self.dropout)\n",
    "\n",
    "        # convolve\n",
    "        supports = list()\n",
    "        for i in range(len(self.support)):\n",
    "            if not self.featureless:\n",
    "                pre_sup = dot(x, self.vars['weights_' + str(i)],\n",
    "                              sparse=self.sparse_inputs)\n",
    "            else:\n",
    "                pre_sup = self.vars['weights_' + str(i)]\n",
    "            support = dot(self.support[i], pre_sup, sparse=True)\n",
    "            supports.append(support)\n",
    "        output = tf.add_n(supports)\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "\n",
    "        return self.act(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcn.metrics import *\n",
    "\n",
    "# flags = tf.compat.v1.flags\n",
    "# FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        allowed_kwargs = {'name', 'logging'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            name = self.__class__.__name__.lower()\n",
    "        self.name = name\n",
    "\n",
    "        logging = kwargs.get('logging', False)\n",
    "        self.logging = logging\n",
    "\n",
    "        self.vars = {}\n",
    "        self.placeholders = {}\n",
    "\n",
    "        self.layers = []\n",
    "        self.activations = []\n",
    "\n",
    "        self.inputs = None\n",
    "        self.outputs = None\n",
    "\n",
    "        self.loss = 0\n",
    "        self.accuracy = 0\n",
    "        self.optimizer = None\n",
    "        self.opt_op = None\n",
    "\n",
    "    def _build(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\" Wrapper for _build() \"\"\"\n",
    "        with tf.variable_scope(self.name):\n",
    "            self._build()\n",
    "\n",
    "        # Build sequential layer model\n",
    "        self.activations.append(self.inputs)\n",
    "        for layer in self.layers:\n",
    "            hidden = layer(self.activations[-1])\n",
    "            self.activations.append(hidden)\n",
    "        self.outputs = self.activations[-1]\n",
    "\n",
    "        # Store model variables for easy access\n",
    "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
    "        self.vars = {var.name: var for var in variables}\n",
    "\n",
    "        # Build metrics\n",
    "        self._loss()\n",
    "        self._accuracy()\n",
    "\n",
    "        self.opt_op = self.optimizer.minimize(self.loss)\n",
    "\n",
    "    def predict(self):\n",
    "        pass\n",
    "\n",
    "    def _loss(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _accuracy(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def save(self, sess=None):\n",
    "        if not sess:\n",
    "            raise AttributeError(\"TensorFlow session not provided.\")\n",
    "        saver = tf.train.Saver(self.vars)\n",
    "        save_path = saver.save(sess, \"tmp/%s.ckpt\" % self.name)\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "    def load(self, sess=None):\n",
    "        if not sess:\n",
    "            raise AttributeError(\"TensorFlow session not provided.\")\n",
    "        saver = tf.train.Saver(self.vars)\n",
    "        save_path = \"tmp/%s.ckpt\" % self.name\n",
    "        saver.restore(sess, save_path)\n",
    "        print(\"Model restored from file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(Model):\n",
    "    def __init__(self, placeholders, input_dim, **kwargs):\n",
    "        super(MLP, self).__init__(**kwargs)\n",
    "\n",
    "        self.inputs = placeholders['features']\n",
    "        self.input_dim = input_dim\n",
    "        # self.input_dim = self.inputs.get_shape().as_list()[1]  # To be supported in future Tensorflow versions\n",
    "        self.output_dim = placeholders['labels'].get_shape().as_list()[1]\n",
    "        self.placeholders = placeholders\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=flags_learning_rate)\n",
    "\n",
    "        self.build()\n",
    "\n",
    "    def _loss(self):\n",
    "        # Weight decay loss\n",
    "        for var in self.layers[0].vars.values():\n",
    "            self.loss += flags_weight_decay * tf.nn.l2_loss(var)\n",
    "\n",
    "        # Cross entropy error\n",
    "        self.loss += masked_softmax_cross_entropy(self.outputs, self.placeholders['labels'],\n",
    "                                                  self.placeholders['labels_mask'])\n",
    "\n",
    "    def _accuracy(self):\n",
    "        self.accuracy = masked_accuracy(self.outputs, self.placeholders['labels'],\n",
    "                                        self.placeholders['labels_mask'])\n",
    "\n",
    "    def _build(self):\n",
    "        self.layers.append(Dense(input_dim=self.input_dim,\n",
    "                                 output_dim=flags_hidden1,\n",
    "                                 placeholders=self.placeholders,\n",
    "                                 act=tf.nn.relu,\n",
    "                                 dropout=True,\n",
    "                                 sparse_inputs=True,\n",
    "                                 logging=self.logging))\n",
    "\n",
    "        self.layers.append(Dense(input_dim=flags_hidden1,\n",
    "                                 output_dim=self.output_dim,\n",
    "                                 placeholders=self.placeholders,\n",
    "                                 act=lambda x: x,\n",
    "                                 dropout=True,\n",
    "                                 logging=self.logging))\n",
    "\n",
    "    def predict(self):\n",
    "        return tf.nn.softmax(self.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(Model):\n",
    "    def __init__(self, placeholders, input_dim, **kwargs):\n",
    "        super(GCN, self).__init__(**kwargs)\n",
    "\n",
    "        self.inputs = placeholders['features']\n",
    "        self.input_dim = input_dim\n",
    "        # self.input_dim = self.inputs.get_shape().as_list()[1]  # To be supported in future Tensorflow versions\n",
    "        self.output_dim = placeholders['labels'].get_shape().as_list()[1]\n",
    "        self.placeholders = placeholders\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=flags_learning_rate)\n",
    "\n",
    "        self.build()\n",
    "\n",
    "    def _loss(self):\n",
    "        # Weight decay loss\n",
    "        for var in self.layers[0].vars.values():\n",
    "            self.loss += flags_weight_decay * tf.nn.l2_loss(var)\n",
    "\n",
    "        # Cross entropy error\n",
    "        self.loss += masked_softmax_cross_entropy(self.outputs, self.placeholders['labels'],\n",
    "                                                  self.placeholders['labels_mask'])\n",
    "\n",
    "    def _accuracy(self):\n",
    "        self.accuracy = masked_accuracy(self.outputs, self.placeholders['labels'],\n",
    "                                        self.placeholders['labels_mask'])\n",
    "\n",
    "    def _build(self):\n",
    "\n",
    "        self.layers.append(GraphConvolution(input_dim=self.input_dim,\n",
    "                                            output_dim=flags_hidden1,\n",
    "                                            placeholders=self.placeholders,\n",
    "                                            act=tf.nn.relu,\n",
    "                                            dropout=True,\n",
    "                                            sparse_inputs=True,\n",
    "                                            logging=self.logging))\n",
    "\n",
    "        self.layers.append(GraphConvolution(input_dim=flags_hidden1,\n",
    "                                            output_dim=self.output_dim,\n",
    "                                            placeholders=self.placeholders,\n",
    "                                            act=lambda x: x,\n",
    "                                            dropout=True,\n",
    "                                            logging=self.logging))\n",
    "\n",
    "    def predict(self):\n",
    "        return tf.nn.softmax(self.outputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "# import tensorflow as tf\n",
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "# flags = tf.compat.v1.flags\n",
    "# FLAGS = flags.FLAGS\n",
    "flags_dataset = 'cora'# , 'Dataset string.')  # 'cora', 'citeseer', 'pubmed'\n",
    "flags_model = 'gcn' #, 'Model string.')  # 'gcn', 'gcn_cheby', 'dense'\n",
    "flags_learning_rate = 0.01 #, 'Initial learning rate.')\n",
    "flags_epochs = 200 #, 'Number of epochs to train.')\n",
    "flags_hidden1 = 16 #, 'Number of units in hidden layer 1.')\n",
    "flags_dropout = 0.5 #, 'Dropout rate (1 - keep probability).')\n",
    "flags_weight_decay = 5e-4 #, 'Weight for L2 loss on embedding matrix.')\n",
    "flags_early_stopping = 10 #, 'Tolerance for early stopping (# of epochs).')\n",
    "flags_max_degree = 3 #, 'Maximum Chebyshev polynomial degree.')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Load Data and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\xuzhi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From c:\\Users\\xuzhi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\uiuc_mcs\\cs598-dl-healthcare-proj\\src\\gcn\\utils.py:70: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "Epoch: 0001 train_loss= 1.95398 train_acc= 0.14286 val_loss= 1.94916 val_acc= 0.19200 time= 3.33444\n",
      "Epoch: 0002 train_loss= 1.94717 train_acc= 0.24286 val_loss= 1.94463 val_acc= 0.25400 time= 0.01202\n",
      "Epoch: 0003 train_loss= 1.93999 train_acc= 0.40000 val_loss= 1.93989 val_acc= 0.36000 time= 0.01203\n",
      "Epoch: 0004 train_loss= 1.92993 train_acc= 0.60714 val_loss= 1.93522 val_acc= 0.38400 time= 0.01507\n",
      "Epoch: 0005 train_loss= 1.92239 train_acc= 0.62143 val_loss= 1.93089 val_acc= 0.38400 time= 0.01603\n",
      "Epoch: 0006 train_loss= 1.91587 train_acc= 0.62143 val_loss= 1.92688 val_acc= 0.38800 time= 0.01759\n",
      "Epoch: 0007 train_loss= 1.90422 train_acc= 0.65714 val_loss= 1.92294 val_acc= 0.38600 time= 0.01346\n",
      "Epoch: 0008 train_loss= 1.90035 train_acc= 0.60714 val_loss= 1.91907 val_acc= 0.37200 time= 0.01701\n",
      "Epoch: 0009 train_loss= 1.88918 train_acc= 0.61429 val_loss= 1.91512 val_acc= 0.37200 time= 0.01653\n",
      "Epoch: 0010 train_loss= 1.88187 train_acc= 0.62143 val_loss= 1.91109 val_acc= 0.36800 time= 0.02273\n",
      "Epoch: 0011 train_loss= 1.87274 train_acc= 0.64286 val_loss= 1.90697 val_acc= 0.36800 time= 0.00983\n",
      "Epoch: 0012 train_loss= 1.86119 train_acc= 0.64286 val_loss= 1.90275 val_acc= 0.37000 time= 0.01890\n",
      "Epoch: 0013 train_loss= 1.85539 train_acc= 0.61429 val_loss= 1.89846 val_acc= 0.36400 time= 0.01451\n",
      "Epoch: 0014 train_loss= 1.84110 train_acc= 0.72857 val_loss= 1.89412 val_acc= 0.36200 time= 0.01553\n",
      "Epoch: 0015 train_loss= 1.83454 train_acc= 0.62143 val_loss= 1.88983 val_acc= 0.37800 time= 0.01105\n",
      "Epoch: 0016 train_loss= 1.82084 train_acc= 0.70714 val_loss= 1.88550 val_acc= 0.39000 time= 0.01614\n",
      "Epoch: 0017 train_loss= 1.81845 train_acc= 0.64286 val_loss= 1.88113 val_acc= 0.40000 time= 0.01552\n",
      "Epoch: 0018 train_loss= 1.80250 train_acc= 0.68571 val_loss= 1.87667 val_acc= 0.40200 time= 0.01480\n",
      "Epoch: 0019 train_loss= 1.78322 train_acc= 0.72143 val_loss= 1.87211 val_acc= 0.40600 time= 0.01251\n",
      "Epoch: 0020 train_loss= 1.77088 train_acc= 0.70714 val_loss= 1.86746 val_acc= 0.40800 time= 0.01201\n",
      "Epoch: 0021 train_loss= 1.75952 train_acc= 0.74286 val_loss= 1.86281 val_acc= 0.41200 time= 0.01524\n",
      "Epoch: 0022 train_loss= 1.74195 train_acc= 0.68571 val_loss= 1.85822 val_acc= 0.41600 time= 0.01152\n",
      "Epoch: 0023 train_loss= 1.74272 train_acc= 0.68571 val_loss= 1.85359 val_acc= 0.42400 time= 0.01109\n",
      "Epoch: 0024 train_loss= 1.73829 train_acc= 0.68571 val_loss= 1.84884 val_acc= 0.43000 time= 0.01147\n",
      "Epoch: 0025 train_loss= 1.72337 train_acc= 0.70000 val_loss= 1.84389 val_acc= 0.43400 time= 0.01251\n",
      "Epoch: 0026 train_loss= 1.72589 train_acc= 0.69286 val_loss= 1.83870 val_acc= 0.44600 time= 0.01202\n",
      "Epoch: 0027 train_loss= 1.68547 train_acc= 0.77143 val_loss= 1.83333 val_acc= 0.45000 time= 0.01703\n",
      "Epoch: 0028 train_loss= 1.68720 train_acc= 0.68571 val_loss= 1.82795 val_acc= 0.46200 time= 0.01201\n",
      "Epoch: 0029 train_loss= 1.66978 train_acc= 0.75000 val_loss= 1.82236 val_acc= 0.46400 time= 0.01452\n",
      "Epoch: 0030 train_loss= 1.63514 train_acc= 0.77143 val_loss= 1.81673 val_acc= 0.47000 time= 0.01151\n",
      "Epoch: 0031 train_loss= 1.64691 train_acc= 0.76429 val_loss= 1.81108 val_acc= 0.48400 time= 0.01606\n",
      "Epoch: 0032 train_loss= 1.63115 train_acc= 0.73571 val_loss= 1.80524 val_acc= 0.49600 time= 0.01152\n",
      "Epoch: 0033 train_loss= 1.61448 train_acc= 0.76429 val_loss= 1.79925 val_acc= 0.50800 time= 0.01451\n",
      "Epoch: 0034 train_loss= 1.60044 train_acc= 0.82143 val_loss= 1.79301 val_acc= 0.51400 time= 0.01453\n",
      "Epoch: 0035 train_loss= 1.57957 train_acc= 0.80000 val_loss= 1.78670 val_acc= 0.52400 time= 0.01301\n",
      "Epoch: 0036 train_loss= 1.58840 train_acc= 0.78571 val_loss= 1.78011 val_acc= 0.53400 time= 0.01252\n",
      "Epoch: 0037 train_loss= 1.54711 train_acc= 0.80714 val_loss= 1.77346 val_acc= 0.54400 time= 0.00801\n",
      "Epoch: 0038 train_loss= 1.55767 train_acc= 0.81429 val_loss= 1.76669 val_acc= 0.55600 time= 0.01052\n",
      "Epoch: 0039 train_loss= 1.51675 train_acc= 0.82857 val_loss= 1.76001 val_acc= 0.56800 time= 0.01301\n",
      "Epoch: 0040 train_loss= 1.52725 train_acc= 0.75714 val_loss= 1.75310 val_acc= 0.58400 time= 0.01307\n",
      "Epoch: 0041 train_loss= 1.48727 train_acc= 0.87857 val_loss= 1.74613 val_acc= 0.59000 time= 0.00900\n",
      "Epoch: 0042 train_loss= 1.50068 train_acc= 0.83571 val_loss= 1.73895 val_acc= 0.59800 time= 0.01408\n",
      "Epoch: 0043 train_loss= 1.44732 train_acc= 0.83571 val_loss= 1.73151 val_acc= 0.61600 time= 0.01002\n",
      "Epoch: 0044 train_loss= 1.43206 train_acc= 0.84286 val_loss= 1.72376 val_acc= 0.64400 time= 0.01151\n",
      "Epoch: 0045 train_loss= 1.46252 train_acc= 0.85714 val_loss= 1.71587 val_acc= 0.65600 time= 0.01152\n",
      "Epoch: 0046 train_loss= 1.44156 train_acc= 0.85000 val_loss= 1.70772 val_acc= 0.67000 time= 0.01251\n",
      "Epoch: 0047 train_loss= 1.40873 train_acc= 0.86429 val_loss= 1.69970 val_acc= 0.68000 time= 0.01123\n",
      "Epoch: 0048 train_loss= 1.39571 train_acc= 0.85714 val_loss= 1.69170 val_acc= 0.68200 time= 0.00951\n",
      "Epoch: 0049 train_loss= 1.38351 train_acc= 0.88571 val_loss= 1.68381 val_acc= 0.68800 time= 0.01001\n",
      "Epoch: 0050 train_loss= 1.37834 train_acc= 0.85000 val_loss= 1.67585 val_acc= 0.68800 time= 0.00851\n",
      "Epoch: 0051 train_loss= 1.34691 train_acc= 0.87143 val_loss= 1.66809 val_acc= 0.69600 time= 0.01252\n",
      "Epoch: 0052 train_loss= 1.31214 train_acc= 0.91429 val_loss= 1.66017 val_acc= 0.70400 time= 0.01301\n",
      "Epoch: 0053 train_loss= 1.33207 train_acc= 0.90000 val_loss= 1.65211 val_acc= 0.71400 time= 0.01023\n",
      "Epoch: 0054 train_loss= 1.26994 train_acc= 0.88571 val_loss= 1.64408 val_acc= 0.71600 time= 0.01001\n",
      "Epoch: 0055 train_loss= 1.30617 train_acc= 0.89286 val_loss= 1.63628 val_acc= 0.71800 time= 0.00858\n",
      "Epoch: 0056 train_loss= 1.26755 train_acc= 0.90714 val_loss= 1.62806 val_acc= 0.72200 time= 0.01020\n",
      "Epoch: 0057 train_loss= 1.24267 train_acc= 0.89286 val_loss= 1.61947 val_acc= 0.72400 time= 0.01202\n",
      "Epoch: 0058 train_loss= 1.25288 train_acc= 0.89286 val_loss= 1.61087 val_acc= 0.73000 time= 0.00751\n",
      "Epoch: 0059 train_loss= 1.27307 train_acc= 0.91429 val_loss= 1.60224 val_acc= 0.73800 time= 0.01001\n",
      "Epoch: 0060 train_loss= 1.23682 train_acc= 0.91429 val_loss= 1.59341 val_acc= 0.73600 time= 0.01351\n",
      "Epoch: 0061 train_loss= 1.19454 train_acc= 0.91429 val_loss= 1.58450 val_acc= 0.73600 time= 0.00906\n",
      "Epoch: 0062 train_loss= 1.17077 train_acc= 0.92143 val_loss= 1.57553 val_acc= 0.73800 time= 0.00902\n",
      "Epoch: 0063 train_loss= 1.16224 train_acc= 0.93571 val_loss= 1.56647 val_acc= 0.74200 time= 0.01201\n",
      "Epoch: 0064 train_loss= 1.15123 train_acc= 0.92143 val_loss= 1.55732 val_acc= 0.74800 time= 0.00751\n",
      "Epoch: 0065 train_loss= 1.16154 train_acc= 0.88571 val_loss= 1.54846 val_acc= 0.75200 time= 0.00963\n",
      "Epoch: 0066 train_loss= 1.18635 train_acc= 0.91429 val_loss= 1.53990 val_acc= 0.75400 time= 0.00902\n",
      "Epoch: 0067 train_loss= 1.12458 train_acc= 0.90000 val_loss= 1.53128 val_acc= 0.75600 time= 0.00851\n",
      "Epoch: 0068 train_loss= 1.12599 train_acc= 0.93571 val_loss= 1.52270 val_acc= 0.76000 time= 0.01009\n",
      "Epoch: 0069 train_loss= 1.12808 train_acc= 0.87143 val_loss= 1.51445 val_acc= 0.76000 time= 0.00951\n",
      "Epoch: 0070 train_loss= 1.07801 train_acc= 0.92857 val_loss= 1.50654 val_acc= 0.76000 time= 0.01502\n",
      "Epoch: 0071 train_loss= 1.05325 train_acc= 0.95000 val_loss= 1.49877 val_acc= 0.76200 time= 0.01001\n",
      "Epoch: 0072 train_loss= 1.09814 train_acc= 0.93571 val_loss= 1.49111 val_acc= 0.76400 time= 0.01001\n",
      "Epoch: 0073 train_loss= 1.02336 train_acc= 0.94286 val_loss= 1.48333 val_acc= 0.75800 time= 0.01159\n",
      "Epoch: 0074 train_loss= 1.02378 train_acc= 0.95714 val_loss= 1.47550 val_acc= 0.75800 time= 0.00852\n",
      "Epoch: 0075 train_loss= 1.01293 train_acc= 0.95000 val_loss= 1.46789 val_acc= 0.76000 time= 0.01051\n",
      "Epoch: 0076 train_loss= 1.06558 train_acc= 0.90000 val_loss= 1.46046 val_acc= 0.75800 time= 0.01204\n",
      "Epoch: 0077 train_loss= 1.01223 train_acc= 0.91429 val_loss= 1.45304 val_acc= 0.76000 time= 0.00901\n",
      "Epoch: 0078 train_loss= 1.00245 train_acc= 0.95000 val_loss= 1.44550 val_acc= 0.76200 time= 0.00952\n",
      "Epoch: 0079 train_loss= 1.04556 train_acc= 0.92857 val_loss= 1.43806 val_acc= 0.76200 time= 0.00851\n",
      "Epoch: 0080 train_loss= 1.04428 train_acc= 0.91429 val_loss= 1.43049 val_acc= 0.76200 time= 0.01001\n",
      "Epoch: 0081 train_loss= 0.97610 train_acc= 0.97143 val_loss= 1.42324 val_acc= 0.76400 time= 0.00969\n",
      "Epoch: 0082 train_loss= 0.99653 train_acc= 0.92143 val_loss= 1.41608 val_acc= 0.76600 time= 0.00801\n",
      "Epoch: 0083 train_loss= 0.97819 train_acc= 0.91429 val_loss= 1.40879 val_acc= 0.77200 time= 0.01058\n",
      "Epoch: 0084 train_loss= 0.96874 train_acc= 0.95000 val_loss= 1.40197 val_acc= 0.77600 time= 0.00800\n",
      "Epoch: 0085 train_loss= 0.93856 train_acc= 0.94286 val_loss= 1.39514 val_acc= 0.78200 time= 0.01001\n",
      "Epoch: 0086 train_loss= 0.96155 train_acc= 0.92857 val_loss= 1.38861 val_acc= 0.78200 time= 0.00903\n",
      "Epoch: 0087 train_loss= 0.96552 train_acc= 0.95714 val_loss= 1.38211 val_acc= 0.78200 time= 0.01008\n",
      "Epoch: 0088 train_loss= 0.93760 train_acc= 0.92143 val_loss= 1.37564 val_acc= 0.78000 time= 0.00951\n",
      "Epoch: 0089 train_loss= 0.91651 train_acc= 0.94286 val_loss= 1.36969 val_acc= 0.78200 time= 0.00801\n",
      "Epoch: 0090 train_loss= 0.94046 train_acc= 0.94286 val_loss= 1.36407 val_acc= 0.78200 time= 0.01051\n",
      "Epoch: 0091 train_loss= 0.89514 train_acc= 0.95714 val_loss= 1.35867 val_acc= 0.78200 time= 0.00801\n",
      "Epoch: 0092 train_loss= 0.93475 train_acc= 0.94286 val_loss= 1.35297 val_acc= 0.78400 time= 0.00951\n",
      "Epoch: 0093 train_loss= 0.90957 train_acc= 0.94286 val_loss= 1.34766 val_acc= 0.78200 time= 0.01001\n",
      "Epoch: 0094 train_loss= 0.91873 train_acc= 0.91429 val_loss= 1.34314 val_acc= 0.78200 time= 0.00901\n",
      "Epoch: 0095 train_loss= 0.93027 train_acc= 0.91429 val_loss= 1.33903 val_acc= 0.78200 time= 0.00901\n",
      "Epoch: 0096 train_loss= 0.92149 train_acc= 0.95000 val_loss= 1.33504 val_acc= 0.78400 time= 0.00802\n",
      "Epoch: 0097 train_loss= 0.87579 train_acc= 0.93571 val_loss= 1.33094 val_acc= 0.78400 time= 0.01102\n",
      "Epoch: 0098 train_loss= 0.89953 train_acc= 0.96429 val_loss= 1.32707 val_acc= 0.78600 time= 0.00901\n",
      "Epoch: 0099 train_loss= 0.89165 train_acc= 0.95000 val_loss= 1.32286 val_acc= 0.78600 time= 0.00810\n",
      "Epoch: 0100 train_loss= 0.83945 train_acc= 0.97143 val_loss= 1.31838 val_acc= 0.78400 time= 0.01101\n",
      "Epoch: 0101 train_loss= 0.89122 train_acc= 0.92857 val_loss= 1.31348 val_acc= 0.78000 time= 0.01102\n",
      "Epoch: 0102 train_loss= 0.84614 train_acc= 0.92857 val_loss= 1.30790 val_acc= 0.78200 time= 0.01014\n",
      "Epoch: 0103 train_loss= 0.85110 train_acc= 0.94286 val_loss= 1.30194 val_acc= 0.78200 time= 0.01101\n",
      "Epoch: 0104 train_loss= 0.83774 train_acc= 0.92857 val_loss= 1.29586 val_acc= 0.78600 time= 0.01152\n",
      "Epoch: 0105 train_loss= 0.82232 train_acc= 0.96429 val_loss= 1.28974 val_acc= 0.78400 time= 0.00801\n",
      "Epoch: 0106 train_loss= 0.84253 train_acc= 0.95714 val_loss= 1.28336 val_acc= 0.78600 time= 0.01101\n",
      "Epoch: 0107 train_loss= 0.83388 train_acc= 0.95000 val_loss= 1.27723 val_acc= 0.78600 time= 0.01253\n",
      "Epoch: 0108 train_loss= 0.83775 train_acc= 0.96429 val_loss= 1.27107 val_acc= 0.78600 time= 0.01050\n",
      "Epoch: 0109 train_loss= 0.83913 train_acc= 0.95000 val_loss= 1.26564 val_acc= 0.79000 time= 0.01102\n",
      "Epoch: 0110 train_loss= 0.81962 train_acc= 0.95000 val_loss= 1.26054 val_acc= 0.79000 time= 0.01106\n",
      "Epoch: 0111 train_loss= 0.80223 train_acc= 0.94286 val_loss= 1.25576 val_acc= 0.79000 time= 0.00700\n",
      "Epoch: 0112 train_loss= 0.84440 train_acc= 0.93571 val_loss= 1.25108 val_acc= 0.79000 time= 0.00951\n",
      "Epoch: 0113 train_loss= 0.82258 train_acc= 0.95714 val_loss= 1.24685 val_acc= 0.79000 time= 0.01453\n",
      "Epoch: 0114 train_loss= 0.81502 train_acc= 0.94286 val_loss= 1.24286 val_acc= 0.79000 time= 0.00750\n",
      "Epoch: 0115 train_loss= 0.83105 train_acc= 0.94286 val_loss= 1.23928 val_acc= 0.79000 time= 0.01202\n",
      "Epoch: 0116 train_loss= 0.80485 train_acc= 0.92857 val_loss= 1.23565 val_acc= 0.78800 time= 0.01053\n",
      "Epoch: 0117 train_loss= 0.78618 train_acc= 0.95714 val_loss= 1.23237 val_acc= 0.78600 time= 0.01051\n",
      "Epoch: 0118 train_loss= 0.77834 train_acc= 0.94286 val_loss= 1.22961 val_acc= 0.78600 time= 0.01001\n",
      "Epoch: 0119 train_loss= 0.77073 train_acc= 0.95000 val_loss= 1.22723 val_acc= 0.78800 time= 0.01252\n",
      "Epoch: 0120 train_loss= 0.77640 train_acc= 0.96429 val_loss= 1.22482 val_acc= 0.78800 time= 0.01051\n",
      "Epoch: 0121 train_loss= 0.76526 train_acc= 0.93571 val_loss= 1.22219 val_acc= 0.78800 time= 0.00900\n",
      "Epoch: 0122 train_loss= 0.75811 train_acc= 0.96429 val_loss= 1.21978 val_acc= 0.78800 time= 0.00959\n",
      "Epoch: 0123 train_loss= 0.74993 train_acc= 0.97143 val_loss= 1.21723 val_acc= 0.78800 time= 0.01202\n",
      "Epoch: 0124 train_loss= 0.77457 train_acc= 0.93571 val_loss= 1.21425 val_acc= 0.79000 time= 0.00901\n",
      "Epoch: 0125 train_loss= 0.77622 train_acc= 0.97143 val_loss= 1.21175 val_acc= 0.79000 time= 0.01372\n",
      "Epoch: 0126 train_loss= 0.72540 train_acc= 0.95714 val_loss= 1.20929 val_acc= 0.79200 time= 0.01013\n",
      "Epoch: 0127 train_loss= 0.74452 train_acc= 0.93571 val_loss= 1.20653 val_acc= 0.79200 time= 0.00901\n",
      "Epoch: 0128 train_loss= 0.73369 train_acc= 0.97857 val_loss= 1.20356 val_acc= 0.79200 time= 0.01251\n",
      "Epoch: 0129 train_loss= 0.71548 train_acc= 0.96429 val_loss= 1.20066 val_acc= 0.79200 time= 0.01000\n",
      "Epoch: 0130 train_loss= 0.77534 train_acc= 0.94286 val_loss= 1.19736 val_acc= 0.79200 time= 0.01251\n",
      "Epoch: 0131 train_loss= 0.72123 train_acc= 0.96429 val_loss= 1.19431 val_acc= 0.79200 time= 0.00801\n",
      "Epoch: 0132 train_loss= 0.78297 train_acc= 0.95000 val_loss= 1.19173 val_acc= 0.79200 time= 0.01001\n",
      "Epoch: 0133 train_loss= 0.71784 train_acc= 0.95000 val_loss= 1.18921 val_acc= 0.79000 time= 0.00914\n",
      "Epoch: 0134 train_loss= 0.76040 train_acc= 0.95000 val_loss= 1.18629 val_acc= 0.79000 time= 0.01152\n",
      "Epoch: 0135 train_loss= 0.69706 train_acc= 0.97143 val_loss= 1.18313 val_acc= 0.79000 time= 0.01051\n",
      "Epoch: 0136 train_loss= 0.74126 train_acc= 0.95714 val_loss= 1.18020 val_acc= 0.79000 time= 0.00900\n",
      "Epoch: 0137 train_loss= 0.74691 train_acc= 0.95714 val_loss= 1.17705 val_acc= 0.79000 time= 0.01102\n",
      "Epoch: 0138 train_loss= 0.70790 train_acc= 0.94286 val_loss= 1.17439 val_acc= 0.79000 time= 0.01052\n",
      "Epoch: 0139 train_loss= 0.73140 train_acc= 0.94286 val_loss= 1.17190 val_acc= 0.79000 time= 0.00850\n",
      "Epoch: 0140 train_loss= 0.69936 train_acc= 0.97143 val_loss= 1.16930 val_acc= 0.78800 time= 0.00901\n",
      "Epoch: 0141 train_loss= 0.71074 train_acc= 0.94286 val_loss= 1.16692 val_acc= 0.78800 time= 0.01000\n",
      "Epoch: 0142 train_loss= 0.67118 train_acc= 0.96429 val_loss= 1.16478 val_acc= 0.78800 time= 0.00851\n",
      "Epoch: 0143 train_loss= 0.67726 train_acc= 0.97143 val_loss= 1.16298 val_acc= 0.78800 time= 0.00900\n",
      "Epoch: 0144 train_loss= 0.72861 train_acc= 0.96429 val_loss= 1.16093 val_acc= 0.78800 time= 0.00801\n",
      "Epoch: 0145 train_loss= 0.70413 train_acc= 0.95714 val_loss= 1.15898 val_acc= 0.78400 time= 0.01112\n",
      "Epoch: 0146 train_loss= 0.71026 train_acc= 0.95000 val_loss= 1.15754 val_acc= 0.78400 time= 0.00951\n",
      "Epoch: 0147 train_loss= 0.70073 train_acc= 0.97857 val_loss= 1.15547 val_acc= 0.78600 time= 0.00851\n",
      "Epoch: 0148 train_loss= 0.70645 train_acc= 0.94286 val_loss= 1.15336 val_acc= 0.78600 time= 0.00901\n",
      "Epoch: 0149 train_loss= 0.66660 train_acc= 0.97143 val_loss= 1.15109 val_acc= 0.78600 time= 0.00751\n",
      "Epoch: 0150 train_loss= 0.68064 train_acc= 0.99286 val_loss= 1.14928 val_acc= 0.78600 time= 0.00951\n",
      "Epoch: 0151 train_loss= 0.70278 train_acc= 0.96429 val_loss= 1.14774 val_acc= 0.78600 time= 0.00750\n",
      "Epoch: 0152 train_loss= 0.70102 train_acc= 0.95714 val_loss= 1.14630 val_acc= 0.78600 time= 0.00801\n",
      "Epoch: 0153 train_loss= 0.68891 train_acc= 0.95714 val_loss= 1.14507 val_acc= 0.78600 time= 0.00801\n",
      "Epoch: 0154 train_loss= 0.68355 train_acc= 0.96429 val_loss= 1.14304 val_acc= 0.78600 time= 0.00801\n",
      "Epoch: 0155 train_loss= 0.67378 train_acc= 0.97143 val_loss= 1.14119 val_acc= 0.78600 time= 0.00906\n",
      "Epoch: 0156 train_loss= 0.63491 train_acc= 0.99286 val_loss= 1.13953 val_acc= 0.78800 time= 0.00950\n",
      "Epoch: 0157 train_loss= 0.65653 train_acc= 0.96429 val_loss= 1.13782 val_acc= 0.79000 time= 0.00903\n",
      "Epoch: 0158 train_loss= 0.65647 train_acc= 0.95714 val_loss= 1.13580 val_acc= 0.79200 time= 0.00851\n",
      "Epoch: 0159 train_loss= 0.71242 train_acc= 0.93571 val_loss= 1.13381 val_acc= 0.79200 time= 0.00960\n",
      "Epoch: 0160 train_loss= 0.68016 train_acc= 0.94286 val_loss= 1.13142 val_acc= 0.79200 time= 0.00851\n",
      "Epoch: 0161 train_loss= 0.67917 train_acc= 0.95714 val_loss= 1.12902 val_acc= 0.79200 time= 0.00952\n",
      "Epoch: 0162 train_loss= 0.68680 train_acc= 0.92857 val_loss= 1.12700 val_acc= 0.79000 time= 0.00952\n",
      "Epoch: 0163 train_loss= 0.68635 train_acc= 0.95714 val_loss= 1.12555 val_acc= 0.79000 time= 0.01001\n",
      "Epoch: 0164 train_loss= 0.63512 train_acc= 0.98571 val_loss= 1.12355 val_acc= 0.79000 time= 0.01051\n",
      "Epoch: 0165 train_loss= 0.67691 train_acc= 0.95000 val_loss= 1.12228 val_acc= 0.79000 time= 0.00951\n",
      "Epoch: 0166 train_loss= 0.67557 train_acc= 0.96429 val_loss= 1.12097 val_acc= 0.78400 time= 0.00951\n",
      "Epoch: 0167 train_loss= 0.69946 train_acc= 0.95000 val_loss= 1.11956 val_acc= 0.78400 time= 0.00901\n",
      "Epoch: 0168 train_loss= 0.66730 train_acc= 0.97143 val_loss= 1.11809 val_acc= 0.78400 time= 0.00902\n",
      "Epoch: 0169 train_loss= 0.66973 train_acc= 0.97143 val_loss= 1.11676 val_acc= 0.78400 time= 0.00801\n",
      "Epoch: 0170 train_loss= 0.62247 train_acc= 0.98571 val_loss= 1.11470 val_acc= 0.78400 time= 0.01057\n",
      "Epoch: 0171 train_loss= 0.66033 train_acc= 0.96429 val_loss= 1.11329 val_acc= 0.78400 time= 0.00951\n",
      "Epoch: 0172 train_loss= 0.70384 train_acc= 0.95000 val_loss= 1.11189 val_acc= 0.78400 time= 0.00959\n",
      "Epoch: 0173 train_loss= 0.63977 train_acc= 0.97143 val_loss= 1.10988 val_acc= 0.78400 time= 0.01101\n",
      "Epoch: 0174 train_loss= 0.68713 train_acc= 0.95714 val_loss= 1.10747 val_acc= 0.78800 time= 0.00851\n",
      "Epoch: 0175 train_loss= 0.68592 train_acc= 0.96429 val_loss= 1.10483 val_acc= 0.78800 time= 0.00851\n",
      "Epoch: 0176 train_loss= 0.64400 train_acc= 0.97143 val_loss= 1.10252 val_acc= 0.78800 time= 0.00750\n",
      "Epoch: 0177 train_loss= 0.65551 train_acc= 0.96429 val_loss= 1.10018 val_acc= 0.79200 time= 0.01109\n",
      "Epoch: 0178 train_loss= 0.64678 train_acc= 0.94286 val_loss= 1.09785 val_acc= 0.79200 time= 0.00852\n",
      "Epoch: 0179 train_loss= 0.62044 train_acc= 0.94286 val_loss= 1.09518 val_acc= 0.79200 time= 0.00801\n",
      "Epoch: 0180 train_loss= 0.64565 train_acc= 0.93571 val_loss= 1.09307 val_acc= 0.79200 time= 0.01359\n",
      "Epoch: 0181 train_loss= 0.62064 train_acc= 0.95000 val_loss= 1.09081 val_acc= 0.79200 time= 0.00809\n",
      "Epoch: 0182 train_loss= 0.62457 train_acc= 0.96429 val_loss= 1.08916 val_acc= 0.79000 time= 0.00901\n",
      "Epoch: 0183 train_loss= 0.62333 train_acc= 0.96429 val_loss= 1.08738 val_acc= 0.79000 time= 0.01001\n",
      "Epoch: 0184 train_loss= 0.65683 train_acc= 0.95000 val_loss= 1.08582 val_acc= 0.79000 time= 0.00901\n",
      "Epoch: 0185 train_loss= 0.66519 train_acc= 0.96429 val_loss= 1.08419 val_acc= 0.79000 time= 0.00908\n",
      "Epoch: 0186 train_loss= 0.68031 train_acc= 0.94286 val_loss= 1.08300 val_acc= 0.79000 time= 0.00852\n",
      "Epoch: 0187 train_loss= 0.60282 train_acc= 0.95000 val_loss= 1.08207 val_acc= 0.79000 time= 0.00951\n",
      "Epoch: 0188 train_loss= 0.60228 train_acc= 0.97857 val_loss= 1.08109 val_acc= 0.79000 time= 0.00951\n",
      "Epoch: 0189 train_loss= 0.61390 train_acc= 0.96429 val_loss= 1.07965 val_acc= 0.79000 time= 0.00904\n",
      "Epoch: 0190 train_loss= 0.59585 train_acc= 0.96429 val_loss= 1.07792 val_acc= 0.79000 time= 0.00851\n",
      "Epoch: 0191 train_loss= 0.60101 train_acc= 0.97857 val_loss= 1.07603 val_acc= 0.79200 time= 0.01036\n",
      "Epoch: 0192 train_loss= 0.61071 train_acc= 0.90714 val_loss= 1.07488 val_acc= 0.79400 time= 0.00952\n",
      "Epoch: 0193 train_loss= 0.61192 train_acc= 0.95714 val_loss= 1.07389 val_acc= 0.79600 time= 0.00956\n",
      "Epoch: 0194 train_loss= 0.63344 train_acc= 0.95714 val_loss= 1.07297 val_acc= 0.79600 time= 0.01001\n",
      "Epoch: 0195 train_loss= 0.67860 train_acc= 0.92857 val_loss= 1.07156 val_acc= 0.79600 time= 0.01001\n",
      "Epoch: 0196 train_loss= 0.60296 train_acc= 0.97857 val_loss= 1.06957 val_acc= 0.79400 time= 0.00859\n",
      "Epoch: 0197 train_loss= 0.64597 train_acc= 0.95714 val_loss= 1.06772 val_acc= 0.79200 time= 0.00851\n",
      "Epoch: 0198 train_loss= 0.62151 train_acc= 0.97143 val_loss= 1.06583 val_acc= 0.79400 time= 0.01056\n",
      "Epoch: 0199 train_loss= 0.62766 train_acc= 0.94286 val_loss= 1.06378 val_acc= 0.79400 time= 0.00953\n",
      "Epoch: 0200 train_loss= 0.60379 train_acc= 0.93571 val_loss= 1.06281 val_acc= 0.79400 time= 0.00850\n",
      "Optimization Finished!\n",
      "total train time 5.46797\n",
      "Test set results: cost= 1.01651 accuracy= 0.81700 time= 0.00502\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(flags_dataset)\n",
    "\n",
    "# Some preprocessing\n",
    "features = preprocess_features(features)\n",
    "if flags_model == 'gcn':\n",
    "    support = [preprocess_adj(adj)]\n",
    "    num_supports = 1\n",
    "    model_func = GCN\n",
    "elif flags_model == 'gcn_cheby':\n",
    "    support = chebyshev_polynomials(adj, flags_max_degree)\n",
    "    num_supports = 1 + flags_max_degree\n",
    "    model_func = GCN\n",
    "elif flags_model == 'dense':\n",
    "    support = [preprocess_adj(adj)]  # Not used\n",
    "    num_supports = 1\n",
    "    model_func = MLP\n",
    "else:\n",
    "    raise ValueError('Invalid argument for model: ' + str(flags_model))\n",
    "\n",
    "# Define placeholders\n",
    "placeholders = {\n",
    "    'support': [tf.sparse_placeholder(tf.float32) for _ in range(num_supports)],\n",
    "    'features': tf.sparse_placeholder(tf.float32, shape=tf.constant(features[2], dtype=tf.int64)),\n",
    "    'labels': tf.placeholder(tf.float32, shape=(None, y_train.shape[1])),\n",
    "    'labels_mask': tf.placeholder(tf.int32),\n",
    "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "    'num_features_nonzero': tf.placeholder(tf.int32)  # helper variable for sparse dropout\n",
    "}\n",
    "\n",
    "# Create model\n",
    "model = model_func(placeholders, input_dim=features[2][1], logging=True)\n",
    "\n",
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "\n",
    "\n",
    "# Define model evaluation function\n",
    "def evaluate(features, support, labels, mask, placeholders):\n",
    "    t_test = time.time()\n",
    "    feed_dict_val = construct_feed_dict(features, support, labels, mask, placeholders)\n",
    "    outs_val = sess.run([model.loss, model.accuracy], feed_dict=feed_dict_val)\n",
    "    return outs_val[0], outs_val[1], (time.time() - t_test)\n",
    "\n",
    "\n",
    "# Init variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "cost_val = []\n",
    "\n",
    "t_begin = time.time()\n",
    "print(\"start training...\")\n",
    "# Train model\n",
    "for epoch in range(flags_epochs):\n",
    "\n",
    "    t = time.time()\n",
    "    # Construct feed dictionary\n",
    "    feed_dict = construct_feed_dict(features, support, y_train, train_mask, placeholders)\n",
    "    feed_dict.update({placeholders['dropout']: flags_dropout})\n",
    "\n",
    "    # Training step\n",
    "    outs = sess.run([model.opt_op, model.loss, model.accuracy], feed_dict=feed_dict)\n",
    "\n",
    "    # Validation\n",
    "    cost, acc, duration = evaluate(features, support, y_val, val_mask, placeholders)\n",
    "    cost_val.append(cost)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(outs[1]),\n",
    "          \"train_acc=\", \"{:.5f}\".format(outs[2]), \"val_loss=\", \"{:.5f}\".format(cost),\n",
    "          \"val_acc=\", \"{:.5f}\".format(acc), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "    if epoch > flags_early_stopping and cost_val[-1] > np.mean(cost_val[-(flags_early_stopping+1):-1]):\n",
    "        print(\"Early stopping...\")\n",
    "        break\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "print(\"total train time {:.5f}\".format(time.time() - t_begin))\n",
    "\n",
    "# Testing\n",
    "test_cost, test_acc, test_duration = evaluate(features, support, y_test, test_mask, placeholders)\n",
    "print(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost),\n",
    "      \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(test_duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
