{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/zhisheng/anaconda3/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from gcn.inits import *\n",
    "from gcn.utils import *\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "flags = tf.compat.v1.flags\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Layers Class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global unique layer ID dictionary for layer name assignment\n",
    "_LAYER_UIDS = {}\n",
    "\n",
    "\n",
    "def get_layer_uid(layer_name=''):\n",
    "    \"\"\"Helper function, assigns unique layer IDs.\"\"\"\n",
    "    if layer_name not in _LAYER_UIDS:\n",
    "        _LAYER_UIDS[layer_name] = 1\n",
    "        return 1\n",
    "    else:\n",
    "        _LAYER_UIDS[layer_name] += 1\n",
    "        return _LAYER_UIDS[layer_name]\n",
    "\n",
    "\n",
    "def sparse_dropout(x, keep_prob, noise_shape):\n",
    "    \"\"\"Dropout for sparse tensors.\"\"\"\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += tf.random_uniform(noise_shape)\n",
    "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
    "    pre_out = tf.sparse_retain(x, dropout_mask)\n",
    "    return pre_out * (1./keep_prob)\n",
    "\n",
    "\n",
    "def dot(x, y, sparse=False):\n",
    "    \"\"\"Wrapper for tf.matmul (sparse vs dense).\"\"\"\n",
    "    if sparse:\n",
    "        res = tf.sparse_tensor_dense_matmul(x, y)\n",
    "    else:\n",
    "        res = tf.matmul(x, y)\n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Layer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    \"\"\"Base layer class. Defines basic API for all layer objects.\n",
    "    Implementation inspired by keras (http://keras.io).\n",
    "\n",
    "    # Properties\n",
    "        name: String, defines the variable scope of the layer.\n",
    "        logging: Boolean, switches Tensorflow histogram logging on/off\n",
    "\n",
    "    # Methods\n",
    "        _call(inputs): Defines computation graph of layer\n",
    "            (i.e. takes input, returns output)\n",
    "        __call__(inputs): Wrapper for _call()\n",
    "        _log_vars(): Log all variables\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        allowed_kwargs = {'name', 'logging'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            layer = self.__class__.__name__.lower()\n",
    "            name = layer + '_' + str(get_layer_uid(layer))\n",
    "        self.name = name\n",
    "        self.vars = {}\n",
    "        logging = kwargs.get('logging', False)\n",
    "        self.logging = logging\n",
    "        self.sparse_inputs = False\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        return inputs\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.name_scope(self.name):\n",
    "            if self.logging and not self.sparse_inputs:\n",
    "                tf.summary.histogram(self.name + '/inputs', inputs)\n",
    "            outputs = self._call(inputs)\n",
    "            if self.logging:\n",
    "                tf.summary.histogram(self.name + '/outputs', outputs)\n",
    "            return outputs\n",
    "\n",
    "    def _log_vars(self):\n",
    "        for var in self.vars:\n",
    "            tf.summary.histogram(self.name + '/vars/' + var, self.vars[var])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    \"\"\"Dense layer.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, placeholders, dropout=0., sparse_inputs=False,\n",
    "                 act=tf.nn.relu, bias=False, featureless=False, **kwargs):\n",
    "        super(Dense, self).__init__(**kwargs)\n",
    "\n",
    "        if dropout:\n",
    "            self.dropout = placeholders['dropout']\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "\n",
    "        self.act = act\n",
    "        self.sparse_inputs = sparse_inputs\n",
    "        self.featureless = featureless\n",
    "        self.bias = bias\n",
    "\n",
    "        # helper variable for sparse dropout\n",
    "        self.num_features_nonzero = placeholders['num_features_nonzero']\n",
    "\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            self.vars['weights'] = glorot([input_dim, output_dim],\n",
    "                                          name='weights')\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([output_dim], name='bias')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        # dropout\n",
    "        if self.sparse_inputs:\n",
    "            x = sparse_dropout(x, 1-self.dropout, self.num_features_nonzero)\n",
    "        else:\n",
    "            x = tf.nn.dropout(x, 1-self.dropout)\n",
    "\n",
    "        # transform\n",
    "        output = dot(x, self.vars['weights'], sparse=self.sparse_inputs)\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "\n",
    "        return self.act(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Graph Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(Layer):\n",
    "    \"\"\"Graph convolution layer.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, placeholders, dropout=0.,\n",
    "                 sparse_inputs=False, act=tf.nn.relu, bias=False,\n",
    "                 featureless=False, **kwargs):\n",
    "        super(GraphConvolution, self).__init__(**kwargs)\n",
    "\n",
    "        if dropout:\n",
    "            self.dropout = placeholders['dropout']\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "\n",
    "        self.act = act\n",
    "        self.support = placeholders['support']\n",
    "        self.sparse_inputs = sparse_inputs\n",
    "        self.featureless = featureless\n",
    "        self.bias = bias\n",
    "\n",
    "        # helper variable for sparse dropout\n",
    "        self.num_features_nonzero = placeholders['num_features_nonzero']\n",
    "\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            for i in range(len(self.support)):\n",
    "                self.vars['weights_' + str(i)] = glorot([input_dim, output_dim],\n",
    "                                                        name='weights_' + str(i))\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([output_dim], name='bias')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        # dropout\n",
    "        if self.sparse_inputs:\n",
    "            x = sparse_dropout(x, 1-self.dropout, self.num_features_nonzero)\n",
    "        else:\n",
    "            x = tf.nn.dropout(x, 1-self.dropout)\n",
    "\n",
    "        # convolve\n",
    "        supports = list()\n",
    "        for i in range(len(self.support)):\n",
    "            if not self.featureless:\n",
    "                pre_sup = dot(x, self.vars['weights_' + str(i)],\n",
    "                              sparse=self.sparse_inputs)\n",
    "            else:\n",
    "                pre_sup = self.vars['weights_' + str(i)]\n",
    "            support = dot(self.support[i], pre_sup, sparse=True)\n",
    "            supports.append(support)\n",
    "        output = tf.add_n(supports)\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "\n",
    "        return self.act(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcn.metrics import *\n",
    "\n",
    "flags = tf.compat.v1.flags\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        allowed_kwargs = {'name', 'logging'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            name = self.__class__.__name__.lower()\n",
    "        self.name = name\n",
    "\n",
    "        logging = kwargs.get('logging', False)\n",
    "        self.logging = logging\n",
    "\n",
    "        self.vars = {}\n",
    "        self.placeholders = {}\n",
    "\n",
    "        self.layers = []\n",
    "        self.activations = []\n",
    "\n",
    "        self.inputs = None\n",
    "        self.outputs = None\n",
    "\n",
    "        self.loss = 0\n",
    "        self.accuracy = 0\n",
    "        self.optimizer = None\n",
    "        self.opt_op = None\n",
    "\n",
    "    def _build(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\" Wrapper for _build() \"\"\"\n",
    "        with tf.variable_scope(self.name):\n",
    "            self._build()\n",
    "\n",
    "        # Build sequential layer model\n",
    "        self.activations.append(self.inputs)\n",
    "        for layer in self.layers:\n",
    "            hidden = layer(self.activations[-1])\n",
    "            self.activations.append(hidden)\n",
    "        self.outputs = self.activations[-1]\n",
    "\n",
    "        # Store model variables for easy access\n",
    "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
    "        self.vars = {var.name: var for var in variables}\n",
    "\n",
    "        # Build metrics\n",
    "        self._loss()\n",
    "        self._accuracy()\n",
    "\n",
    "        self.opt_op = self.optimizer.minimize(self.loss)\n",
    "\n",
    "    def predict(self):\n",
    "        pass\n",
    "\n",
    "    def _loss(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _accuracy(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def save(self, sess=None):\n",
    "        if not sess:\n",
    "            raise AttributeError(\"TensorFlow session not provided.\")\n",
    "        saver = tf.train.Saver(self.vars)\n",
    "        save_path = saver.save(sess, \"tmp/%s.ckpt\" % self.name)\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "    def load(self, sess=None):\n",
    "        if not sess:\n",
    "            raise AttributeError(\"TensorFlow session not provided.\")\n",
    "        saver = tf.train.Saver(self.vars)\n",
    "        save_path = \"tmp/%s.ckpt\" % self.name\n",
    "        saver.restore(sess, save_path)\n",
    "        print(\"Model restored from file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(Model):\n",
    "    def __init__(self, placeholders, input_dim, **kwargs):\n",
    "        super(MLP, self).__init__(**kwargs)\n",
    "\n",
    "        self.inputs = placeholders['features']\n",
    "        self.input_dim = input_dim\n",
    "        # self.input_dim = self.inputs.get_shape().as_list()[1]  # To be supported in future Tensorflow versions\n",
    "        self.output_dim = placeholders['labels'].get_shape().as_list()[1]\n",
    "        self.placeholders = placeholders\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=flags_learning_rate)\n",
    "\n",
    "        self.build()\n",
    "\n",
    "    def _loss(self):\n",
    "        # Weight decay loss\n",
    "        for var in self.layers[0].vars.values():\n",
    "            self.loss += flags_weight_decay * tf.nn.l2_loss(var)\n",
    "\n",
    "        # Cross entropy error\n",
    "        self.loss += masked_softmax_cross_entropy(self.outputs, self.placeholders['labels'],\n",
    "                                                  self.placeholders['labels_mask'])\n",
    "\n",
    "    def _accuracy(self):\n",
    "        self.accuracy = masked_accuracy(self.outputs, self.placeholders['labels'],\n",
    "                                        self.placeholders['labels_mask'])\n",
    "\n",
    "    def _build(self):\n",
    "        self.layers.append(Dense(input_dim=self.input_dim,\n",
    "                                 output_dim=flags_hidden1,\n",
    "                                 placeholders=self.placeholders,\n",
    "                                 act=tf.nn.relu,\n",
    "                                 dropout=True,\n",
    "                                 sparse_inputs=True,\n",
    "                                 logging=self.logging))\n",
    "\n",
    "        self.layers.append(Dense(input_dim=flags_hidden1,\n",
    "                                 output_dim=self.output_dim,\n",
    "                                 placeholders=self.placeholders,\n",
    "                                 act=lambda x: x,\n",
    "                                 dropout=True,\n",
    "                                 logging=self.logging))\n",
    "\n",
    "    def predict(self):\n",
    "        return tf.nn.softmax(self.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(Model):\n",
    "    def __init__(self, placeholders, input_dim, **kwargs):\n",
    "        super(GCN, self).__init__(**kwargs)\n",
    "\n",
    "        self.inputs = placeholders['features']\n",
    "        self.input_dim = input_dim\n",
    "        # self.input_dim = self.inputs.get_shape().as_list()[1]  # To be supported in future Tensorflow versions\n",
    "        self.output_dim = placeholders['labels'].get_shape().as_list()[1]\n",
    "        self.placeholders = placeholders\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=flags_learning_rate)\n",
    "\n",
    "        self.build()\n",
    "\n",
    "    def _loss(self):\n",
    "        # Weight decay loss\n",
    "        for var in self.layers[0].vars.values():\n",
    "            self.loss += flags_weight_decay * tf.nn.l2_loss(var)\n",
    "\n",
    "        # Cross entropy error\n",
    "        self.loss += masked_softmax_cross_entropy(self.outputs, self.placeholders['labels'],\n",
    "                                                  self.placeholders['labels_mask'])\n",
    "\n",
    "    def _accuracy(self):\n",
    "        self.accuracy = masked_accuracy(self.outputs, self.placeholders['labels'],\n",
    "                                        self.placeholders['labels_mask'])\n",
    "\n",
    "    def _build(self):\n",
    "\n",
    "        self.layers.append(GraphConvolution(input_dim=self.input_dim,\n",
    "                                            output_dim=flags_hidden1,\n",
    "                                            placeholders=self.placeholders,\n",
    "                                            act=tf.nn.relu,\n",
    "                                            dropout=True,\n",
    "                                            sparse_inputs=True,\n",
    "                                            logging=self.logging))\n",
    "\n",
    "        self.layers.append(GraphConvolution(input_dim=flags_hidden1,\n",
    "                                            output_dim=self.output_dim,\n",
    "                                            placeholders=self.placeholders,\n",
    "                                            act=lambda x: x,\n",
    "                                            dropout=True,\n",
    "                                            logging=self.logging))\n",
    "\n",
    "    def predict(self):\n",
    "        return tf.nn.softmax(self.outputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "# import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "# flags = tf.compat.v1.flags\n",
    "# FLAGS = flags.FLAGS\n",
    "flags_dataset = 'cora'# , 'Dataset string.')  # 'cora', 'citeseer', 'pubmed'\n",
    "flags_model = 'gcn' #, 'Model string.')  # 'gcn', 'gcn_cheby', 'dense'\n",
    "flags_learning_rate = 0.01 #, 'Initial learning rate.')\n",
    "flags_epochs = 200 #, 'Number of epochs to train.')\n",
    "flags_hidden1 = 16 #, 'Number of units in hidden layer 1.')\n",
    "flags_dropout = 0.5 #, 'Dropout rate (1 - keep probability).')\n",
    "flags_weight_decay = 5e-4 #, 'Weight for L2 loss on embedding matrix.')\n",
    "flags_early_stopping = 10 #, 'Tolerance for early stopping (# of epochs).')\n",
    "flags_max_degree = 3 #, 'Maximum Chebyshev polynomial degree.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/zhisheng/anaconda3/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Users/zhisheng/anaconda3/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhisheng/uiuc_mcs/repos/cs598-dl-healthcare-proj/src/gcn/utils.py:70: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
      "2023-04-27 23:02:44.903773: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n",
      "start training...\n",
      "Epoch: 0001 train_loss= 1.95398 train_acc= 0.14286 val_loss= 1.94916 val_acc= 0.19200 time= 1.78490\n",
      "Epoch: 0002 train_loss= 1.94703 train_acc= 0.25000 val_loss= 1.94439 val_acc= 0.24600 time= 0.02696\n",
      "Epoch: 0003 train_loss= 1.93786 train_acc= 0.47143 val_loss= 1.93939 val_acc= 0.29800 time= 0.02360\n",
      "Epoch: 0004 train_loss= 1.92763 train_acc= 0.61429 val_loss= 1.93472 val_acc= 0.29800 time= 0.02288\n",
      "Epoch: 0005 train_loss= 1.91676 train_acc= 0.66429 val_loss= 1.93066 val_acc= 0.30000 time= 0.02347\n",
      "Epoch: 0006 train_loss= 1.90550 train_acc= 0.68571 val_loss= 1.92699 val_acc= 0.29800 time= 0.02228\n",
      "Epoch: 0007 train_loss= 1.89381 train_acc= 0.71429 val_loss= 1.92353 val_acc= 0.29600 time= 0.02303\n",
      "Epoch: 0008 train_loss= 1.88170 train_acc= 0.72857 val_loss= 1.92018 val_acc= 0.30000 time= 0.02344\n",
      "Epoch: 0009 train_loss= 1.86913 train_acc= 0.75714 val_loss= 1.91694 val_acc= 0.30400 time= 0.02287\n",
      "Epoch: 0010 train_loss= 1.85615 train_acc= 0.76429 val_loss= 1.91378 val_acc= 0.30800 time= 0.02385\n",
      "Epoch: 0011 train_loss= 1.84276 train_acc= 0.76429 val_loss= 1.91071 val_acc= 0.31200 time= 0.02524\n",
      "Epoch: 0012 train_loss= 1.82894 train_acc= 0.77857 val_loss= 1.90769 val_acc= 0.32000 time= 0.04722\n",
      "Epoch: 0013 train_loss= 1.81469 train_acc= 0.77857 val_loss= 1.90471 val_acc= 0.33800 time= 0.02365\n",
      "Epoch: 0014 train_loss= 1.80000 train_acc= 0.79286 val_loss= 1.90172 val_acc= 0.34600 time= 0.02420\n",
      "Epoch: 0015 train_loss= 1.78490 train_acc= 0.79286 val_loss= 1.89874 val_acc= 0.35000 time= 0.02217\n",
      "Epoch: 0016 train_loss= 1.76937 train_acc= 0.80000 val_loss= 1.89577 val_acc= 0.35400 time= 0.02331\n",
      "Epoch: 0017 train_loss= 1.75346 train_acc= 0.80714 val_loss= 1.89279 val_acc= 0.35600 time= 0.02344\n",
      "Epoch: 0018 train_loss= 1.73716 train_acc= 0.82143 val_loss= 1.88979 val_acc= 0.35800 time= 0.02294\n",
      "Epoch: 0019 train_loss= 1.72046 train_acc= 0.83571 val_loss= 1.88672 val_acc= 0.36600 time= 0.02508\n",
      "Epoch: 0020 train_loss= 1.70337 train_acc= 0.85000 val_loss= 1.88361 val_acc= 0.37400 time= 0.04388\n",
      "Epoch: 0021 train_loss= 1.68590 train_acc= 0.87143 val_loss= 1.88048 val_acc= 0.38800 time= 0.02299\n",
      "Epoch: 0022 train_loss= 1.66809 train_acc= 0.87143 val_loss= 1.87729 val_acc= 0.39600 time= 0.02236\n",
      "Epoch: 0023 train_loss= 1.64990 train_acc= 0.87857 val_loss= 1.87397 val_acc= 0.40200 time= 0.02336\n",
      "Epoch: 0024 train_loss= 1.63140 train_acc= 0.89286 val_loss= 1.87059 val_acc= 0.41000 time= 0.02469\n",
      "Epoch: 0025 train_loss= 1.61256 train_acc= 0.90714 val_loss= 1.86709 val_acc= 0.41600 time= 0.02238\n",
      "Epoch: 0026 train_loss= 1.59343 train_acc= 0.90714 val_loss= 1.86351 val_acc= 0.42800 time= 0.02255\n",
      "Epoch: 0027 train_loss= 1.57403 train_acc= 0.91429 val_loss= 1.85971 val_acc= 0.44200 time= 0.02634\n",
      "Epoch: 0028 train_loss= 1.55433 train_acc= 0.92857 val_loss= 1.85572 val_acc= 0.45400 time= 0.03779\n",
      "Epoch: 0029 train_loss= 1.53440 train_acc= 0.94286 val_loss= 1.85158 val_acc= 0.47000 time= 0.02274\n",
      "Epoch: 0030 train_loss= 1.51423 train_acc= 0.94286 val_loss= 1.84740 val_acc= 0.49600 time= 0.02367\n",
      "Epoch: 0031 train_loss= 1.49389 train_acc= 0.94286 val_loss= 1.84307 val_acc= 0.52200 time= 0.02457\n",
      "Epoch: 0032 train_loss= 1.47338 train_acc= 0.95000 val_loss= 1.83857 val_acc= 0.54200 time= 0.02266\n",
      "Epoch: 0033 train_loss= 1.45272 train_acc= 0.95000 val_loss= 1.83386 val_acc= 0.56400 time= 0.03893\n",
      "Epoch: 0034 train_loss= 1.43198 train_acc= 0.95000 val_loss= 1.82892 val_acc= 0.58600 time= 0.02497\n",
      "Epoch: 0035 train_loss= 1.41118 train_acc= 0.95000 val_loss= 1.82380 val_acc= 0.61000 time= 0.02515\n",
      "Epoch: 0036 train_loss= 1.39035 train_acc= 0.95000 val_loss= 1.81847 val_acc= 0.62000 time= 0.02331\n",
      "Epoch: 0037 train_loss= 1.36953 train_acc= 0.95000 val_loss= 1.81300 val_acc= 0.63600 time= 0.02307\n",
      "Epoch: 0038 train_loss= 1.34876 train_acc= 0.95000 val_loss= 1.80734 val_acc= 0.64800 time= 0.02311\n",
      "Epoch: 0039 train_loss= 1.32808 train_acc= 0.95000 val_loss= 1.80158 val_acc= 0.66800 time= 0.02337\n",
      "Epoch: 0040 train_loss= 1.30752 train_acc= 0.95000 val_loss= 1.79561 val_acc= 0.68200 time= 0.02351\n",
      "Epoch: 0041 train_loss= 1.28714 train_acc= 0.95000 val_loss= 1.78948 val_acc= 0.69600 time= 0.02229\n",
      "Epoch: 0042 train_loss= 1.26698 train_acc= 0.95714 val_loss= 1.78323 val_acc= 0.70600 time= 0.02281\n",
      "Epoch: 0043 train_loss= 1.24704 train_acc= 0.97143 val_loss= 1.77680 val_acc= 0.71600 time= 0.02384\n",
      "Epoch: 0044 train_loss= 1.22733 train_acc= 0.97143 val_loss= 1.77022 val_acc= 0.72200 time= 0.02568\n",
      "Epoch: 0045 train_loss= 1.20796 train_acc= 0.97143 val_loss= 1.76358 val_acc= 0.72200 time= 0.02450\n",
      "Epoch: 0046 train_loss= 1.18889 train_acc= 0.97857 val_loss= 1.75681 val_acc= 0.73000 time= 0.02513\n",
      "Epoch: 0047 train_loss= 1.17014 train_acc= 0.97857 val_loss= 1.75000 val_acc= 0.73400 time= 0.02429\n",
      "Epoch: 0048 train_loss= 1.15175 train_acc= 0.97857 val_loss= 1.74319 val_acc= 0.74200 time= 0.02354\n",
      "Epoch: 0049 train_loss= 1.13372 train_acc= 0.98571 val_loss= 1.73626 val_acc= 0.74000 time= 0.02291\n",
      "Epoch: 0050 train_loss= 1.11606 train_acc= 0.98571 val_loss= 1.72921 val_acc= 0.74000 time= 0.02253\n",
      "Epoch: 0051 train_loss= 1.09881 train_acc= 0.98571 val_loss= 1.72218 val_acc= 0.74800 time= 0.02256\n",
      "Epoch: 0052 train_loss= 1.08193 train_acc= 0.98571 val_loss= 1.71506 val_acc= 0.75000 time= 0.02349\n",
      "Epoch: 0053 train_loss= 1.06548 train_acc= 0.98571 val_loss= 1.70786 val_acc= 0.74800 time= 0.02979\n",
      "Epoch: 0054 train_loss= 1.04940 train_acc= 0.98571 val_loss= 1.70070 val_acc= 0.75600 time= 0.02276\n",
      "Epoch: 0055 train_loss= 1.03370 train_acc= 0.98571 val_loss= 1.69354 val_acc= 0.75400 time= 0.02317\n",
      "Epoch: 0056 train_loss= 1.01844 train_acc= 0.98571 val_loss= 1.68641 val_acc= 0.75600 time= 0.02359\n",
      "Epoch: 0057 train_loss= 1.00355 train_acc= 0.98571 val_loss= 1.67935 val_acc= 0.75600 time= 0.02284\n",
      "Epoch: 0058 train_loss= 0.98907 train_acc= 0.98571 val_loss= 1.67229 val_acc= 0.75800 time= 0.02279\n",
      "Epoch: 0059 train_loss= 0.97500 train_acc= 0.98571 val_loss= 1.66514 val_acc= 0.76000 time= 0.02230\n",
      "Epoch: 0060 train_loss= 0.96130 train_acc= 0.98571 val_loss= 1.65787 val_acc= 0.76000 time= 0.02241\n",
      "Epoch: 0061 train_loss= 0.94796 train_acc= 0.99286 val_loss= 1.65052 val_acc= 0.76000 time= 0.02407\n",
      "Epoch: 0062 train_loss= 0.93500 train_acc= 0.99286 val_loss= 1.64333 val_acc= 0.76200 time= 0.02680\n",
      "Epoch: 0063 train_loss= 0.92242 train_acc= 0.99286 val_loss= 1.63625 val_acc= 0.76400 time= 0.02310\n",
      "Epoch: 0064 train_loss= 0.91021 train_acc= 0.99286 val_loss= 1.62909 val_acc= 0.76800 time= 0.02314\n",
      "Epoch: 0065 train_loss= 0.89835 train_acc= 0.99286 val_loss= 1.62194 val_acc= 0.76800 time= 0.02331\n",
      "Epoch: 0066 train_loss= 0.88682 train_acc= 0.99286 val_loss= 1.61477 val_acc= 0.76800 time= 0.02420\n",
      "Epoch: 0067 train_loss= 0.87562 train_acc= 0.99286 val_loss= 1.60771 val_acc= 0.77200 time= 0.02306\n",
      "Epoch: 0068 train_loss= 0.86472 train_acc= 0.99286 val_loss= 1.60074 val_acc= 0.77200 time= 0.02291\n",
      "Epoch: 0069 train_loss= 0.85413 train_acc= 0.99286 val_loss= 1.59390 val_acc= 0.77200 time= 0.02345\n",
      "Epoch: 0070 train_loss= 0.84387 train_acc= 0.99286 val_loss= 1.58707 val_acc= 0.77600 time= 0.02292\n",
      "Epoch: 0071 train_loss= 0.83387 train_acc= 0.99286 val_loss= 1.58009 val_acc= 0.77600 time= 0.02388\n",
      "Epoch: 0072 train_loss= 0.82418 train_acc= 0.99286 val_loss= 1.57320 val_acc= 0.77600 time= 0.02347\n",
      "Epoch: 0073 train_loss= 0.81476 train_acc= 0.99286 val_loss= 1.56647 val_acc= 0.77800 time= 0.02383\n",
      "Epoch: 0074 train_loss= 0.80560 train_acc= 0.99286 val_loss= 1.55983 val_acc= 0.77800 time= 0.02424\n",
      "Epoch: 0075 train_loss= 0.79669 train_acc= 0.99286 val_loss= 1.55336 val_acc= 0.77800 time= 0.02475\n",
      "Epoch: 0076 train_loss= 0.78803 train_acc= 0.99286 val_loss= 1.54685 val_acc= 0.78000 time= 0.02472\n",
      "Epoch: 0077 train_loss= 0.77960 train_acc= 0.99286 val_loss= 1.54028 val_acc= 0.78200 time= 0.02525\n",
      "Epoch: 0078 train_loss= 0.77141 train_acc= 0.99286 val_loss= 1.53382 val_acc= 0.78200 time= 0.02420\n",
      "Epoch: 0079 train_loss= 0.76345 train_acc= 1.00000 val_loss= 1.52763 val_acc= 0.78400 time= 0.02395\n",
      "Epoch: 0080 train_loss= 0.75569 train_acc= 1.00000 val_loss= 1.52147 val_acc= 0.78400 time= 0.02617\n",
      "Epoch: 0081 train_loss= 0.74813 train_acc= 1.00000 val_loss= 1.51541 val_acc= 0.78600 time= 0.02382\n",
      "Epoch: 0082 train_loss= 0.74078 train_acc= 1.00000 val_loss= 1.50938 val_acc= 0.78600 time= 0.02384\n",
      "Epoch: 0083 train_loss= 0.73363 train_acc= 1.00000 val_loss= 1.50351 val_acc= 0.78800 time= 0.03376\n",
      "Epoch: 0084 train_loss= 0.72666 train_acc= 1.00000 val_loss= 1.49775 val_acc= 0.78800 time= 0.02301\n",
      "Epoch: 0085 train_loss= 0.71986 train_acc= 1.00000 val_loss= 1.49204 val_acc= 0.78800 time= 0.02314\n",
      "Epoch: 0086 train_loss= 0.71324 train_acc= 1.00000 val_loss= 1.48642 val_acc= 0.79000 time= 0.02204\n",
      "Epoch: 0087 train_loss= 0.70677 train_acc= 1.00000 val_loss= 1.48077 val_acc= 0.79000 time= 0.02307\n",
      "Epoch: 0088 train_loss= 0.70048 train_acc= 1.00000 val_loss= 1.47535 val_acc= 0.79000 time= 0.02318\n",
      "Epoch: 0089 train_loss= 0.69435 train_acc= 1.00000 val_loss= 1.47010 val_acc= 0.79200 time= 0.02297\n",
      "Epoch: 0090 train_loss= 0.68836 train_acc= 1.00000 val_loss= 1.46497 val_acc= 0.79000 time= 0.02409\n",
      "Epoch: 0091 train_loss= 0.68250 train_acc= 1.00000 val_loss= 1.46006 val_acc= 0.79000 time= 0.02197\n",
      "Epoch: 0092 train_loss= 0.67678 train_acc= 1.00000 val_loss= 1.45517 val_acc= 0.79000 time= 0.02255\n",
      "Epoch: 0093 train_loss= 0.67122 train_acc= 1.00000 val_loss= 1.45018 val_acc= 0.79000 time= 0.02295\n",
      "Epoch: 0094 train_loss= 0.66578 train_acc= 1.00000 val_loss= 1.44527 val_acc= 0.79200 time= 0.02359\n",
      "Epoch: 0095 train_loss= 0.66047 train_acc= 1.00000 val_loss= 1.44043 val_acc= 0.79200 time= 0.02365\n",
      "Epoch: 0096 train_loss= 0.65527 train_acc= 1.00000 val_loss= 1.43587 val_acc= 0.79200 time= 0.02394\n",
      "Epoch: 0097 train_loss= 0.65019 train_acc= 1.00000 val_loss= 1.43138 val_acc= 0.79200 time= 0.02459\n",
      "Epoch: 0098 train_loss= 0.64523 train_acc= 1.00000 val_loss= 1.42705 val_acc= 0.79200 time= 0.02607\n",
      "Epoch: 0099 train_loss= 0.64040 train_acc= 1.00000 val_loss= 1.42259 val_acc= 0.79200 time= 0.02702\n",
      "Epoch: 0100 train_loss= 0.63566 train_acc= 1.00000 val_loss= 1.41827 val_acc= 0.79000 time= 0.02380\n",
      "Epoch: 0101 train_loss= 0.63103 train_acc= 1.00000 val_loss= 1.41389 val_acc= 0.79000 time= 0.02339\n",
      "Epoch: 0102 train_loss= 0.62647 train_acc= 1.00000 val_loss= 1.40946 val_acc= 0.79000 time= 0.02386\n",
      "Epoch: 0103 train_loss= 0.62203 train_acc= 1.00000 val_loss= 1.40506 val_acc= 0.78800 time= 0.02398\n",
      "Epoch: 0104 train_loss= 0.61768 train_acc= 1.00000 val_loss= 1.40083 val_acc= 0.78600 time= 0.02281\n",
      "Epoch: 0105 train_loss= 0.61341 train_acc= 1.00000 val_loss= 1.39689 val_acc= 0.78600 time= 0.02284\n",
      "Epoch: 0106 train_loss= 0.60923 train_acc= 1.00000 val_loss= 1.39300 val_acc= 0.78400 time= 0.02354\n",
      "Epoch: 0107 train_loss= 0.60514 train_acc= 1.00000 val_loss= 1.38920 val_acc= 0.78400 time= 0.02731\n",
      "Epoch: 0108 train_loss= 0.60112 train_acc= 1.00000 val_loss= 1.38553 val_acc= 0.78400 time= 0.02412\n",
      "Epoch: 0109 train_loss= 0.59718 train_acc= 1.00000 val_loss= 1.38189 val_acc= 0.78400 time= 0.02308\n",
      "Epoch: 0110 train_loss= 0.59331 train_acc= 1.00000 val_loss= 1.37820 val_acc= 0.78400 time= 0.02312\n",
      "Epoch: 0111 train_loss= 0.58954 train_acc= 1.00000 val_loss= 1.37453 val_acc= 0.78200 time= 0.02449\n",
      "Epoch: 0112 train_loss= 0.58583 train_acc= 1.00000 val_loss= 1.37088 val_acc= 0.78200 time= 0.02320\n",
      "Epoch: 0113 train_loss= 0.58218 train_acc= 1.00000 val_loss= 1.36724 val_acc= 0.78200 time= 0.02284\n",
      "Epoch: 0114 train_loss= 0.57860 train_acc= 1.00000 val_loss= 1.36363 val_acc= 0.78200 time= 0.02300\n",
      "Epoch: 0115 train_loss= 0.57508 train_acc= 1.00000 val_loss= 1.36020 val_acc= 0.78200 time= 0.02368\n",
      "Epoch: 0116 train_loss= 0.57164 train_acc= 1.00000 val_loss= 1.35700 val_acc= 0.78200 time= 0.03293\n",
      "Epoch: 0117 train_loss= 0.56824 train_acc= 1.00000 val_loss= 1.35378 val_acc= 0.78200 time= 0.02338\n",
      "Epoch: 0118 train_loss= 0.56489 train_acc= 1.00000 val_loss= 1.35053 val_acc= 0.78200 time= 0.02322\n",
      "Epoch: 0119 train_loss= 0.56162 train_acc= 1.00000 val_loss= 1.34727 val_acc= 0.78200 time= 0.02363\n",
      "Epoch: 0120 train_loss= 0.55840 train_acc= 1.00000 val_loss= 1.34416 val_acc= 0.78200 time= 0.02247\n",
      "Epoch: 0121 train_loss= 0.55524 train_acc= 1.00000 val_loss= 1.34109 val_acc= 0.78200 time= 0.02328\n",
      "Epoch: 0122 train_loss= 0.55213 train_acc= 1.00000 val_loss= 1.33789 val_acc= 0.78200 time= 0.02260\n",
      "Epoch: 0123 train_loss= 0.54907 train_acc= 1.00000 val_loss= 1.33480 val_acc= 0.78200 time= 0.02324\n",
      "Epoch: 0124 train_loss= 0.54607 train_acc= 1.00000 val_loss= 1.33175 val_acc= 0.78200 time= 0.02594\n",
      "Epoch: 0125 train_loss= 0.54311 train_acc= 1.00000 val_loss= 1.32875 val_acc= 0.78200 time= 0.02678\n",
      "Epoch: 0126 train_loss= 0.54020 train_acc= 1.00000 val_loss= 1.32587 val_acc= 0.78400 time= 0.02323\n",
      "Epoch: 0127 train_loss= 0.53733 train_acc= 1.00000 val_loss= 1.32295 val_acc= 0.78400 time= 0.02338\n",
      "Epoch: 0128 train_loss= 0.53450 train_acc= 1.00000 val_loss= 1.31996 val_acc= 0.78400 time= 0.02325\n",
      "Epoch: 0129 train_loss= 0.53174 train_acc= 1.00000 val_loss= 1.31726 val_acc= 0.78400 time= 0.02250\n",
      "Epoch: 0130 train_loss= 0.52900 train_acc= 1.00000 val_loss= 1.31485 val_acc= 0.78400 time= 0.02272\n",
      "Epoch: 0131 train_loss= 0.52632 train_acc= 1.00000 val_loss= 1.31217 val_acc= 0.78400 time= 0.02400\n",
      "Epoch: 0132 train_loss= 0.52367 train_acc= 1.00000 val_loss= 1.30949 val_acc= 0.78400 time= 0.02288\n",
      "Epoch: 0133 train_loss= 0.52106 train_acc= 1.00000 val_loss= 1.30664 val_acc= 0.78200 time= 0.02413\n",
      "Epoch: 0134 train_loss= 0.51848 train_acc= 1.00000 val_loss= 1.30378 val_acc= 0.78200 time= 0.02334\n",
      "Epoch: 0135 train_loss= 0.51593 train_acc= 1.00000 val_loss= 1.30120 val_acc= 0.78000 time= 0.02472\n",
      "Epoch: 0136 train_loss= 0.51343 train_acc= 1.00000 val_loss= 1.29877 val_acc= 0.78000 time= 0.02469\n",
      "Epoch: 0137 train_loss= 0.51097 train_acc= 1.00000 val_loss= 1.29639 val_acc= 0.78000 time= 0.02426\n",
      "Epoch: 0138 train_loss= 0.50854 train_acc= 1.00000 val_loss= 1.29383 val_acc= 0.78000 time= 0.02258\n",
      "Epoch: 0139 train_loss= 0.50613 train_acc= 1.00000 val_loss= 1.29124 val_acc= 0.78000 time= 0.02213\n",
      "Epoch: 0140 train_loss= 0.50376 train_acc= 1.00000 val_loss= 1.28897 val_acc= 0.78000 time= 0.02327\n",
      "Epoch: 0141 train_loss= 0.50142 train_acc= 1.00000 val_loss= 1.28673 val_acc= 0.78000 time= 0.02302\n",
      "Epoch: 0142 train_loss= 0.49913 train_acc= 1.00000 val_loss= 1.28429 val_acc= 0.78200 time= 0.02322\n",
      "Epoch: 0143 train_loss= 0.49687 train_acc= 1.00000 val_loss= 1.28198 val_acc= 0.78200 time= 0.02621\n",
      "Epoch: 0144 train_loss= 0.49463 train_acc= 1.00000 val_loss= 1.27972 val_acc= 0.78200 time= 0.02325\n",
      "Epoch: 0145 train_loss= 0.49241 train_acc= 1.00000 val_loss= 1.27760 val_acc= 0.78000 time= 0.02369\n",
      "Epoch: 0146 train_loss= 0.49023 train_acc= 1.00000 val_loss= 1.27518 val_acc= 0.78000 time= 0.02246\n",
      "Epoch: 0147 train_loss= 0.48808 train_acc= 1.00000 val_loss= 1.27287 val_acc= 0.78000 time= 0.02256\n",
      "Epoch: 0148 train_loss= 0.48596 train_acc= 1.00000 val_loss= 1.27056 val_acc= 0.78000 time= 0.02305\n",
      "Epoch: 0149 train_loss= 0.48386 train_acc= 1.00000 val_loss= 1.26838 val_acc= 0.78000 time= 0.02350\n",
      "Epoch: 0150 train_loss= 0.48178 train_acc= 1.00000 val_loss= 1.26641 val_acc= 0.78000 time= 0.02378\n",
      "Epoch: 0151 train_loss= 0.47974 train_acc= 1.00000 val_loss= 1.26446 val_acc= 0.78000 time= 0.02405\n",
      "Epoch: 0152 train_loss= 0.47772 train_acc= 1.00000 val_loss= 1.26237 val_acc= 0.78000 time= 0.02572\n",
      "Epoch: 0153 train_loss= 0.47572 train_acc= 1.00000 val_loss= 1.26017 val_acc= 0.78000 time= 0.02273\n",
      "Epoch: 0154 train_loss= 0.47376 train_acc= 1.00000 val_loss= 1.25805 val_acc= 0.78000 time= 0.02247\n",
      "Epoch: 0155 train_loss= 0.47180 train_acc= 1.00000 val_loss= 1.25607 val_acc= 0.78000 time= 0.02290\n",
      "Epoch: 0156 train_loss= 0.46987 train_acc= 1.00000 val_loss= 1.25395 val_acc= 0.78000 time= 0.02254\n",
      "Epoch: 0157 train_loss= 0.46797 train_acc= 1.00000 val_loss= 1.25168 val_acc= 0.78000 time= 0.02283\n",
      "Epoch: 0158 train_loss= 0.46608 train_acc= 1.00000 val_loss= 1.24963 val_acc= 0.78000 time= 0.02363\n",
      "Epoch: 0159 train_loss= 0.46422 train_acc= 1.00000 val_loss= 1.24778 val_acc= 0.78000 time= 0.02350\n",
      "Epoch: 0160 train_loss= 0.46238 train_acc= 1.00000 val_loss= 1.24618 val_acc= 0.78000 time= 0.02429\n",
      "Epoch: 0161 train_loss= 0.46056 train_acc= 1.00000 val_loss= 1.24445 val_acc= 0.78000 time= 0.02472\n",
      "Epoch: 0162 train_loss= 0.45877 train_acc= 1.00000 val_loss= 1.24247 val_acc= 0.78000 time= 0.02369\n",
      "Epoch: 0163 train_loss= 0.45699 train_acc= 1.00000 val_loss= 1.24062 val_acc= 0.78000 time= 0.02191\n",
      "Epoch: 0164 train_loss= 0.45523 train_acc= 1.00000 val_loss= 1.23879 val_acc= 0.78000 time= 0.02226\n",
      "Epoch: 0165 train_loss= 0.45348 train_acc= 1.00000 val_loss= 1.23688 val_acc= 0.78000 time= 0.02241\n",
      "Epoch: 0166 train_loss= 0.45176 train_acc= 1.00000 val_loss= 1.23486 val_acc= 0.78000 time= 0.02296\n",
      "Epoch: 0167 train_loss= 0.45006 train_acc= 1.00000 val_loss= 1.23313 val_acc= 0.78000 time= 0.02394\n",
      "Epoch: 0168 train_loss= 0.44838 train_acc= 1.00000 val_loss= 1.23146 val_acc= 0.78000 time= 0.02850\n",
      "Epoch: 0169 train_loss= 0.44672 train_acc= 1.00000 val_loss= 1.22970 val_acc= 0.78000 time= 0.02272\n",
      "Epoch: 0170 train_loss= 0.44506 train_acc= 1.00000 val_loss= 1.22793 val_acc= 0.78000 time= 0.02390\n",
      "Epoch: 0171 train_loss= 0.44343 train_acc= 1.00000 val_loss= 1.22626 val_acc= 0.78000 time= 0.02509\n",
      "Epoch: 0172 train_loss= 0.44183 train_acc= 1.00000 val_loss= 1.22470 val_acc= 0.78000 time= 0.02284\n",
      "Epoch: 0173 train_loss= 0.44022 train_acc= 1.00000 val_loss= 1.22322 val_acc= 0.78000 time= 0.02221\n",
      "Epoch: 0174 train_loss= 0.43863 train_acc= 1.00000 val_loss= 1.22149 val_acc= 0.78000 time= 0.02305\n",
      "Epoch: 0175 train_loss= 0.43706 train_acc= 1.00000 val_loss= 1.21960 val_acc= 0.78000 time= 0.02241\n",
      "Epoch: 0176 train_loss= 0.43552 train_acc= 1.00000 val_loss= 1.21786 val_acc= 0.78000 time= 0.02189\n",
      "Epoch: 0177 train_loss= 0.43399 train_acc= 1.00000 val_loss= 1.21624 val_acc= 0.78000 time= 0.02326\n",
      "Epoch: 0178 train_loss= 0.43246 train_acc= 1.00000 val_loss= 1.21487 val_acc= 0.78000 time= 0.02232\n",
      "Epoch: 0179 train_loss= 0.43095 train_acc= 1.00000 val_loss= 1.21346 val_acc= 0.78000 time= 0.02368\n",
      "Epoch: 0180 train_loss= 0.42947 train_acc= 1.00000 val_loss= 1.21198 val_acc= 0.78000 time= 0.02601\n",
      "Epoch: 0181 train_loss= 0.42800 train_acc= 1.00000 val_loss= 1.20989 val_acc= 0.78000 time= 0.02257\n",
      "Epoch: 0182 train_loss= 0.42652 train_acc= 1.00000 val_loss= 1.20800 val_acc= 0.78000 time= 0.02325\n",
      "Epoch: 0183 train_loss= 0.42507 train_acc= 1.00000 val_loss= 1.20659 val_acc= 0.78000 time= 0.02284\n",
      "Epoch: 0184 train_loss= 0.42364 train_acc= 1.00000 val_loss= 1.20536 val_acc= 0.78000 time= 0.02276\n",
      "Epoch: 0185 train_loss= 0.42224 train_acc= 1.00000 val_loss= 1.20403 val_acc= 0.78000 time= 0.02243\n",
      "Epoch: 0186 train_loss= 0.42084 train_acc= 1.00000 val_loss= 1.20251 val_acc= 0.78000 time= 0.02206\n",
      "Epoch: 0187 train_loss= 0.41945 train_acc= 1.00000 val_loss= 1.20105 val_acc= 0.78000 time= 0.02364\n",
      "Epoch: 0188 train_loss= 0.41804 train_acc= 1.00000 val_loss= 1.19972 val_acc= 0.78000 time= 0.02350\n",
      "Epoch: 0189 train_loss= 0.41666 train_acc= 1.00000 val_loss= 1.19815 val_acc= 0.78000 time= 0.02602\n",
      "Epoch: 0190 train_loss= 0.41531 train_acc= 1.00000 val_loss= 1.19653 val_acc= 0.78000 time= 0.02280\n",
      "Epoch: 0191 train_loss= 0.41397 train_acc= 1.00000 val_loss= 1.19509 val_acc= 0.78000 time= 0.02217\n",
      "Epoch: 0192 train_loss= 0.41262 train_acc= 1.00000 val_loss= 1.19378 val_acc= 0.78000 time= 0.02343\n",
      "Epoch: 0193 train_loss= 0.41129 train_acc= 1.00000 val_loss= 1.19243 val_acc= 0.78000 time= 0.02236\n",
      "Epoch: 0194 train_loss= 0.40998 train_acc= 1.00000 val_loss= 1.19100 val_acc= 0.78000 time= 0.02314\n",
      "Epoch: 0195 train_loss= 0.40868 train_acc= 1.00000 val_loss= 1.18955 val_acc= 0.78000 time= 0.02255\n",
      "Epoch: 0196 train_loss= 0.40738 train_acc= 1.00000 val_loss= 1.18817 val_acc= 0.78000 time= 0.02320\n",
      "Epoch: 0197 train_loss= 0.40610 train_acc= 1.00000 val_loss= 1.18691 val_acc= 0.78000 time= 0.02404\n",
      "Epoch: 0198 train_loss= 0.40483 train_acc= 1.00000 val_loss= 1.18558 val_acc= 0.78000 time= 0.02443\n",
      "Epoch: 0199 train_loss= 0.40357 train_acc= 1.00000 val_loss= 1.18420 val_acc= 0.78000 time= 0.02272\n",
      "Epoch: 0200 train_loss= 0.40234 train_acc= 1.00000 val_loss= 1.18285 val_acc= 0.78000 time= 0.02328\n",
      "Optimization Finished!\n",
      "total train time 6.58892\n",
      "Test set results: cost= 1.13425 accuracy= 0.79300 time= 0.01040\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(flags_dataset)\n",
    "\n",
    "# Some preprocessing\n",
    "features = preprocess_features(features)\n",
    "if flags_model == 'gcn':\n",
    "    support = [preprocess_adj(adj)]\n",
    "    num_supports = 1\n",
    "    model_func = GCN\n",
    "elif flags_model == 'gcn_cheby':\n",
    "    support = chebyshev_polynomials(adj, flags_max_degree)\n",
    "    num_supports = 1 + flags_max_degree\n",
    "    model_func = GCN\n",
    "elif flags_model == 'dense':\n",
    "    support = [preprocess_adj(adj)]  # Not used\n",
    "    num_supports = 1\n",
    "    model_func = MLP\n",
    "else:\n",
    "    raise ValueError('Invalid argument for model: ' + str(flags_model))\n",
    "\n",
    "# Define placeholders\n",
    "placeholders = {\n",
    "    'support': [tf.sparse_placeholder(tf.float32) for _ in range(num_supports)],\n",
    "    'features': tf.sparse_placeholder(tf.float32, shape=tf.constant(features[2], dtype=tf.int64)),\n",
    "    'labels': tf.placeholder(tf.float32, shape=(None, y_train.shape[1])),\n",
    "    'labels_mask': tf.placeholder(tf.int32),\n",
    "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "    'num_features_nonzero': tf.placeholder(tf.int32)  # helper variable for sparse dropout\n",
    "}\n",
    "\n",
    "# Create model\n",
    "model = model_func(placeholders, input_dim=features[2][1], logging=True)\n",
    "\n",
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "\n",
    "\n",
    "# Define model evaluation function\n",
    "def evaluate(features, support, labels, mask, placeholders):\n",
    "    t_test = time.time()\n",
    "    feed_dict_val = construct_feed_dict(features, support, labels, mask, placeholders)\n",
    "    outs_val = sess.run([model.loss, model.accuracy], feed_dict=feed_dict_val)\n",
    "    return outs_val[0], outs_val[1], (time.time() - t_test)\n",
    "\n",
    "\n",
    "# Init variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "cost_val = []\n",
    "\n",
    "t_begin = time.time()\n",
    "print(\"start training...\")\n",
    "# Train model\n",
    "for epoch in range(flags_epochs):\n",
    "\n",
    "    t = time.time()\n",
    "    # Construct feed dictionary\n",
    "    feed_dict = construct_feed_dict(features, support, y_train, train_mask, placeholders)\n",
    "    feed_dict.update({placeholders['dropout']: flags_dropout})\n",
    "\n",
    "    # Training step\n",
    "    outs = sess.run([model.opt_op, model.loss, model.accuracy], feed_dict=feed_dict)\n",
    "\n",
    "    # Validation\n",
    "    cost, acc, duration = evaluate(features, support, y_val, val_mask, placeholders)\n",
    "    cost_val.append(cost)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(outs[1]),\n",
    "          \"train_acc=\", \"{:.5f}\".format(outs[2]), \"val_loss=\", \"{:.5f}\".format(cost),\n",
    "          \"val_acc=\", \"{:.5f}\".format(acc), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "    if epoch > flags_early_stopping and cost_val[-1] > np.mean(cost_val[-(flags_early_stopping+1):-1]):\n",
    "        print(\"Early stopping...\")\n",
    "        break\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "print(\"total train time {:.5f}\".format(time.time() - t_begin))\n",
    "\n",
    "# Testing\n",
    "test_cost, test_acc, test_duration = evaluate(features, support, y_test, test_mask, placeholders)\n",
    "print(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost),\n",
    "      \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(test_duration))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
