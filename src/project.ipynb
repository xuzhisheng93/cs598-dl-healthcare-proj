{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter notebook is created to reproduce the results and experiment abalations of paper \"SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS\" by Thomas N. Kipf and Max Welling. It was published on \"Proceedings of the International Conference on Learning Representations (ICLR)\" and can be found at https://arxiv.org/pdf/1609.02907.pdf.\n",
    "\n",
    "The authors of the paper proposed a semi-supervised classification algorithm based on graph convolutional networks (GCNs). And the original code can be found at https://github.com/tkipf/gcn. In this notebook, we reused most of the code and made a few modifications. \n",
    "\n",
    "The main modifications are:\n",
    "1. We modified the code so that it works in compatibility mode with tensorflow 2.12.0.\n",
    "2. We put the original code except `train.py` under `src/gcn` directory. And they were modified to be compatible with tensorflow 2.12.0.\n",
    "3. TODO: ablations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Download"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is included in the original GCN repository. Thus, there is no need to download it again. It can be found under `gcn/data` directory."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of data statistics\n",
    "\n",
    "TODO: An overview of the data with any helpful charts and visualizations from the report and ideally directly using the dataset in the notebook (maybe link to the data folder or URL)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducibility Summary\n",
    "\n",
    "By running this notebook, we are able to reproduce the results of GCN on all three datasets. It achieves the same accuracy as the original paper.\n",
    "\n",
    "TODO: A summary of the report and findings, about 200 words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages and Set Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\xuzhi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from gcn.inits import *\n",
    "from gcn.utils import *\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Layers Class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global unique layer ID dictionary for layer name assignment\n",
    "_LAYER_UIDS = {}\n",
    "\n",
    "\n",
    "def get_layer_uid(layer_name=''):\n",
    "    \"\"\"Helper function, assigns unique layer IDs.\"\"\"\n",
    "    if layer_name not in _LAYER_UIDS:\n",
    "        _LAYER_UIDS[layer_name] = 1\n",
    "        return 1\n",
    "    else:\n",
    "        _LAYER_UIDS[layer_name] += 1\n",
    "        return _LAYER_UIDS[layer_name]\n",
    "\n",
    "\n",
    "def sparse_dropout(x, keep_prob, noise_shape):\n",
    "    \"\"\"Dropout for sparse tensors.\"\"\"\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += tf.random_uniform(noise_shape)\n",
    "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
    "    pre_out = tf.sparse_retain(x, dropout_mask)\n",
    "    return pre_out * (1./keep_prob)\n",
    "\n",
    "\n",
    "def dot(x, y, sparse=False):\n",
    "    \"\"\"Wrapper for tf.matmul (sparse vs dense).\"\"\"\n",
    "    if sparse:\n",
    "        res = tf.sparse_tensor_dense_matmul(x, y)\n",
    "    else:\n",
    "        res = tf.matmul(x, y)\n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Layer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    \"\"\"Base layer class. Defines basic API for all layer objects.\n",
    "    Implementation inspired by keras (http://keras.io).\n",
    "\n",
    "    # Properties\n",
    "        name: String, defines the variable scope of the layer.\n",
    "        logging: Boolean, switches Tensorflow histogram logging on/off\n",
    "\n",
    "    # Methods\n",
    "        _call(inputs): Defines computation graph of layer\n",
    "            (i.e. takes input, returns output)\n",
    "        __call__(inputs): Wrapper for _call()\n",
    "        _log_vars(): Log all variables\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        allowed_kwargs = {'name', 'logging'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            layer = self.__class__.__name__.lower()\n",
    "            name = layer + '_' + str(get_layer_uid(layer))\n",
    "        self.name = name\n",
    "        self.vars = {}\n",
    "        logging = kwargs.get('logging', False)\n",
    "        self.logging = logging\n",
    "        self.sparse_inputs = False\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        return inputs\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.name_scope(self.name):\n",
    "            if self.logging and not self.sparse_inputs:\n",
    "                tf.summary.histogram(self.name + '/inputs', inputs)\n",
    "            outputs = self._call(inputs)\n",
    "            if self.logging:\n",
    "                tf.summary.histogram(self.name + '/outputs', outputs)\n",
    "            return outputs\n",
    "\n",
    "    def _log_vars(self):\n",
    "        for var in self.vars:\n",
    "            tf.summary.histogram(self.name + '/vars/' + var, self.vars[var])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Graph Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(Layer):\n",
    "    \"\"\"Graph convolution layer.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, placeholders, dropout=0.,\n",
    "                 sparse_inputs=False, act=tf.nn.relu, bias=False,\n",
    "                 featureless=False, **kwargs):\n",
    "        super(GraphConvolution, self).__init__(**kwargs)\n",
    "\n",
    "        if dropout:\n",
    "            self.dropout = placeholders['dropout']\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "\n",
    "        self.act = act\n",
    "        self.support = placeholders['support']\n",
    "        self.sparse_inputs = sparse_inputs\n",
    "        self.featureless = featureless\n",
    "        self.bias = bias\n",
    "\n",
    "        # helper variable for sparse dropout\n",
    "        self.num_features_nonzero = placeholders['num_features_nonzero']\n",
    "\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            for i in range(len(self.support)):\n",
    "                self.vars['weights_' + str(i)] = glorot([input_dim, output_dim],\n",
    "                                                        name='weights_' + str(i))\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([output_dim], name='bias')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        # dropout\n",
    "        if self.sparse_inputs:\n",
    "            x = sparse_dropout(x, 1-self.dropout, self.num_features_nonzero)\n",
    "        else:\n",
    "            x = tf.nn.dropout(x, 1-self.dropout)\n",
    "\n",
    "        # convolve\n",
    "        supports = list()\n",
    "        for i in range(len(self.support)):\n",
    "            if not self.featureless:\n",
    "                pre_sup = dot(x, self.vars['weights_' + str(i)],\n",
    "                              sparse=self.sparse_inputs)\n",
    "            else:\n",
    "                pre_sup = self.vars['weights_' + str(i)]\n",
    "            support = dot(self.support[i], pre_sup, sparse=True)\n",
    "            supports.append(support)\n",
    "        output = tf.add_n(supports)\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "\n",
    "        return self.act(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcn.metrics import *\n",
    "\n",
    "# flags = tf.compat.v1.flags\n",
    "# FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        print(\"new model, seed=\", tf.get_default_graph().seed)\n",
    "        allowed_kwargs = {'name', 'logging'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            name = self.__class__.__name__.lower()\n",
    "        self.name = name\n",
    "\n",
    "        logging = kwargs.get('logging', False)\n",
    "        self.logging = logging\n",
    "\n",
    "        self.vars = {}\n",
    "        self.placeholders = {}\n",
    "\n",
    "        self.layers = []\n",
    "        self.activations = []\n",
    "\n",
    "        self.inputs = None\n",
    "        self.outputs = None\n",
    "\n",
    "        self.loss = 0\n",
    "        self.accuracy = 0\n",
    "        self.optimizer = None\n",
    "        self.opt_op = None\n",
    "\n",
    "    def _build(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\" Wrapper for _build() \"\"\"\n",
    "        with tf.variable_scope(self.name):\n",
    "            self._build()\n",
    "\n",
    "        # Build sequential layer model\n",
    "        self.activations.append(self.inputs)\n",
    "        for layer in self.layers:\n",
    "            hidden = layer(self.activations[-1])\n",
    "            self.activations.append(hidden)\n",
    "        self.outputs = self.activations[-1]\n",
    "\n",
    "        # Store model variables for easy access\n",
    "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
    "        self.vars = {var.name: var for var in variables}\n",
    "\n",
    "        # Build metrics\n",
    "        self._loss()\n",
    "        self._accuracy()\n",
    "\n",
    "        self.opt_op = self.optimizer.minimize(self.loss)\n",
    "\n",
    "    def predict(self):\n",
    "        pass\n",
    "\n",
    "    def _loss(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _accuracy(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def save(self, sess=None):\n",
    "        if not sess:\n",
    "            raise AttributeError(\"TensorFlow session not provided.\")\n",
    "        saver = tf.train.Saver(self.vars)\n",
    "        save_path = saver.save(sess, \"tmp/%s.ckpt\" % self.name)\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "    def load(self, sess=None):\n",
    "        if not sess:\n",
    "            raise AttributeError(\"TensorFlow session not provided.\")\n",
    "        saver = tf.train.Saver(self.vars)\n",
    "        save_path = \"tmp/%s.ckpt\" % self.name\n",
    "        saver.restore(sess, save_path)\n",
    "        print(\"Model restored from file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(Model):\n",
    "    def __init__(self, placeholders, input_dim, **kwargs):\n",
    "        super(GCN, self).__init__(**kwargs)\n",
    "\n",
    "        # print(\"new GCN\")\n",
    "\n",
    "        self.inputs = placeholders['features']\n",
    "        self.input_dim = input_dim\n",
    "        # self.input_dim = self.inputs.get_shape().as_list()[1]  # To be supported in future Tensorflow versions\n",
    "        self.output_dim = placeholders['labels'].get_shape().as_list()[1]\n",
    "        self.placeholders = placeholders\n",
    "\n",
    "        self.optimizer = flags_optimizer(learning_rate=flags_learning_rate)\n",
    "\n",
    "        self.build()\n",
    "\n",
    "    def _loss(self):\n",
    "        # Weight decay loss\n",
    "        for var in self.layers[0].vars.values():\n",
    "            self.loss += flags_weight_decay * tf.nn.l2_loss(var)\n",
    "\n",
    "        # Cross entropy error\n",
    "        self.loss += masked_softmax_cross_entropy(self.outputs, self.placeholders['labels'],\n",
    "                                                  self.placeholders['labels_mask'])\n",
    "\n",
    "    def _accuracy(self):\n",
    "        self.accuracy = masked_accuracy(self.outputs, self.placeholders['labels'],\n",
    "                                        self.placeholders['labels_mask'])\n",
    "\n",
    "    def _build(self):\n",
    "\n",
    "        self.layers=[]\n",
    "\n",
    "        if flags_layers == 2:\n",
    "            # Paper layer configuration\n",
    "            self.layers.append(GraphConvolution(input_dim=self.input_dim,\n",
    "                                                output_dim=flags_hidden1,\n",
    "                                                placeholders=self.placeholders,\n",
    "                                                act=flags_act_func,\n",
    "                                                dropout=True,\n",
    "                                                sparse_inputs=True,\n",
    "                                                logging=self.logging))\n",
    "\n",
    "            self.layers.append(GraphConvolution(input_dim=flags_hidden1,\n",
    "                                                output_dim=self.output_dim,\n",
    "                                                placeholders=self.placeholders,\n",
    "                                                act=lambda x: x,\n",
    "                                                dropout=True,\n",
    "                                                logging=self.logging))\n",
    "        elif flags_layers == 1:\n",
    "            # Single layer configuration\n",
    "            self.layers.append(GraphConvolution(input_dim=self.input_dim,\n",
    "                                                output_dim=self.output_dim,\n",
    "                                                placeholders=self.placeholders,\n",
    "                                                act=flags_act_func,\n",
    "                                                dropout=True,\n",
    "                                                sparse_inputs=True,\n",
    "                                                logging=self.logging))\n",
    "        elif flags_layers == 3:\n",
    "            # Triple layer configuration\n",
    "            self.layers.append(GraphConvolution(input_dim=self.input_dim,\n",
    "                                                output_dim=64,\n",
    "                                                placeholders=self.placeholders,\n",
    "                                                act=flags_act_func,\n",
    "                                                dropout=True,\n",
    "                                                sparse_inputs=True,\n",
    "                                                logging=self.logging))\n",
    "\n",
    "            self.layers.append(GraphConvolution(input_dim=64,\n",
    "                                                output_dim=flags_hidden1,\n",
    "                                                placeholders=self.placeholders,\n",
    "                                                act=flags_act_func,\n",
    "                                                dropout=True,\n",
    "                                                logging=self.logging))\n",
    "\n",
    "            self.layers.append(GraphConvolution(input_dim=flags_hidden1,\n",
    "                                                output_dim=self.output_dim,\n",
    "                                                placeholders=self.placeholders,\n",
    "                                                act=lambda x: x,\n",
    "                                                dropout=True,\n",
    "                                                logging=self.logging))\n",
    "\n",
    "    def predict(self):\n",
    "        return tf.nn.softmax(self.outputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "# flags = tf.compat.v1.flags\n",
    "# FLAGS = flags.FLAGS\n",
    "flags_dataset = 'cora'# , 'Dataset string.')  # 'cora', 'citeseer', 'pubmed'\n",
    "flags_model = 'gcn' #, 'Model string.')  # 'gcn', 'gcn_cheby', 'dense'\n",
    "flags_learning_rate = 0.01 #, 'Initial learning rate.')\n",
    "flags_epochs = 200 #, 'Number of epochs to train.')\n",
    "flags_hidden1 = 16 #, 'Number of units in hidden layer 1.')\n",
    "flags_dropout = 0.5 #, 'Dropout rate (1 - keep probability).')\n",
    "flags_weight_decay = 5e-4 #, 'Weight for L2 loss on embedding matrix.')\n",
    "flags_early_stopping = 10 #, 'Tolerance for early stopping (# of epochs).')\n",
    "flags_max_degree = 3 #, 'Maximum Chebyshev polynomial degree.')\n",
    "flags_act_func = tf.nn.relu # Activation function: tf.nn.relu, tf.nn.leaky_relu, tf.nn.sigmoid, tf.nn.tanh, tf.nn.elu\n",
    "flags_optimizer = tf.train.AdamOptimizer # Optimizer: tf.train.AdamOptimizer, tf.train.GradientDescentOptimizer, tf.train.AdadeltaOptimizer, tf.train.RMSPropOptimizer\n",
    "flags_layers = 2 # layers: 1, 2, 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Functions to Load Data and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model evaluation function\n",
    "def evaluate(features, support, labels, mask, placeholders, sess, model):\n",
    "    t_test = time.time()\n",
    "    feed_dict_val = construct_feed_dict(features, support, labels, mask, placeholders)\n",
    "    outs_val = sess.run([model.loss, model.accuracy], feed_dict=feed_dict_val)\n",
    "    return outs_val[0], outs_val[1], (time.time() - t_test)\n",
    "\n",
    "def train(adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask):\n",
    "\n",
    "    \"\"\" reset global \"\"\"\n",
    "    _LAYER_UIDS = {}\n",
    "    \n",
    "    # Some preprocessing\n",
    "    features = preprocess_features(features)\n",
    "    if flags_model == 'gcn':\n",
    "        support = [preprocess_adj(adj)]\n",
    "        num_supports = 1\n",
    "        model_func = GCN\n",
    "    elif flags_model == 'gcn_cheby':\n",
    "        support = chebyshev_polynomials(adj, flags_max_degree)\n",
    "        num_supports = 1 + flags_max_degree\n",
    "        model_func = GCN\n",
    "    elif flags_model == 'dense':\n",
    "        support = [preprocess_adj(adj)]  # Not used\n",
    "        num_supports = 1\n",
    "        model_func = MLP\n",
    "    else:\n",
    "        raise ValueError('Invalid argument for model: ' + str(flags_model))\n",
    "\n",
    "    # Define placeholders\n",
    "    placeholders = {\n",
    "        'support': [tf.sparse_placeholder(tf.float32) for _ in range(num_supports)],\n",
    "        'features': tf.sparse_placeholder(tf.float32, shape=tf.constant(features[2], dtype=tf.int64)),\n",
    "        'labels': tf.placeholder(tf.float32, shape=(None, y_train.shape[1])),\n",
    "        'labels_mask': tf.placeholder(tf.int32),\n",
    "        'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "        'num_features_nonzero': tf.placeholder(tf.int32)  # helper variable for sparse dropout\n",
    "    }\n",
    "\n",
    "    # Create model\n",
    "    model = model_func(placeholders, input_dim=features[2][1], logging=True)\n",
    "\n",
    "    # Initialize session\n",
    "    sess = tf.Session()\n",
    "    # Init variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    cost_val = []\n",
    "\n",
    "    t_begin = time.time()\n",
    "    # print(\"start training '{}'...\".format(flags_dataset))\n",
    "    # Train model\n",
    "    for epoch in range(flags_epochs):\n",
    "\n",
    "        t = time.time()\n",
    "        # Construct feed dictionary\n",
    "        feed_dict = construct_feed_dict(features, support, y_train, train_mask, placeholders)\n",
    "        feed_dict.update({placeholders['dropout']: flags_dropout})\n",
    "\n",
    "        # Training step\n",
    "        outs = sess.run([model.opt_op, model.loss, model.accuracy], feed_dict=feed_dict)\n",
    "\n",
    "        # Validation\n",
    "        cost, acc, duration = evaluate(features, support, y_val, val_mask, placeholders, sess, model)\n",
    "        cost_val.append(cost)\n",
    "\n",
    "        # Print results\n",
    "        # print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(outs[1]),\n",
    "        #     \"train_acc=\", \"{:.5f}\".format(outs[2]), \"val_loss=\", \"{:.5f}\".format(cost),\n",
    "        #     \"val_acc=\", \"{:.5f}\".format(acc), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "        if epoch % 5 == 0:\n",
    "            print(\".\", end=\"\")\n",
    "\n",
    "        if epoch > flags_early_stopping and cost_val[-1] > np.mean(cost_val[-(flags_early_stopping+1):-1]):\n",
    "            print(\"Early stopping...\")\n",
    "            break\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "    # print(\"Optimization Finished!\")\n",
    "\n",
    "    # print(\"total train time {:.5f}\".format(time.time() - t_begin))\n",
    "    duration = time.time() - t_begin\n",
    "\n",
    "    # Testing\n",
    "    test_cost, test_acc, test_duration = evaluate(features, support, y_test, test_mask, placeholders, sess, model)\n",
    "    sess.close()\n",
    "    return test_cost, test_acc, duration\n",
    "\n",
    "    # print(\"[{}][{}][{}] Test set results:\".format(flags_dataset, flags_act_func.__name__, flags_optimizer.__name__),\n",
    "    #       \"cost=\", \"{:.5f}\".format(test_cost),\n",
    "    #       \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(test_duration))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduce of Original Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resetAllRandomSeeds():\n",
    "    seed = 123\n",
    "    np.random.seed(seed)\n",
    "    tf.reset_default_graph() # reset session\n",
    "    tf.set_random_seed(seed)\n",
    "    tf.config.set_soft_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new model, seed= 123\n",
      "WARNING:tensorflow:From c:\\Users\\xuzhi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From c:\\Users\\xuzhi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\uiuc_mcs\\cs598-dl-healthcare-proj\\src\\gcn\\utils.py:70: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
      "f:\\uiuc_mcs\\cs598-dl-healthcare-proj\\src\\gcn\\utils.py:115: RuntimeWarning: divide by zero encountered in power\n",
      "  r_inv = np.power(rowsum, -1).flatten()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................................\n",
      "Test set results: cost=1.30971, accuracy=0.70600, time=2.87125\n"
     ]
    }
   ],
   "source": [
    "def loadDataAndTrain():\n",
    "     adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(flags_dataset)\n",
    "     return train(adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask)\n",
    "\n",
    "flags_layers = 2\n",
    "flags_dataset = 'citeseer'# , 'Dataset string.')  # 'cora', 'citeseer', 'pubmed'\n",
    "\n",
    "resetAllRandomSeeds()\n",
    "test_cost, test_acc, test_duration = loadDataAndTrain()\n",
    "\n",
    "print(\"Test set results: cost={cost:.5f}, accuracy={accuracy:.5f}, time={time:.5f}\".format(\n",
    "     cost=test_cost, accuracy=test_acc, time=test_duration))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology explanation and examples\n",
    "\n",
    "Here we loop through the three datasets and train the model on each of them with different combination of optimizer, activation function and number of layers. It will call function `train()` defined in the above cell to perform the training. We keep all the other parameters the same as the original paper. The `train()` function creates the GCN model and train it on the dataset. It will return the accuracy on the test set which will be later recorded in the Result section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list = ['cora', 'citeseer', 'pubmed']\n",
    "optimizer_list = [tf.train.AdamOptimizer, tf.train.GradientDescentOptimizer, tf.train.AdadeltaOptimizer, tf.train.RMSPropOptimizer]\n",
    "activation_list = [tf.nn.relu, tf.nn.leaky_relu, tf.nn.sigmoid, tf.nn.tanh, tf.nn.elu]\n",
    "lr_list = [0.01, 0.99]\n",
    "layers_list = [2, 1, 3]\n",
    "result = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== cora begin ===========\n",
      "+ [cora] activation function trial begin\n",
      "new model, seed= 123\n",
      "........................................\n",
      "+ + [cora][relu] Test set results: cost=1.01651, accuracy=0.81700, time=1.97206\n",
      "new model, seed= 123\n",
      "........................................\n",
      "+ + [cora][leaky_relu] Test set results: cost=1.01419, accuracy=0.81300, time=1.81067\n",
      "new model, seed= 123\n",
      "...Early stopping...\n",
      "\n",
      "+ + [cora][sigmoid] Test set results: cost=1.95441, accuracy=0.09300, time=0.19309\n",
      "new model, seed= 123\n",
      "........................................\n",
      "+ + [cora][tanh] Test set results: cost=0.97253, accuracy=0.81600, time=1.81478\n",
      "new model, seed= 123\n",
      "........................................\n",
      "+ + [cora][elu] Test set results: cost=0.97768, accuracy=0.81700, time=1.64820\n",
      "+ [cora] optimizer trial begin\n",
      "new model, seed= 123\n",
      "........................................\n",
      "+ + [cora][AdamOptimizer][0.01] Test set results: cost=1.01651, accuracy=0.81700, time=1.68475\n",
      "new model, seed= 123\n",
      "......Early stopping...\n",
      "\n",
      "+ + [cora][AdamOptimizer][0.99] Test set results: cost=2.62268, accuracy=0.72000, time=0.29326\n",
      "new model, seed= 123\n",
      "........................................\n",
      "+ + [cora][GradientDescentOptimizer][0.01] Test set results: cost=1.95306, accuracy=0.14400, time=1.69134\n",
      "new model, seed= 123\n",
      "........................................\n",
      "+ + [cora][GradientDescentOptimizer][0.99] Test set results: cost=1.90279, accuracy=0.44400, time=1.77429\n",
      "new model, seed= 123\n",
      "........................................\n",
      "+ + [cora][AdadeltaOptimizer][0.01] Test set results: cost=1.95313, accuracy=0.14500, time=1.70822\n",
      "new model, seed= 123\n",
      "........................................\n",
      "+ + [cora][AdadeltaOptimizer][0.99] Test set results: cost=1.92976, accuracy=0.38800, time=1.70445\n",
      "new model, seed= 123\n",
      "WARNING:tensorflow:From c:\\Users\\xuzhi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\rmsprop.py:188: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "........................................\n",
      "+ + [cora][RMSPropOptimizer][0.01] Test set results: cost=1.41405, accuracy=0.76000, time=1.69780\n",
      "new model, seed= 123\n",
      "................Early stopping...\n",
      "\n",
      "+ + [cora][RMSPropOptimizer][0.99] Test set results: cost=1.05851, accuracy=0.73000, time=0.77489\n",
      "+ [cora] layers trial begin\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'layers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m+ [\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m] layers trial begin\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(flags_dataset))\n\u001b[0;32m     37\u001b[0m result[flags_dataset][\u001b[39m'\u001b[39m\u001b[39mlayers\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m {}\n\u001b[1;32m---> 38\u001b[0m \u001b[39mfor\u001b[39;00m flags_layers \u001b[39min\u001b[39;00m layers:\n\u001b[0;32m     39\u001b[0m     resetAllRandomSeeds()\n\u001b[0;32m     40\u001b[0m     test_cost, test_acc, test_duration \u001b[39m=\u001b[39m loadDataAndTrain()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'layers' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\" RESET GLOBALS \"\"\"\n",
    "_LAYER_UIDS = {}\n",
    "\n",
    "for flags_dataset in dataset_list:\n",
    "    print(\"=========== {} begin ===========\".format(flags_dataset))\n",
    "    if flags_dataset not in result.keys():\n",
    "        result[flags_dataset] = {}\n",
    "    print(\"+ [{}] activation function trial begin\".format(flags_dataset))\n",
    "    result[flags_dataset]['activation'] = {}\n",
    "    for flags_act_func in activation_list:\n",
    "        # adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(flags_dataset)\n",
    "        # test_cost, test_acc, test_duration = train(adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask)\n",
    "        resetAllRandomSeeds()\n",
    "        test_cost, test_acc, test_duration = loadDataAndTrain()\n",
    "        print(\"+ + [{}][{}] Test set results: cost={cost:.5f}, accuracy={accuracy:.5f}, time={time:.5f}\".format(\n",
    "            flags_dataset, flags_act_func.__name__, cost=test_cost, accuracy=test_acc, time=test_duration))\n",
    "        result[flags_dataset]['activation'][flags_act_func.__name__] = {\"cost\": test_cost, \"accuracy\": test_acc, \"time\": test_duration}\n",
    "    # reset activation function\n",
    "    flags_act_func = tf.nn.relu\n",
    "\n",
    "    \n",
    "    print(\"+ [{}] optimizer trial begin\".format(flags_dataset))\n",
    "    result[flags_dataset]['optimizer'] = {}\n",
    "    for flags_optimizer in optimizer_list:\n",
    "        result[flags_dataset]['optimizer'][flags_optimizer.__name__] = []\n",
    "        for flags_learning_rate in lr_list:\n",
    "            resetAllRandomSeeds()\n",
    "            test_cost, test_acc, test_duration = loadDataAndTrain()\n",
    "            print(\"+ + [{}][{}][{lr}] Test set results: cost={cost:.5f}, accuracy={accuracy:.5f}, time={time:.5f}\".format(\n",
    "                flags_dataset, flags_optimizer.__name__, cost=test_cost, accuracy=test_acc, time=test_duration, lr=flags_learning_rate))\n",
    "            result[flags_dataset]['optimizer'][flags_optimizer.__name__].append({\"cost\": test_cost, \"accuracy\": test_acc, \"time\": test_duration})\n",
    "    # reset optimizer\n",
    "    flags_optimizer = tf.train.AdamOptimizer\n",
    "    flags_learning_rate = 0.01\n",
    "    \n",
    "    print(\"+ [{}] layers trial begin\".format(flags_dataset))\n",
    "    result[flags_dataset]['layers'] = {}\n",
    "    for flags_layers in layers_list:\n",
    "        resetAllRandomSeeds()\n",
    "        test_cost, test_acc, test_duration = loadDataAndTrain()\n",
    "        print(\"+ + [{}][{}] Test set results: cost={cost:.5f}, accuracy={accuracy:.5f}, time={time:.5f}\".format(\n",
    "            flags_dataset, flags_layers, cost=test_cost, accuracy=test_acc, time=test_duration))\n",
    "        result[flags_dataset]['layers'][flags_layers] = {\"cost\": test_cost, \"accuracy\": test_acc, \"time\": test_duration}\n",
    "    # reset number of layers\n",
    "    flags_layers = 2\n",
    "\n",
    "    print(\"=========== {} end ===========\".format(flags_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== cora begin ===========\n",
      "+ [cora] optimizer trial begin\n",
      "new model, seed= 123\n",
      "........................................\n",
      "+ + [cora][AdamOptimizer][0.01] Test set results: cost=1.01649, accuracy=0.81700, time=1.82474\n",
      "new model, seed= 123\n",
      "......Early stopping...\n",
      "\n",
      "+ + [cora][AdamOptimizer][0.99] Test set results: cost=2.61574, accuracy=0.72100, time=0.29472\n",
      "new model, seed= 123\n",
      "........................................\n",
      "+ + [cora][GradientDescentOptimizer][0.01] Test set results: cost=1.95306, accuracy=0.14400, time=1.81536\n",
      "new model, seed= 123\n",
      "........................................\n",
      "+ + [cora][GradientDescentOptimizer][0.99] Test set results: cost=1.90279, accuracy=0.44400, time=1.73021\n",
      "new model, seed= 123\n",
      "........................................\n",
      "+ + [cora][AdadeltaOptimizer][0.01] Test set results: cost=1.95313, accuracy=0.14500, time=1.73942\n",
      "new model, seed= 123\n",
      "........................................\n",
      "+ + [cora][AdadeltaOptimizer][0.99] Test set results: cost=1.92976, accuracy=0.38800, time=1.69093\n",
      "new model, seed= 123\n",
      "WARNING:tensorflow:From c:\\Users\\xuzhi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\rmsprop.py:188: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "........................................\n",
      "+ + [cora][RMSPropOptimizer][0.01] Test set results: cost=1.41405, accuracy=0.76000, time=1.77653\n",
      "new model, seed= 123\n",
      "................Early stopping...\n",
      "\n",
      "+ + [cora][RMSPropOptimizer][0.99] Test set results: cost=1.05817, accuracy=0.73000, time=0.72106\n",
      "=========== citeseer begin ===========\n",
      "+ [citeseer] optimizer trial begin\n",
      "new model, seed= 123\n",
      "........................................\n",
      "+ + [citeseer][AdamOptimizer][0.01] Test set results: cost=1.30973, accuracy=0.70600, time=2.48863\n",
      "new model, seed= 123\n",
      "....Early stopping...\n",
      "\n",
      "+ + [citeseer][AdamOptimizer][0.99] Test set results: cost=4.29122, accuracy=0.67900, time=0.31729\n",
      "new model, seed= 123\n",
      "........................................\n",
      "+ + [citeseer][GradientDescentOptimizer][0.01] Test set results: cost=1.79932, accuracy=0.26900, time=2.46899\n",
      "new model, seed= 123\n",
      "........................................\n",
      "+ + [citeseer][GradientDescentOptimizer][0.99] Test set results: cost=1.78069, accuracy=0.44500, time=2.41128\n",
      "new model, seed= 123\n",
      "........................................\n",
      "+ + [citeseer][AdadeltaOptimizer][0.01] Test set results: cost=1.79933, accuracy=0.26800, time=2.41646\n",
      "new model, seed= 123\n",
      "........................................\n",
      "+ + [citeseer][AdadeltaOptimizer][0.99] Test set results: cost=1.78703, accuracy=0.43300, time=2.49410\n",
      "new model, seed= 123\n",
      "........................................\n",
      "+ + [citeseer][RMSPropOptimizer][0.01] Test set results: cost=1.55498, accuracy=0.66000, time=2.49843\n",
      "new model, seed= 123\n",
      ".................Early stopping...\n",
      "\n",
      "+ + [citeseer][RMSPropOptimizer][0.99] Test set results: cost=1.13254, accuracy=0.69700, time=1.21242\n",
      "=========== pubmed begin ===========\n",
      "+ [pubmed] optimizer trial begin\n",
      "new model, seed= 123\n",
      "...............................Early stopping...\n",
      "\n",
      "+ + [pubmed][AdamOptimizer][0.01] Test set results: cost=0.72828, accuracy=0.79400, time=9.65529\n",
      "new model, seed= 123\n",
      "...Early stopping...\n",
      "\n",
      "+ + [pubmed][AdamOptimizer][0.99] Test set results: cost=1.96890, accuracy=0.66600, time=0.92204\n",
      "new model, seed= 123\n",
      "........................................\n",
      "+ + [pubmed][GradientDescentOptimizer][0.01] Test set results: cost=1.10629, accuracy=0.32800, time=12.47285\n",
      "new model, seed= 123\n",
      "........................................\n",
      "+ + [pubmed][GradientDescentOptimizer][0.99] Test set results: cost=1.04026, accuracy=0.54700, time=12.59660\n",
      "new model, seed= 123\n",
      "........................................\n",
      "+ + [pubmed][AdadeltaOptimizer][0.01] Test set results: cost=1.10631, accuracy=0.32700, time=12.47099\n",
      "new model, seed= 123\n",
      "........................................\n",
      "+ + [pubmed][AdadeltaOptimizer][0.99] Test set results: cost=1.08722, accuracy=0.60800, time=12.48596\n",
      "new model, seed= 123\n",
      "........................................\n",
      "+ + [pubmed][RMSPropOptimizer][0.01] Test set results: cost=0.78005, accuracy=0.77400, time=12.60819\n",
      "new model, seed= 123\n",
      "...............Early stopping...\n",
      "\n",
      "+ + [pubmed][RMSPropOptimizer][0.99] Test set results: cost=0.69200, accuracy=0.76500, time=4.77621\n"
     ]
    }
   ],
   "source": [
    "for flags_dataset in dataset_list:\n",
    "    print(\"=========== {} begin ===========\".format(flags_dataset))\n",
    "    if flags_dataset not in result.keys():\n",
    "        result[flags_dataset] = {}\n",
    "    # print(\"+ [{}] activation function trial begin\".format(flags_dataset))\n",
    "    # result[flags_dataset]['activation'] = {}\n",
    "    # for flags_act_func in activation_list:\n",
    "    #     # adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(flags_dataset)\n",
    "    #     # test_cost, test_acc, test_duration = train(adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask)\n",
    "    #     resetAllRandomSeeds()\n",
    "    #     test_cost, test_acc, test_duration = loadDataAndTrain()\n",
    "    #     print(\"+ + [{}][{}] Test set results: cost={cost:.5f}, accuracy={accuracy:.5f}, time={time:.5f}\".format(\n",
    "    #         flags_dataset, flags_act_func.__name__, cost=test_cost, accuracy=test_acc, time=test_duration))\n",
    "    #     result[flags_dataset]['activation'][flags_act_func.__name__] = {\"cost\": test_cost, \"accuracy\": test_acc, \"time\": test_duration}\n",
    "    # # reset activation function\n",
    "    # flags_act_func = tf.nn.relu\n",
    "\n",
    "    \n",
    "    print(\"+ [{}] optimizer trial begin\".format(flags_dataset))\n",
    "    result[flags_dataset]['optimizer'] = {}\n",
    "    for flags_optimizer in optimizer_list:\n",
    "        result[flags_dataset]['optimizer'][flags_optimizer.__name__] = []\n",
    "        for flags_learning_rate in lr_list:\n",
    "            resetAllRandomSeeds()\n",
    "            test_cost, test_acc, test_duration = loadDataAndTrain()\n",
    "            print(\"+ + [{}][{}][{lr}] Test set results: cost={cost:.5f}, accuracy={accuracy:.5f}, time={time:.5f}\".format(\n",
    "                flags_dataset, flags_optimizer.__name__, cost=test_cost, accuracy=test_acc, time=test_duration, lr=flags_learning_rate))\n",
    "            result[flags_dataset]['optimizer'][flags_optimizer.__name__].append({\"cost\": test_cost, \"accuracy\": test_acc, \"time\": test_duration})\n",
    "    # reset optimizer\n",
    "    flags_optimizer = tf.train.AdamOptimizer\n",
    "    \n",
    "    # print(\"+ [{}] layers trial begin\".format(flags_dataset))\n",
    "    # result[flags_dataset]['layers'] = {}\n",
    "    # for flags_layers in layers:\n",
    "    #     resetAllRandomSeeds()\n",
    "    #     test_cost, test_acc, test_duration = loadDataAndTrain()\n",
    "    #     print(\"+ + [{}][{}] Test set results: cost={cost:.5f}, accuracy={accuracy:.5f}, time={time:.5f}\".format(\n",
    "    #         flags_dataset, flags_layers, cost=test_cost, accuracy=test_acc, time=test_duration))\n",
    "    #     result[flags_dataset]['layers'][flags_layers] = {\"cost\": test_cost, \"accuracy\": test_acc, \"time\": test_duration}\n",
    "    # reset number of layers\n",
    "    flags_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== cora begin ===========\n",
      "+ [cora] layers trial begin\n",
      "new model, seed= 123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\uiuc_mcs\\cs598-dl-healthcare-proj\\src\\gcn\\utils.py:70: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......Early stopping...\n",
      "\n",
      "+ + [cora][2] Test set results: cost=2.62050, accuracy=0.71700, time=0.40019\n",
      "new model, seed= 123\n",
      ".....Early stopping...\n",
      "\n",
      "+ + [cora][1] Test set results: cost=2.12601, accuracy=0.72800, time=0.28443\n",
      "new model, seed= 123\n",
      ".......Early stopping...\n",
      "\n",
      "+ + [cora][3] Test set results: cost=11.04254, accuracy=0.26700, time=0.37514\n",
      "=========== citeseer begin ===========\n",
      "+ [citeseer] layers trial begin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\uiuc_mcs\\cs598-dl-healthcare-proj\\src\\gcn\\utils.py:115: RuntimeWarning: divide by zero encountered in power\n",
      "  r_inv = np.power(rowsum, -1).flatten()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new model, seed= 123\n",
      "....Early stopping...\n",
      "\n",
      "+ + [citeseer][2] Test set results: cost=4.29122, accuracy=0.67900, time=0.27195\n",
      "new model, seed= 123\n",
      ".....Early stopping...\n",
      "\n",
      "+ + [citeseer][1] Test set results: cost=2.22939, accuracy=0.65300, time=0.38951\n",
      "new model, seed= 123\n",
      "......Early stopping...\n",
      "\n",
      "+ + [citeseer][3] Test set results: cost=8.93039, accuracy=0.09100, time=0.51587\n",
      "=========== pubmed begin ===========\n",
      "+ [pubmed] layers trial begin\n",
      "new model, seed= 123\n",
      "...Early stopping...\n",
      "\n",
      "+ + [pubmed][2] Test set results: cost=1.96890, accuracy=0.66600, time=0.96230\n",
      "new model, seed= 123\n",
      "...........Early stopping...\n",
      "\n",
      "+ + [pubmed][1] Test set results: cost=1.12308, accuracy=0.74000, time=3.24572\n",
      "new model, seed= 123\n",
      "........Early stopping...\n",
      "\n",
      "+ + [pubmed][3] Test set results: cost=5.44003, accuracy=0.18000, time=2.63489\n"
     ]
    }
   ],
   "source": [
    "for flags_dataset in dataset_list:\n",
    "    print(\"=========== {} begin ===========\".format(flags_dataset))\n",
    "    if flags_dataset not in result.keys():\n",
    "        result[flags_dataset] = {}\n",
    "    # print(\"+ [{}] activation function trial begin\".format(flags_dataset))\n",
    "    # result[flags_dataset]['activation'] = {}\n",
    "    # for flags_act_func in activation_list:\n",
    "    #     # adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(flags_dataset)\n",
    "    #     # test_cost, test_acc, test_duration = train(adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask)\n",
    "    #     resetAllRandomSeeds()\n",
    "    #     test_cost, test_acc, test_duration = loadDataAndTrain()\n",
    "    #     print(\"+ + [{}][{}] Test set results: cost={cost:.5f}, accuracy={accuracy:.5f}, time={time:.5f}\".format(\n",
    "    #         flags_dataset, flags_act_func.__name__, cost=test_cost, accuracy=test_acc, time=test_duration))\n",
    "    #     result[flags_dataset]['activation'][flags_act_func.__name__] = {\"cost\": test_cost, \"accuracy\": test_acc, \"time\": test_duration}\n",
    "    # # reset activation function\n",
    "    # flags_act_func = tf.nn.relu\n",
    "\n",
    "    \n",
    "    # print(\"+ [{}] optimizer trial begin\".format(flags_dataset))\n",
    "    # result[flags_dataset]['optimizer'] = {}\n",
    "    # for flags_optimizer in optimizer_list:\n",
    "    #     result[flags_dataset]['optimizer'][flags_optimizer.__name__] = []\n",
    "    #     for flags_learning_rate in lr_list:\n",
    "    #         resetAllRandomSeeds()\n",
    "    #         test_cost, test_acc, test_duration = loadDataAndTrain()\n",
    "    #         print(\"+ + [{}][{}][{lr}] Test set results: cost={cost:.5f}, accuracy={accuracy:.5f}, time={time:.5f}\".format(\n",
    "    #             flags_dataset, flags_optimizer.__name__, cost=test_cost, accuracy=test_acc, time=test_duration, lr=flags_learning_rate))\n",
    "    #         result[flags_dataset]['optimizer'][flags_optimizer.__name__].append({\"cost\": test_cost, \"accuracy\": test_acc, \"time\": test_duration})\n",
    "    # # reset optimizer\n",
    "    # flags_optimizer = tf.train.AdamOptimizer\n",
    "    \n",
    "    print(\"+ [{}] layers trial begin\".format(flags_dataset))\n",
    "    result[flags_dataset]['layers'] = {}\n",
    "    for flags_layers in layers_list:\n",
    "        resetAllRandomSeeds()\n",
    "        test_cost, test_acc, test_duration = loadDataAndTrain()\n",
    "        print(\"+ + [{}][{}] Test set results: cost={cost:.5f}, accuracy={accuracy:.5f}, time={time:.5f}\".format(\n",
    "            flags_dataset, flags_layers, cost=test_cost, accuracy=test_acc, time=test_duration))\n",
    "        result[flags_dataset]['layers'][flags_layers] = {\"cost\": test_cost, \"accuracy\": test_acc, \"time\": test_duration}\n",
    "    # reset number of layers\n",
    "    flags_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: {'cost': 2.6193116, 'accuracy': 0.719, 'time': 0.3068504333496094}, 1: {'cost': 2.1260145, 'accuracy': 0.728, 'time': 0.23398423194885254}, 3: {'cost': 6.8418922, 'accuracy': 0.269, 'time': 0.6258764266967773}}\n"
     ]
    }
   ],
   "source": [
    "print(result['cora']['layers'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "TODO: A summary of the key results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cora\n",
      "+ activation\n",
      "+ + relu\n",
      "+ + + cost 1.0165104\n",
      "+ + + accuracy 0.8169999\n",
      "+ + + time 1.618004560470581\n",
      "+ + leaky_relu\n",
      "+ + + cost 1.0141885\n",
      "+ + + accuracy 0.813\n",
      "+ + + time 1.6548216342926025\n",
      "+ + sigmoid\n",
      "+ + + cost 1.9544058\n",
      "+ + + accuracy 0.092999995\n",
      "+ + + time 0.180633544921875\n",
      "+ + tanh\n",
      "+ + + cost 0.9725252\n",
      "+ + + accuracy 0.816\n",
      "+ + + time 1.7491328716278076\n",
      "+ + elu\n",
      "+ + + cost 0.9776821\n",
      "+ + + accuracy 0.81700003\n",
      "+ + + time 1.7887444496154785\n",
      "+ optimizer\n",
      "+ + AdamOptimizer\n",
      "+ + + lr=0.01\n",
      "+ + + + cost 1.0164897\n",
      "+ + + + accuracy 0.8169999\n",
      "+ + + + time 1.824744701385498\n",
      "+ + + lr=0.99\n",
      "+ + + + cost 2.6157386\n",
      "+ + + + accuracy 0.72099996\n",
      "+ + + + time 0.29471731185913086\n",
      "+ + GradientDescentOptimizer\n",
      "+ + + lr=0.01\n",
      "+ + + + cost 1.9530557\n",
      "+ + + + accuracy 0.144\n",
      "+ + + + time 1.8153603076934814\n",
      "+ + + lr=0.99\n",
      "+ + + + cost 1.9027857\n",
      "+ + + + accuracy 0.44399998\n",
      "+ + + + time 1.7302136421203613\n",
      "+ + AdadeltaOptimizer\n",
      "+ + + lr=0.01\n",
      "+ + + + cost 1.9531313\n",
      "+ + + + accuracy 0.145\n",
      "+ + + + time 1.739422082901001\n",
      "+ + + lr=0.99\n",
      "+ + + + cost 1.9297608\n",
      "+ + + + accuracy 0.38799998\n",
      "+ + + + time 1.6909289360046387\n",
      "+ + RMSPropOptimizer\n",
      "+ + + lr=0.01\n",
      "+ + + + cost 1.4140474\n",
      "+ + + + accuracy 0.76000005\n",
      "+ + + + time 1.7765295505523682\n",
      "+ + + lr=0.99\n",
      "+ + + + cost 1.0581666\n",
      "+ + + + accuracy 0.72999996\n",
      "+ + + + time 0.7210595607757568\n",
      "+ layers\n",
      "+ + 2\n",
      "+ + + cost 2.6193116\n",
      "+ + + accuracy 0.719\n",
      "+ + + time 0.3068504333496094\n",
      "+ + 1\n",
      "+ + + cost 2.1260145\n",
      "+ + + accuracy 0.728\n",
      "+ + + time 0.23398423194885254\n",
      "+ + 3\n",
      "+ + + cost 6.8418922\n",
      "+ + + accuracy 0.269\n",
      "+ + + time 0.6258764266967773\n",
      "citeseer\n",
      "+ activation\n",
      "+ + relu\n",
      "+ + + cost 1.3097099\n",
      "+ + + accuracy 0.706\n",
      "+ + + time 2.323282480239868\n",
      "+ + leaky_relu\n",
      "+ + + cost 1.3057288\n",
      "+ + + accuracy 0.70400006\n",
      "+ + + time 2.5206780433654785\n",
      "+ + sigmoid\n",
      "+ + + cost 1.8164393\n",
      "+ + + accuracy 0.07700001\n",
      "+ + + time 0.2772049903869629\n",
      "+ + tanh\n",
      "+ + + cost 1.263225\n",
      "+ + + accuracy 0.704\n",
      "+ + + time 2.5095863342285156\n",
      "+ + elu\n",
      "+ + + cost 1.2652336\n",
      "+ + + accuracy 0.70100003\n",
      "+ + + time 2.4142062664031982\n",
      "+ optimizer\n",
      "+ + AdamOptimizer\n",
      "+ + + lr=0.01\n",
      "+ + + + cost 1.3097309\n",
      "+ + + + accuracy 0.706\n",
      "+ + + + time 2.4886319637298584\n",
      "+ + + lr=0.99\n",
      "+ + + + cost 4.2912235\n",
      "+ + + + accuracy 0.6790001\n",
      "+ + + + time 0.3172931671142578\n",
      "+ + GradientDescentOptimizer\n",
      "+ + + lr=0.01\n",
      "+ + + + cost 1.7993151\n",
      "+ + + + accuracy 0.269\n",
      "+ + + + time 2.468993663787842\n",
      "+ + + lr=0.99\n",
      "+ + + + cost 1.7806909\n",
      "+ + + + accuracy 0.445\n",
      "+ + + + time 2.411280632019043\n",
      "+ + AdadeltaOptimizer\n",
      "+ + + lr=0.01\n",
      "+ + + + cost 1.7993264\n",
      "+ + + + accuracy 0.268\n",
      "+ + + + time 2.4164562225341797\n",
      "+ + + lr=0.99\n",
      "+ + + + cost 1.7870291\n",
      "+ + + + accuracy 0.43299997\n",
      "+ + + + time 2.49410080909729\n",
      "+ + RMSPropOptimizer\n",
      "+ + + lr=0.01\n",
      "+ + + + cost 1.5549834\n",
      "+ + + + accuracy 0.66\n",
      "+ + + + time 2.498431921005249\n",
      "+ + + lr=0.99\n",
      "+ + + + cost 1.132539\n",
      "+ + + + accuracy 0.69699997\n",
      "+ + + + time 1.2124230861663818\n",
      "+ layers\n",
      "+ + 2\n",
      "+ + + cost 4.2912073\n",
      "+ + + accuracy 0.6790001\n",
      "+ + + time 0.2635495662689209\n",
      "+ + 1\n",
      "+ + + cost 2.2293935\n",
      "+ + + accuracy 0.653\n",
      "+ + + time 0.3091874122619629\n",
      "+ + 3\n",
      "+ + + cost 8.820029\n",
      "+ + + accuracy 0.078\n",
      "+ + + time 0.5502688884735107\n",
      "pubmed\n",
      "+ activation\n",
      "+ + relu\n",
      "+ + + cost 0.72828436\n",
      "+ + + accuracy 0.79399997\n",
      "+ + + time 9.653547525405884\n",
      "+ + leaky_relu\n",
      "+ + + cost 0.7296622\n",
      "+ + + accuracy 0.7889999\n",
      "+ + + time 10.060568571090698\n",
      "+ + sigmoid\n",
      "+ + + cost 1.1090643\n",
      "+ + + accuracy 0.21799998\n",
      "+ + + time 1.5433979034423828\n",
      "+ + tanh\n",
      "+ + + cost 0.7081878\n",
      "+ + + accuracy 0.793\n",
      "+ + + time 10.012193202972412\n",
      "+ + elu\n",
      "+ + + cost 0.71048594\n",
      "+ + + accuracy 0.79099995\n",
      "+ + + time 9.981307983398438\n",
      "+ optimizer\n",
      "+ + AdamOptimizer\n",
      "+ + + lr=0.01\n",
      "+ + + + cost 0.72828436\n",
      "+ + + + accuracy 0.79399997\n",
      "+ + + + time 9.655287981033325\n",
      "+ + + lr=0.99\n",
      "+ + + + cost 1.9689028\n",
      "+ + + + accuracy 0.66599995\n",
      "+ + + + time 0.9220385551452637\n",
      "+ + GradientDescentOptimizer\n",
      "+ + + lr=0.01\n",
      "+ + + + cost 1.1062919\n",
      "+ + + + accuracy 0.32799998\n",
      "+ + + + time 12.472854614257812\n",
      "+ + + lr=0.99\n",
      "+ + + + cost 1.0402629\n",
      "+ + + + accuracy 0.547\n",
      "+ + + + time 12.596595764160156\n",
      "+ + AdadeltaOptimizer\n",
      "+ + + lr=0.01\n",
      "+ + + + cost 1.1063143\n",
      "+ + + + accuracy 0.327\n",
      "+ + + + time 12.470990419387817\n",
      "+ + + lr=0.99\n",
      "+ + + + cost 1.0872197\n",
      "+ + + + accuracy 0.608\n",
      "+ + + + time 12.485963344573975\n",
      "+ + RMSPropOptimizer\n",
      "+ + + lr=0.01\n",
      "+ + + + cost 0.78004575\n",
      "+ + + + accuracy 0.774\n",
      "+ + + + time 12.608192682266235\n",
      "+ + + lr=0.99\n",
      "+ + + + cost 0.6920025\n",
      "+ + + + accuracy 0.7649999\n",
      "+ + + + time 4.776209592819214\n",
      "+ layers\n",
      "+ + 2\n",
      "+ + + cost 1.968904\n",
      "+ + + accuracy 0.66599995\n",
      "+ + + time 0.9404304027557373\n",
      "+ + 1\n",
      "+ + + cost 1.1230756\n",
      "+ + + accuracy 0.73999995\n",
      "+ + + time 3.2468395233154297\n",
      "+ + 3\n",
      "+ + + cost 5.4401913\n",
      "+ + + accuracy 0.17999999\n",
      "+ + + time 2.690995454788208\n"
     ]
    }
   ],
   "source": [
    "for dataset in result.keys():\n",
    "    print(dataset)\n",
    "    for ablation in result[dataset]:\n",
    "        print(\"+\", ablation)\n",
    "        if ablation == 'optimizer':\n",
    "            for opt in result[dataset][ablation]:\n",
    "                print(\"+\", \"+\", opt)\n",
    "                for lr_idx in range(len(result[dataset][ablation][opt])):\n",
    "                    print(\"+\", \"+\", \"+\", \"lr={}\".format(lr_list[lr_idx]))\n",
    "                    for key, val in result[dataset][ablation][opt][lr_idx].items():\n",
    "                        print(\"+\", \"+\", \"+\", \"+\", key, val)\n",
    "        if ablation == 'activation':\n",
    "            for act in result[dataset][ablation]:\n",
    "                print(\"+\", \"+\", act)\n",
    "                for key, val in result[dataset][ablation][act].items():\n",
    "                    print(\"+\", \"+\", \"+\", key, val)\n",
    "        if ablation == \"layers\":\n",
    "            for layer in result[dataset][ablation]:\n",
    "                print(\"+\", \"+\", layer)\n",
    "                for key, val in result[dataset][ablation][layer].items():\n",
    "                    print(\"+\", \"+\", \"+\", key, val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "TODO: add references"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
