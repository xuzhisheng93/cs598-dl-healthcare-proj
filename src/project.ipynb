{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter notebook is created to reproduce the results and experiment abalations of paper \"SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS\" by Thomas N. Kipf and Max Welling. It was published on \"Proceedings of the International Conference on Learning Representations (ICLR)\" and can be found at https://arxiv.org/pdf/1609.02907.pdf.\n",
    "\n",
    "The authors of the paper proposed a semi-supervised classification algorithm based on graph convolutional networks (GCNs). And the original code can be found at https://github.com/tkipf/gcn. In this notebook, we reused most of the code and made a few modifications. \n",
    "\n",
    "The main modifications are:\n",
    "1. We modified the code so that it works in compatibility mode with tensorflow 2.12.0.\n",
    "2. We put the original code except `train.py` under `src/gcn` directory. And they were modified to be compatible with tensorflow 2.12.0.\n",
    "3. TODO: ablations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Download"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is included in the original GCN repository. Thus, there is no need to download it again. It can be found under `gcn/data` directory."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducibility Summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running this notebook, we are able to reproduce the results of GCN on all three datasets. It achieves the same accuracy as the original paper. TODO: ablations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcn.inits import *\n",
    "from gcn.utils import *\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "# flags = tf.compat.v1.flags\n",
    "# FLAGS = flags.FLAGS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Layers Class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global unique layer ID dictionary for layer name assignment\n",
    "_LAYER_UIDS = {}\n",
    "\n",
    "\n",
    "def get_layer_uid(layer_name=''):\n",
    "    \"\"\"Helper function, assigns unique layer IDs.\"\"\"\n",
    "    if layer_name not in _LAYER_UIDS:\n",
    "        _LAYER_UIDS[layer_name] = 1\n",
    "        return 1\n",
    "    else:\n",
    "        _LAYER_UIDS[layer_name] += 1\n",
    "        return _LAYER_UIDS[layer_name]\n",
    "\n",
    "\n",
    "def sparse_dropout(x, keep_prob, noise_shape):\n",
    "    \"\"\"Dropout for sparse tensors.\"\"\"\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += tf.random_uniform(noise_shape)\n",
    "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
    "    pre_out = tf.sparse_retain(x, dropout_mask)\n",
    "    return pre_out * (1./keep_prob)\n",
    "\n",
    "\n",
    "def dot(x, y, sparse=False):\n",
    "    \"\"\"Wrapper for tf.matmul (sparse vs dense).\"\"\"\n",
    "    if sparse:\n",
    "        res = tf.sparse_tensor_dense_matmul(x, y)\n",
    "    else:\n",
    "        res = tf.matmul(x, y)\n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Layer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    \"\"\"Base layer class. Defines basic API for all layer objects.\n",
    "    Implementation inspired by keras (http://keras.io).\n",
    "\n",
    "    # Properties\n",
    "        name: String, defines the variable scope of the layer.\n",
    "        logging: Boolean, switches Tensorflow histogram logging on/off\n",
    "\n",
    "    # Methods\n",
    "        _call(inputs): Defines computation graph of layer\n",
    "            (i.e. takes input, returns output)\n",
    "        __call__(inputs): Wrapper for _call()\n",
    "        _log_vars(): Log all variables\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        allowed_kwargs = {'name', 'logging'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            layer = self.__class__.__name__.lower()\n",
    "            name = layer + '_' + str(get_layer_uid(layer))\n",
    "        self.name = name\n",
    "        self.vars = {}\n",
    "        logging = kwargs.get('logging', False)\n",
    "        self.logging = logging\n",
    "        self.sparse_inputs = False\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        return inputs\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.name_scope(self.name):\n",
    "            if self.logging and not self.sparse_inputs:\n",
    "                tf.summary.histogram(self.name + '/inputs', inputs)\n",
    "            outputs = self._call(inputs)\n",
    "            if self.logging:\n",
    "                tf.summary.histogram(self.name + '/outputs', outputs)\n",
    "            return outputs\n",
    "\n",
    "    def _log_vars(self):\n",
    "        for var in self.vars:\n",
    "            tf.summary.histogram(self.name + '/vars/' + var, self.vars[var])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    \"\"\"Dense layer.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, placeholders, dropout=0., sparse_inputs=False,\n",
    "                 act=tf.nn.relu, bias=False, featureless=False, **kwargs):\n",
    "        super(Dense, self).__init__(**kwargs)\n",
    "\n",
    "        if dropout:\n",
    "            self.dropout = placeholders['dropout']\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "\n",
    "        self.act = act\n",
    "        self.sparse_inputs = sparse_inputs\n",
    "        self.featureless = featureless\n",
    "        self.bias = bias\n",
    "\n",
    "        # helper variable for sparse dropout\n",
    "        self.num_features_nonzero = placeholders['num_features_nonzero']\n",
    "\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            self.vars['weights'] = glorot([input_dim, output_dim],\n",
    "                                          name='weights')\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([output_dim], name='bias')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        # dropout\n",
    "        if self.sparse_inputs:\n",
    "            x = sparse_dropout(x, 1-self.dropout, self.num_features_nonzero)\n",
    "        else:\n",
    "            x = tf.nn.dropout(x, 1-self.dropout)\n",
    "\n",
    "        # transform\n",
    "        output = dot(x, self.vars['weights'], sparse=self.sparse_inputs)\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "\n",
    "        return self.act(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Graph Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(Layer):\n",
    "    \"\"\"Graph convolution layer.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, placeholders, dropout=0.,\n",
    "                 sparse_inputs=False, act=tf.nn.relu, bias=False,\n",
    "                 featureless=False, **kwargs):\n",
    "        super(GraphConvolution, self).__init__(**kwargs)\n",
    "\n",
    "        if dropout:\n",
    "            self.dropout = placeholders['dropout']\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "\n",
    "        self.act = act\n",
    "        self.support = placeholders['support']\n",
    "        self.sparse_inputs = sparse_inputs\n",
    "        self.featureless = featureless\n",
    "        self.bias = bias\n",
    "\n",
    "        # helper variable for sparse dropout\n",
    "        self.num_features_nonzero = placeholders['num_features_nonzero']\n",
    "\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            for i in range(len(self.support)):\n",
    "                self.vars['weights_' + str(i)] = glorot([input_dim, output_dim],\n",
    "                                                        name='weights_' + str(i))\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([output_dim], name='bias')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        # dropout\n",
    "        if self.sparse_inputs:\n",
    "            x = sparse_dropout(x, 1-self.dropout, self.num_features_nonzero)\n",
    "        else:\n",
    "            x = tf.nn.dropout(x, 1-self.dropout)\n",
    "\n",
    "        # convolve\n",
    "        supports = list()\n",
    "        for i in range(len(self.support)):\n",
    "            if not self.featureless:\n",
    "                pre_sup = dot(x, self.vars['weights_' + str(i)],\n",
    "                              sparse=self.sparse_inputs)\n",
    "            else:\n",
    "                pre_sup = self.vars['weights_' + str(i)]\n",
    "            support = dot(self.support[i], pre_sup, sparse=True)\n",
    "            supports.append(support)\n",
    "        output = tf.add_n(supports)\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "\n",
    "        return self.act(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcn.metrics import *\n",
    "\n",
    "# flags = tf.compat.v1.flags\n",
    "# FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        allowed_kwargs = {'name', 'logging'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            name = self.__class__.__name__.lower()\n",
    "        self.name = name\n",
    "\n",
    "        logging = kwargs.get('logging', False)\n",
    "        self.logging = logging\n",
    "\n",
    "        self.vars = {}\n",
    "        self.placeholders = {}\n",
    "\n",
    "        self.layers = []\n",
    "        self.activations = []\n",
    "\n",
    "        self.inputs = None\n",
    "        self.outputs = None\n",
    "\n",
    "        self.loss = 0\n",
    "        self.accuracy = 0\n",
    "        self.optimizer = None\n",
    "        self.opt_op = None\n",
    "\n",
    "    def _build(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\" Wrapper for _build() \"\"\"\n",
    "        with tf.variable_scope(self.name):\n",
    "            self._build()\n",
    "\n",
    "        # Build sequential layer model\n",
    "        self.activations.append(self.inputs)\n",
    "        for layer in self.layers:\n",
    "            hidden = layer(self.activations[-1])\n",
    "            self.activations.append(hidden)\n",
    "        self.outputs = self.activations[-1]\n",
    "\n",
    "        # Store model variables for easy access\n",
    "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
    "        self.vars = {var.name: var for var in variables}\n",
    "\n",
    "        # Build metrics\n",
    "        self._loss()\n",
    "        self._accuracy()\n",
    "\n",
    "        self.opt_op = self.optimizer.minimize(self.loss)\n",
    "\n",
    "    def predict(self):\n",
    "        pass\n",
    "\n",
    "    def _loss(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _accuracy(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def save(self, sess=None):\n",
    "        if not sess:\n",
    "            raise AttributeError(\"TensorFlow session not provided.\")\n",
    "        saver = tf.train.Saver(self.vars)\n",
    "        save_path = saver.save(sess, \"tmp/%s.ckpt\" % self.name)\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "    def load(self, sess=None):\n",
    "        if not sess:\n",
    "            raise AttributeError(\"TensorFlow session not provided.\")\n",
    "        saver = tf.train.Saver(self.vars)\n",
    "        save_path = \"tmp/%s.ckpt\" % self.name\n",
    "        saver.restore(sess, save_path)\n",
    "        print(\"Model restored from file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(Model):\n",
    "    def __init__(self, placeholders, input_dim, **kwargs):\n",
    "        super(MLP, self).__init__(**kwargs)\n",
    "\n",
    "        self.inputs = placeholders['features']\n",
    "        self.input_dim = input_dim\n",
    "        # self.input_dim = self.inputs.get_shape().as_list()[1]  # To be supported in future Tensorflow versions\n",
    "        self.output_dim = placeholders['labels'].get_shape().as_list()[1]\n",
    "        self.placeholders = placeholders\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=flags_learning_rate)\n",
    "\n",
    "        self.build()\n",
    "\n",
    "    def _loss(self):\n",
    "        # Weight decay loss\n",
    "        for var in self.layers[0].vars.values():\n",
    "            self.loss += flags_weight_decay * tf.nn.l2_loss(var)\n",
    "\n",
    "        # Cross entropy error\n",
    "        self.loss += masked_softmax_cross_entropy(self.outputs, self.placeholders['labels'],\n",
    "                                                  self.placeholders['labels_mask'])\n",
    "\n",
    "    def _accuracy(self):\n",
    "        self.accuracy = masked_accuracy(self.outputs, self.placeholders['labels'],\n",
    "                                        self.placeholders['labels_mask'])\n",
    "\n",
    "    def _build(self):\n",
    "        self.layers.append(Dense(input_dim=self.input_dim,\n",
    "                                 output_dim=flags_hidden1,\n",
    "                                 placeholders=self.placeholders,\n",
    "                                 act=tf.nn.relu,\n",
    "                                 dropout=True,\n",
    "                                 sparse_inputs=True,\n",
    "                                 logging=self.logging))\n",
    "\n",
    "        self.layers.append(Dense(input_dim=flags_hidden1,\n",
    "                                 output_dim=self.output_dim,\n",
    "                                 placeholders=self.placeholders,\n",
    "                                 act=lambda x: x,\n",
    "                                 dropout=True,\n",
    "                                 logging=self.logging))\n",
    "\n",
    "    def predict(self):\n",
    "        return tf.nn.softmax(self.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(Model):\n",
    "    def __init__(self, placeholders, input_dim, **kwargs):\n",
    "        super(GCN, self).__init__(**kwargs)\n",
    "\n",
    "        self.inputs = placeholders['features']\n",
    "        self.input_dim = input_dim\n",
    "        # self.input_dim = self.inputs.get_shape().as_list()[1]  # To be supported in future Tensorflow versions\n",
    "        self.output_dim = placeholders['labels'].get_shape().as_list()[1]\n",
    "        self.placeholders = placeholders\n",
    "\n",
    "        self.optimizer = flags_optimizer(learning_rate=flags_learning_rate)\n",
    "\n",
    "        self.build()\n",
    "\n",
    "    def _loss(self):\n",
    "        # Weight decay loss\n",
    "        for var in self.layers[0].vars.values():\n",
    "            self.loss += flags_weight_decay * tf.nn.l2_loss(var)\n",
    "\n",
    "        # Cross entropy error\n",
    "        self.loss += masked_softmax_cross_entropy(self.outputs, self.placeholders['labels'],\n",
    "                                                  self.placeholders['labels_mask'])\n",
    "\n",
    "    def _accuracy(self):\n",
    "        self.accuracy = masked_accuracy(self.outputs, self.placeholders['labels'],\n",
    "                                        self.placeholders['labels_mask'])\n",
    "\n",
    "    def _build(self):\n",
    "\n",
    "        if flags_layers == 2:\n",
    "            # Paper layer configuration\n",
    "            self.layers.append(GraphConvolution(input_dim=self.input_dim,\n",
    "                                                output_dim=flags_hidden1,\n",
    "                                                placeholders=self.placeholders,\n",
    "                                                act=flags_act_func,\n",
    "                                                dropout=True,\n",
    "                                                sparse_inputs=True,\n",
    "                                                logging=self.logging))\n",
    "\n",
    "            self.layers.append(GraphConvolution(input_dim=flags_hidden1,\n",
    "                                                output_dim=self.output_dim,\n",
    "                                                placeholders=self.placeholders,\n",
    "                                                act=lambda x: x,\n",
    "                                                dropout=True,\n",
    "                                                logging=self.logging))\n",
    "        elif flags_layers == 1:\n",
    "            # Single layer configuration\n",
    "            self.layers.append(GraphConvolution(input_dim=self.input_dim,\n",
    "                                                output_dim=self.output_dim,\n",
    "                                                placeholders=self.placeholders,\n",
    "                                                act=flags_act_func,\n",
    "                                                dropout=True,\n",
    "                                                sparse_inputs=True,\n",
    "                                                logging=self.logging))\n",
    "        elif flags_layers == 3:\n",
    "            # Triple layer configuration\n",
    "            self.layers.append(GraphConvolution(input_dim=self.input_dim,\n",
    "                                                output_dim=64,\n",
    "                                                placeholders=self.placeholders,\n",
    "                                                act=flags_act_func,\n",
    "                                                dropout=True,\n",
    "                                                sparse_inputs=True,\n",
    "                                                logging=self.logging))\n",
    "\n",
    "            self.layers.append(GraphConvolution(input_dim=64,\n",
    "                                                output_dim=flags_hidden1,\n",
    "                                                placeholders=self.placeholders,\n",
    "                                                act=flags_act_func,\n",
    "                                                dropout=True,\n",
    "                                                logging=self.logging))\n",
    "\n",
    "            self.layers.append(GraphConvolution(input_dim=flags_hidden1,\n",
    "                                                output_dim=self.output_dim,\n",
    "                                                placeholders=self.placeholders,\n",
    "                                                act=lambda x: x,\n",
    "                                                dropout=True,\n",
    "                                                logging=self.logging))\n",
    "\n",
    "    def predict(self):\n",
    "        return tf.nn.softmax(self.outputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "# import tensorflow as tf\n",
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()\n",
    "\n",
    "# # Set random seed\n",
    "# seed = 123\n",
    "# np.random.seed(seed)\n",
    "# tf.set_random_seed(seed)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "# flags = tf.compat.v1.flags\n",
    "# FLAGS = flags.FLAGS\n",
    "flags_dataset = 'cora'# , 'Dataset string.')  # 'cora', 'citeseer', 'pubmed'\n",
    "flags_model = 'gcn' #, 'Model string.')  # 'gcn', 'gcn_cheby', 'dense'\n",
    "flags_learning_rate = 0.01 #, 'Initial learning rate.')\n",
    "flags_epochs = 200 #, 'Number of epochs to train.')\n",
    "flags_hidden1 = 16 #, 'Number of units in hidden layer 1.')\n",
    "flags_dropout = 0.5 #, 'Dropout rate (1 - keep probability).')\n",
    "flags_weight_decay = 5e-4 #, 'Weight for L2 loss on embedding matrix.')\n",
    "flags_early_stopping = 10 #, 'Tolerance for early stopping (# of epochs).')\n",
    "flags_max_degree = 3 #, 'Maximum Chebyshev polynomial degree.')\n",
    "flags_act_func = tf.nn.relu # Activation function: tf.nn.relu, tf.nn.leaky_relu, tf.nn.sigmoid, tf.nn.tanh, tf.nn.elu\n",
    "flags_optimizer = tf.train.AdamOptimizer # Optimizer: tf.train.AdamOptimizer, tf.train.GradientDescentOptimizer, tf.train.AdadeltaOptimizer, tf.train.RMSPropOptimizer\n",
    "flags_layers = 2 # layers: 1, 2, 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Load Data and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model evaluation function\n",
    "def evaluate(features, support, labels, mask, placeholders, sess, model):\n",
    "    t_test = time.time()\n",
    "    feed_dict_val = construct_feed_dict(features, support, labels, mask, placeholders)\n",
    "    outs_val = sess.run([model.loss, model.accuracy], feed_dict=feed_dict_val)\n",
    "    return outs_val[0], outs_val[1], (time.time() - t_test)\n",
    "\n",
    "def train(adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask):\n",
    "    seed = 123\n",
    "    np.random.seed(seed)\n",
    "    tf.set_random_seed(seed)\n",
    "    \n",
    "    # Some preprocessing\n",
    "    features = preprocess_features(features)\n",
    "    if flags_model == 'gcn':\n",
    "        support = [preprocess_adj(adj)]\n",
    "        num_supports = 1\n",
    "        model_func = GCN\n",
    "    elif flags_model == 'gcn_cheby':\n",
    "        support = chebyshev_polynomials(adj, flags_max_degree)\n",
    "        num_supports = 1 + flags_max_degree\n",
    "        model_func = GCN\n",
    "    elif flags_model == 'dense':\n",
    "        support = [preprocess_adj(adj)]  # Not used\n",
    "        num_supports = 1\n",
    "        model_func = MLP\n",
    "    else:\n",
    "        raise ValueError('Invalid argument for model: ' + str(flags_model))\n",
    "\n",
    "    # Define placeholders\n",
    "    placeholders = {\n",
    "        'support': [tf.sparse_placeholder(tf.float32) for _ in range(num_supports)],\n",
    "        'features': tf.sparse_placeholder(tf.float32, shape=tf.constant(features[2], dtype=tf.int64)),\n",
    "        'labels': tf.placeholder(tf.float32, shape=(None, y_train.shape[1])),\n",
    "        'labels_mask': tf.placeholder(tf.int32),\n",
    "        'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "        'num_features_nonzero': tf.placeholder(tf.int32)  # helper variable for sparse dropout\n",
    "    }\n",
    "\n",
    "    # Create model\n",
    "    model = model_func(placeholders, input_dim=features[2][1], logging=True)\n",
    "\n",
    "    # Initialize session\n",
    "    sess = tf.Session()\n",
    "    # Init variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    cost_val = []\n",
    "\n",
    "    t_begin = time.time()\n",
    "    # print(\"start training '{}'...\".format(flags_dataset))\n",
    "    # Train model\n",
    "    for epoch in range(flags_epochs):\n",
    "\n",
    "        t = time.time()\n",
    "        # Construct feed dictionary\n",
    "        feed_dict = construct_feed_dict(features, support, y_train, train_mask, placeholders)\n",
    "        feed_dict.update({placeholders['dropout']: flags_dropout})\n",
    "\n",
    "        # Training step\n",
    "        outs = sess.run([model.opt_op, model.loss, model.accuracy], feed_dict=feed_dict)\n",
    "\n",
    "        # Validation\n",
    "        cost, acc, duration = evaluate(features, support, y_val, val_mask, placeholders, sess, model)\n",
    "        cost_val.append(cost)\n",
    "\n",
    "        # Print results\n",
    "        # print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(outs[1]),\n",
    "        #     \"train_acc=\", \"{:.5f}\".format(outs[2]), \"val_loss=\", \"{:.5f}\".format(cost),\n",
    "        #     \"val_acc=\", \"{:.5f}\".format(acc), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "        if epoch % 5 == 0:\n",
    "            print(\".\", end=\"\")\n",
    "\n",
    "        if epoch > flags_early_stopping and cost_val[-1] > np.mean(cost_val[-(flags_early_stopping+1):-1]):\n",
    "            print(\"Early stopping...\")\n",
    "            break\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "    # print(\"Optimization Finished!\")\n",
    "\n",
    "    # print(\"total train time {:.5f}\".format(time.time() - t_begin))\n",
    "    duration = time.time() - t_begin\n",
    "\n",
    "    # Testing\n",
    "    test_cost, test_acc, test_duration = evaluate(features, support, y_test, test_mask, placeholders, sess, model)\n",
    "    return test_cost, test_acc, duration\n",
    "\n",
    "    # print(\"[{}][{}][{}] Test set results:\".format(flags_dataset, flags_act_func.__name__, flags_optimizer.__name__),\n",
    "    #       \"cost=\", \"{:.5f}\".format(test_cost),\n",
    "    #       \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(test_duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== cora begin ===========\n",
      "+ [cora] layers trial begin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhisheng/uiuc_mcs/repos/cs598-dl-healthcare-proj/src/gcn/utils.py:70: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....Early stopping...\n",
      "\n",
      "+ + [cora][1] Test set results: cost=1.92891, accuracy=0.72500, time=1.71725\n",
      "........................................\n",
      "+ + [cora][2] Test set results: cost=1.13065, accuracy=0.80500, time=8.49623\n",
      "........................................\n",
      "+ + [cora][3] Test set results: cost=0.67888, accuracy=0.81600, time=9.93770\n",
      "=========== cora end ===========\n",
      "=========== citeseer begin ===========\n",
      "+ [citeseer] layers trial begin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhisheng/uiuc_mcs/repos/cs598-dl-healthcare-proj/src/gcn/utils.py:115: RuntimeWarning: divide by zero encountered in power\n",
      "  r_inv = np.power(rowsum, -1).flatten()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Early stopping...\n",
      "\n",
      "+ + [citeseer][1] Test set results: cost=1.79088, accuracy=0.53700, time=1.29467\n"
     ]
    }
   ],
   "source": [
    "dataset_list = ['cora', 'citeseer', 'pubmed']\n",
    "optimizer_list = [tf.train.AdamOptimizer, tf.train.GradientDescentOptimizer, tf.train.AdadeltaOptimizer, tf.train.RMSPropOptimizer]\n",
    "activation_list = [tf.nn.relu, tf.nn.leaky_relu, tf.nn.sigmoid, tf.nn.tanh, tf.nn.elu]\n",
    "lr_list = [0.01, 0.99]\n",
    "layers = [1, 2, 3]\n",
    "result = {}\n",
    "\n",
    "\"\"\" RESET GLOBALS \"\"\"\n",
    "_LAYER_UIDS = {}\n",
    "\n",
    "# adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(flags_dataset)\n",
    "\n",
    "for flags_dataset in dataset_list:\n",
    "    print(\"=========== {} begin ===========\".format(flags_dataset))\n",
    "    if flags_dataset not in result.keys():\n",
    "        result[flags_dataset] = {}\n",
    "    print(\"+ [{}] activation function trial begin\".format(flags_dataset))\n",
    "    result[flags_dataset]['activation'] = {}\n",
    "    for flags_act_func in activation_list:\n",
    "        adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(flags_dataset)\n",
    "        test_cost, test_acc, test_duration = train(adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask)\n",
    "        print(\"+ + [{}][{}] Test set results: cost={cost:.5f}, accuracy={accuracy:.5f}, time={time:.5f}\".format(\n",
    "            flags_dataset, flags_act_func.__name__, cost=test_cost, accuracy=test_acc, time=test_duration))\n",
    "        result[flags_dataset]['activation'][flags_act_func.__name__] = {\"cost\": test_cost, \"accuracy\": test_acc, \"time\": test_duration}\n",
    "    # reset activation function\n",
    "    flags_act_func = tf.nn.relu\n",
    "\n",
    "    \n",
    "    print(\"+ [{}] optimizer trial begin\".format(flags_dataset))\n",
    "    result[flags_dataset]['optimizer'] = {}\n",
    "    for flags_optimizer in optimizer_list:\n",
    "        result[flags_dataset]['optimizer'][flags_optimizer.__name__] = []\n",
    "        for flags_learning_rate in lr_list:\n",
    "            adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(flags_dataset)\n",
    "            test_cost, test_acc, test_duration = train(adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask)\n",
    "            print(\"+ + [{}][{}][{lr}] Test set results: cost={cost:.5f}, accuracy={accuracy:.5f}, time={time:.5f}\".format(\n",
    "                flags_dataset, flags_optimizer.__name__, cost=test_cost, accuracy=test_acc, time=test_duration, lr=flags_learning_rate))\n",
    "            result[flags_dataset]['optimizer'][flags_optimizer.__name__].append({\"cost\": test_cost, \"accuracy\": test_acc, \"time\": test_duration})\n",
    "    # reset optimizer\n",
    "    flags_optimizer = tf.train.AdamOptimizer\n",
    "    \n",
    "    print(\"+ [{}] layers trial begin\".format(flags_dataset))\n",
    "    result[flags_dataset]['layers'] = {}\n",
    "    for flags_layers in layers:\n",
    "        adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(flags_dataset)\n",
    "        test_cost, test_acc, test_duration = train(adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask)\n",
    "        print(\"+ + [{}][{}] Test set results: cost={cost:.5f}, accuracy={accuracy:.5f}, time={time:.5f}\".format(\n",
    "            flags_dataset, flags_layers, cost=test_cost, accuracy=test_acc, time=test_duration))\n",
    "        result[flags_dataset]['layers'][flags_layers] = {\"cost\": test_cost, \"accuracy\": test_acc, \"time\": test_duration}\n",
    "\n",
    "    print(\"=========== {} end ===========\".format(flags_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['cora']['optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in result.keys():\n",
    "    print(dataset)\n",
    "    for ablation in result[dataset]:\n",
    "        print(\"+\", ablation)\n",
    "        if ablation == 'optimizer':\n",
    "            for opt in result[dataset][ablation]:\n",
    "                print(\"+\", \"+\", opt)\n",
    "                for lr_idx in range(len(result[dataset][ablation][opt])):\n",
    "                    print(\"+\", \"+\", \"+\", \"lr={}\".format(lr_list[lr_idx]))\n",
    "                    for key, val in result[dataset][ablation][opt][lr_idx].items():\n",
    "                        print(\"+\", \"+\", \"+\", \"+\", key, val)\n",
    "        if ablation == 'activation':\n",
    "            for act in result[dataset][ablation]:\n",
    "                print(\"+\", \"+\", act)\n",
    "                for key, val in result[dataset][ablation][act].items():\n",
    "                    print(\"+\", \"+\", \"+\", key, val)\n",
    "        if ablation == \"layer\":\n",
    "            for layer in result[dataset][ablation]:\n",
    "                print(\"+\", \"+\", layer)\n",
    "                for key, val in result[dataset][ablation][layer].items():\n",
    "                    print(\"+\", \"+\", \"+\", key, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Load data\n",
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(flags_dataset)\n",
    "\n",
    "# Some preprocessing\n",
    "features = preprocess_features(features)\n",
    "if flags_model == 'gcn':\n",
    "    support = [preprocess_adj(adj)]\n",
    "    num_supports = 1\n",
    "    model_func = GCN\n",
    "elif flags_model == 'gcn_cheby':\n",
    "    support = chebyshev_polynomials(adj, flags_max_degree)\n",
    "    num_supports = 1 + flags_max_degree\n",
    "    model_func = GCN\n",
    "elif flags_model == 'dense':\n",
    "    support = [preprocess_adj(adj)]  # Not used\n",
    "    num_supports = 1\n",
    "    model_func = MLP\n",
    "else:\n",
    "    raise ValueError('Invalid argument for model: ' + str(flags_model))\n",
    "\n",
    "# Define placeholders\n",
    "placeholders = {\n",
    "    'support': [tf.sparse_placeholder(tf.float32) for _ in range(num_supports)],\n",
    "    'features': tf.sparse_placeholder(tf.float32, shape=tf.constant(features[2], dtype=tf.int64)),\n",
    "    'labels': tf.placeholder(tf.float32, shape=(None, y_train.shape[1])),\n",
    "    'labels_mask': tf.placeholder(tf.int32),\n",
    "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "    'num_features_nonzero': tf.placeholder(tf.int32)  # helper variable for sparse dropout\n",
    "}\n",
    "\n",
    "# Create model\n",
    "model = model_func(placeholders, input_dim=features[2][1], logging=True)\n",
    "\n",
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "\n",
    "\n",
    "# Define model evaluation function\n",
    "def evaluate(features, support, labels, mask, placeholders):\n",
    "    t_test = time.time()\n",
    "    feed_dict_val = construct_feed_dict(features, support, labels, mask, placeholders)\n",
    "    outs_val = sess.run([model.loss, model.accuracy], feed_dict=feed_dict_val)\n",
    "    return outs_val[0], outs_val[1], (time.time() - t_test)\n",
    "\n",
    "\n",
    "# Init variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "cost_val = []\n",
    "\n",
    "t_begin = time.time()\n",
    "print(\"start training...\")\n",
    "# Train model\n",
    "for epoch in range(flags_epochs):\n",
    "\n",
    "    t = time.time()\n",
    "    # Construct feed dictionary\n",
    "    feed_dict = construct_feed_dict(features, support, y_train, train_mask, placeholders)\n",
    "    feed_dict.update({placeholders['dropout']: flags_dropout})\n",
    "\n",
    "    # Training step\n",
    "    outs = sess.run([model.opt_op, model.loss, model.accuracy], feed_dict=feed_dict)\n",
    "\n",
    "    # Validation\n",
    "    cost, acc, duration = evaluate(features, support, y_val, val_mask, placeholders)\n",
    "    cost_val.append(cost)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(outs[1]),\n",
    "          \"train_acc=\", \"{:.5f}\".format(outs[2]), \"val_loss=\", \"{:.5f}\".format(cost),\n",
    "          \"val_acc=\", \"{:.5f}\".format(acc), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "    # if epoch > flags_early_stopping and cost_val[-1] > np.mean(cost_val[-(flags_early_stopping+1):-1]):\n",
    "    #     print(\"Early stopping...\")\n",
    "    #     break\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "print(\"total train time {:.5f}\".format(time.time() - t_begin))\n",
    "\n",
    "# Testing\n",
    "test_cost, test_acc, test_duration = evaluate(features, support, y_test, test_mask, placeholders)\n",
    "print(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost),\n",
    "      \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(test_duration))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
