{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/zhisheng/anaconda3/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from gcn.inits import *\n",
    "from gcn.utils import *\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "# flags = tf.compat.v1.flags\n",
    "# FLAGS = flags.FLAGS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Layers Class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global unique layer ID dictionary for layer name assignment\n",
    "_LAYER_UIDS = {}\n",
    "\n",
    "\n",
    "def get_layer_uid(layer_name=''):\n",
    "    \"\"\"Helper function, assigns unique layer IDs.\"\"\"\n",
    "    if layer_name not in _LAYER_UIDS:\n",
    "        _LAYER_UIDS[layer_name] = 1\n",
    "        return 1\n",
    "    else:\n",
    "        _LAYER_UIDS[layer_name] += 1\n",
    "        return _LAYER_UIDS[layer_name]\n",
    "\n",
    "\n",
    "def sparse_dropout(x, keep_prob, noise_shape):\n",
    "    \"\"\"Dropout for sparse tensors.\"\"\"\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += tf.random_uniform(noise_shape)\n",
    "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
    "    pre_out = tf.sparse_retain(x, dropout_mask)\n",
    "    return pre_out * (1./keep_prob)\n",
    "\n",
    "\n",
    "def dot(x, y, sparse=False):\n",
    "    \"\"\"Wrapper for tf.matmul (sparse vs dense).\"\"\"\n",
    "    if sparse:\n",
    "        res = tf.sparse_tensor_dense_matmul(x, y)\n",
    "    else:\n",
    "        res = tf.matmul(x, y)\n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Layer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    \"\"\"Base layer class. Defines basic API for all layer objects.\n",
    "    Implementation inspired by keras (http://keras.io).\n",
    "\n",
    "    # Properties\n",
    "        name: String, defines the variable scope of the layer.\n",
    "        logging: Boolean, switches Tensorflow histogram logging on/off\n",
    "\n",
    "    # Methods\n",
    "        _call(inputs): Defines computation graph of layer\n",
    "            (i.e. takes input, returns output)\n",
    "        __call__(inputs): Wrapper for _call()\n",
    "        _log_vars(): Log all variables\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        allowed_kwargs = {'name', 'logging'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            layer = self.__class__.__name__.lower()\n",
    "            name = layer + '_' + str(get_layer_uid(layer))\n",
    "        self.name = name\n",
    "        self.vars = {}\n",
    "        logging = kwargs.get('logging', False)\n",
    "        self.logging = logging\n",
    "        self.sparse_inputs = False\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        return inputs\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.name_scope(self.name):\n",
    "            if self.logging and not self.sparse_inputs:\n",
    "                tf.summary.histogram(self.name + '/inputs', inputs)\n",
    "            outputs = self._call(inputs)\n",
    "            if self.logging:\n",
    "                tf.summary.histogram(self.name + '/outputs', outputs)\n",
    "            return outputs\n",
    "\n",
    "    def _log_vars(self):\n",
    "        for var in self.vars:\n",
    "            tf.summary.histogram(self.name + '/vars/' + var, self.vars[var])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    \"\"\"Dense layer.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, placeholders, dropout=0., sparse_inputs=False,\n",
    "                 act=tf.nn.relu, bias=False, featureless=False, **kwargs):\n",
    "        super(Dense, self).__init__(**kwargs)\n",
    "\n",
    "        if dropout:\n",
    "            self.dropout = placeholders['dropout']\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "\n",
    "        self.act = act\n",
    "        self.sparse_inputs = sparse_inputs\n",
    "        self.featureless = featureless\n",
    "        self.bias = bias\n",
    "\n",
    "        # helper variable for sparse dropout\n",
    "        self.num_features_nonzero = placeholders['num_features_nonzero']\n",
    "\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            self.vars['weights'] = glorot([input_dim, output_dim],\n",
    "                                          name='weights')\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([output_dim], name='bias')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        # dropout\n",
    "        if self.sparse_inputs:\n",
    "            x = sparse_dropout(x, 1-self.dropout, self.num_features_nonzero)\n",
    "        else:\n",
    "            x = tf.nn.dropout(x, 1-self.dropout)\n",
    "\n",
    "        # transform\n",
    "        output = dot(x, self.vars['weights'], sparse=self.sparse_inputs)\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "\n",
    "        return self.act(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Graph Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(Layer):\n",
    "    \"\"\"Graph convolution layer.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, placeholders, dropout=0.,\n",
    "                 sparse_inputs=False, act=tf.nn.relu, bias=False,\n",
    "                 featureless=False, **kwargs):\n",
    "        super(GraphConvolution, self).__init__(**kwargs)\n",
    "\n",
    "        if dropout:\n",
    "            self.dropout = placeholders['dropout']\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "\n",
    "        self.act = act\n",
    "        self.support = placeholders['support']\n",
    "        self.sparse_inputs = sparse_inputs\n",
    "        self.featureless = featureless\n",
    "        self.bias = bias\n",
    "\n",
    "        # helper variable for sparse dropout\n",
    "        self.num_features_nonzero = placeholders['num_features_nonzero']\n",
    "\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            for i in range(len(self.support)):\n",
    "                self.vars['weights_' + str(i)] = glorot([input_dim, output_dim],\n",
    "                                                        name='weights_' + str(i))\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([output_dim], name='bias')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        # dropout\n",
    "        if self.sparse_inputs:\n",
    "            x = sparse_dropout(x, 1-self.dropout, self.num_features_nonzero)\n",
    "        else:\n",
    "            x = tf.nn.dropout(x, 1-self.dropout)\n",
    "\n",
    "        # convolve\n",
    "        supports = list()\n",
    "        for i in range(len(self.support)):\n",
    "            if not self.featureless:\n",
    "                pre_sup = dot(x, self.vars['weights_' + str(i)],\n",
    "                              sparse=self.sparse_inputs)\n",
    "            else:\n",
    "                pre_sup = self.vars['weights_' + str(i)]\n",
    "            support = dot(self.support[i], pre_sup, sparse=True)\n",
    "            supports.append(support)\n",
    "        output = tf.add_n(supports)\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "\n",
    "        return self.act(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcn.metrics import *\n",
    "\n",
    "# flags = tf.compat.v1.flags\n",
    "# FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        allowed_kwargs = {'name', 'logging'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            name = self.__class__.__name__.lower()\n",
    "        self.name = name\n",
    "\n",
    "        logging = kwargs.get('logging', False)\n",
    "        self.logging = logging\n",
    "\n",
    "        self.vars = {}\n",
    "        self.placeholders = {}\n",
    "\n",
    "        self.layers = []\n",
    "        self.activations = []\n",
    "\n",
    "        self.inputs = None\n",
    "        self.outputs = None\n",
    "\n",
    "        self.loss = 0\n",
    "        self.accuracy = 0\n",
    "        self.optimizer = None\n",
    "        self.opt_op = None\n",
    "\n",
    "    def _build(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\" Wrapper for _build() \"\"\"\n",
    "        with tf.variable_scope(self.name):\n",
    "            self._build()\n",
    "\n",
    "        # Build sequential layer model\n",
    "        self.activations.append(self.inputs)\n",
    "        for layer in self.layers:\n",
    "            hidden = layer(self.activations[-1])\n",
    "            self.activations.append(hidden)\n",
    "        self.outputs = self.activations[-1]\n",
    "\n",
    "        # Store model variables for easy access\n",
    "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
    "        self.vars = {var.name: var for var in variables}\n",
    "\n",
    "        # Build metrics\n",
    "        self._loss()\n",
    "        self._accuracy()\n",
    "\n",
    "        self.opt_op = self.optimizer.minimize(self.loss)\n",
    "\n",
    "    def predict(self):\n",
    "        pass\n",
    "\n",
    "    def _loss(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _accuracy(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def save(self, sess=None):\n",
    "        if not sess:\n",
    "            raise AttributeError(\"TensorFlow session not provided.\")\n",
    "        saver = tf.train.Saver(self.vars)\n",
    "        save_path = saver.save(sess, \"tmp/%s.ckpt\" % self.name)\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "    def load(self, sess=None):\n",
    "        if not sess:\n",
    "            raise AttributeError(\"TensorFlow session not provided.\")\n",
    "        saver = tf.train.Saver(self.vars)\n",
    "        save_path = \"tmp/%s.ckpt\" % self.name\n",
    "        saver.restore(sess, save_path)\n",
    "        print(\"Model restored from file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(Model):\n",
    "    def __init__(self, placeholders, input_dim, **kwargs):\n",
    "        super(MLP, self).__init__(**kwargs)\n",
    "\n",
    "        self.inputs = placeholders['features']\n",
    "        self.input_dim = input_dim\n",
    "        # self.input_dim = self.inputs.get_shape().as_list()[1]  # To be supported in future Tensorflow versions\n",
    "        self.output_dim = placeholders['labels'].get_shape().as_list()[1]\n",
    "        self.placeholders = placeholders\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=flags_learning_rate)\n",
    "\n",
    "        self.build()\n",
    "\n",
    "    def _loss(self):\n",
    "        # Weight decay loss\n",
    "        for var in self.layers[0].vars.values():\n",
    "            self.loss += flags_weight_decay * tf.nn.l2_loss(var)\n",
    "\n",
    "        # Cross entropy error\n",
    "        self.loss += masked_softmax_cross_entropy(self.outputs, self.placeholders['labels'],\n",
    "                                                  self.placeholders['labels_mask'])\n",
    "\n",
    "    def _accuracy(self):\n",
    "        self.accuracy = masked_accuracy(self.outputs, self.placeholders['labels'],\n",
    "                                        self.placeholders['labels_mask'])\n",
    "\n",
    "    def _build(self):\n",
    "        self.layers.append(Dense(input_dim=self.input_dim,\n",
    "                                 output_dim=flags_hidden1,\n",
    "                                 placeholders=self.placeholders,\n",
    "                                 act=tf.nn.relu,\n",
    "                                 dropout=True,\n",
    "                                 sparse_inputs=True,\n",
    "                                 logging=self.logging))\n",
    "\n",
    "        self.layers.append(Dense(input_dim=flags_hidden1,\n",
    "                                 output_dim=self.output_dim,\n",
    "                                 placeholders=self.placeholders,\n",
    "                                 act=lambda x: x,\n",
    "                                 dropout=True,\n",
    "                                 logging=self.logging))\n",
    "\n",
    "    def predict(self):\n",
    "        return tf.nn.softmax(self.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(Model):\n",
    "    def __init__(self, placeholders, input_dim, **kwargs):\n",
    "        super(GCN, self).__init__(**kwargs)\n",
    "\n",
    "        self.inputs = placeholders['features']\n",
    "        self.input_dim = input_dim\n",
    "        # self.input_dim = self.inputs.get_shape().as_list()[1]  # To be supported in future Tensorflow versions\n",
    "        self.output_dim = placeholders['labels'].get_shape().as_list()[1]\n",
    "        self.placeholders = placeholders\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=flags_learning_rate)\n",
    "\n",
    "        self.build()\n",
    "\n",
    "    def _loss(self):\n",
    "        # Weight decay loss\n",
    "        for var in self.layers[0].vars.values():\n",
    "            self.loss += flags_weight_decay * tf.nn.l2_loss(var)\n",
    "\n",
    "        # Cross entropy error\n",
    "        self.loss += masked_softmax_cross_entropy(self.outputs, self.placeholders['labels'],\n",
    "                                                  self.placeholders['labels_mask'])\n",
    "\n",
    "    def _accuracy(self):\n",
    "        self.accuracy = masked_accuracy(self.outputs, self.placeholders['labels'],\n",
    "                                        self.placeholders['labels_mask'])\n",
    "\n",
    "    def _build(self):\n",
    "\n",
    "        self.layers.append(GraphConvolution(input_dim=self.input_dim,\n",
    "                                            output_dim=flags_hidden1,\n",
    "                                            placeholders=self.placeholders,\n",
    "                                            act=tf.nn.relu,\n",
    "                                            dropout=True,\n",
    "                                            sparse_inputs=True,\n",
    "                                            logging=self.logging))\n",
    "\n",
    "        self.layers.append(GraphConvolution(input_dim=flags_hidden1,\n",
    "                                            output_dim=self.output_dim,\n",
    "                                            placeholders=self.placeholders,\n",
    "                                            act=lambda x: x,\n",
    "                                            dropout=True,\n",
    "                                            logging=self.logging))\n",
    "\n",
    "    def predict(self):\n",
    "        return tf.nn.softmax(self.outputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "# import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# Set random seed\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "# flags = tf.compat.v1.flags\n",
    "# FLAGS = flags.FLAGS\n",
    "flags_dataset = 'cora'# , 'Dataset string.')  # 'cora', 'citeseer', 'pubmed'\n",
    "flags_model = 'gcn' #, 'Model string.')  # 'gcn', 'gcn_cheby', 'dense'\n",
    "flags_learning_rate = 0.01 #, 'Initial learning rate.')\n",
    "flags_epochs = 200 #, 'Number of epochs to train.')\n",
    "flags_hidden1 = 16 #, 'Number of units in hidden layer 1.')\n",
    "flags_dropout = 0.5 #, 'Dropout rate (1 - keep probability).')\n",
    "flags_weight_decay = 5e-4 #, 'Weight for L2 loss on embedding matrix.')\n",
    "flags_early_stopping = 10 #, 'Tolerance for early stopping (# of epochs).')\n",
    "flags_max_degree = 3 #, 'Maximum Chebyshev polynomial degree.')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Load Data and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "Epoch: 0001 train_loss= 1.95411 train_acc= 0.12143 val_loss= 1.95055 val_acc= 0.16400 time= 0.16731\n",
      "Epoch: 0002 train_loss= 1.94770 train_acc= 0.23571 val_loss= 1.94666 val_acc= 0.18000 time= 0.02549\n",
      "Epoch: 0003 train_loss= 1.93916 train_acc= 0.40714 val_loss= 1.94264 val_acc= 0.19200 time= 0.02534\n",
      "Epoch: 0004 train_loss= 1.92905 train_acc= 0.45714 val_loss= 1.93915 val_acc= 0.21800 time= 0.02643\n",
      "Epoch: 0005 train_loss= 1.91791 train_acc= 0.48571 val_loss= 1.93631 val_acc= 0.22800 time= 0.02329\n",
      "Epoch: 0006 train_loss= 1.90632 train_acc= 0.53571 val_loss= 1.93382 val_acc= 0.23400 time= 0.02507\n",
      "Epoch: 0007 train_loss= 1.89404 train_acc= 0.58571 val_loss= 1.93155 val_acc= 0.24400 time= 0.02300\n",
      "Epoch: 0008 train_loss= 1.88116 train_acc= 0.65714 val_loss= 1.92949 val_acc= 0.25000 time= 0.02378\n",
      "Epoch: 0009 train_loss= 1.86781 train_acc= 0.67143 val_loss= 1.92762 val_acc= 0.25400 time= 0.02296\n",
      "Epoch: 0010 train_loss= 1.85404 train_acc= 0.67857 val_loss= 1.92596 val_acc= 0.26000 time= 0.02356\n",
      "Epoch: 0011 train_loss= 1.83990 train_acc= 0.71429 val_loss= 1.92449 val_acc= 0.26400 time= 0.02331\n",
      "Epoch: 0012 train_loss= 1.82538 train_acc= 0.71429 val_loss= 1.92321 val_acc= 0.27200 time= 0.02457\n",
      "Epoch: 0013 train_loss= 1.81047 train_acc= 0.72143 val_loss= 1.92215 val_acc= 0.27600 time= 0.02684\n",
      "Epoch: 0014 train_loss= 1.79523 train_acc= 0.72857 val_loss= 1.92127 val_acc= 0.28200 time= 0.02401\n",
      "Epoch: 0015 train_loss= 1.77965 train_acc= 0.74286 val_loss= 1.92048 val_acc= 0.28600 time= 0.02338\n",
      "Epoch: 0016 train_loss= 1.76375 train_acc= 0.75000 val_loss= 1.91969 val_acc= 0.29000 time= 0.02228\n",
      "Epoch: 0017 train_loss= 1.74752 train_acc= 0.75714 val_loss= 1.91892 val_acc= 0.30800 time= 0.02327\n",
      "Epoch: 0018 train_loss= 1.73097 train_acc= 0.75714 val_loss= 1.91820 val_acc= 0.31000 time= 0.02400\n",
      "Epoch: 0019 train_loss= 1.71409 train_acc= 0.75714 val_loss= 1.91757 val_acc= 0.31600 time= 0.02405\n",
      "Epoch: 0020 train_loss= 1.69698 train_acc= 0.76429 val_loss= 1.91701 val_acc= 0.32000 time= 0.02371\n",
      "Epoch: 0021 train_loss= 1.67963 train_acc= 0.76429 val_loss= 1.91637 val_acc= 0.32200 time= 0.02478\n",
      "Epoch: 0022 train_loss= 1.66200 train_acc= 0.76429 val_loss= 1.91563 val_acc= 0.32800 time= 0.02535\n",
      "Epoch: 0023 train_loss= 1.64417 train_acc= 0.77143 val_loss= 1.91489 val_acc= 0.33000 time= 0.02379\n",
      "Epoch: 0024 train_loss= 1.62612 train_acc= 0.77143 val_loss= 1.91416 val_acc= 0.33400 time= 0.02586\n",
      "Epoch: 0025 train_loss= 1.60787 train_acc= 0.78571 val_loss= 1.91329 val_acc= 0.34200 time= 0.02332\n",
      "Epoch: 0026 train_loss= 1.58945 train_acc= 0.78571 val_loss= 1.91221 val_acc= 0.35800 time= 0.02319\n",
      "Epoch: 0027 train_loss= 1.57088 train_acc= 0.78571 val_loss= 1.91098 val_acc= 0.36400 time= 0.02350\n",
      "Epoch: 0028 train_loss= 1.55219 train_acc= 0.80000 val_loss= 1.90969 val_acc= 0.37000 time= 0.02469\n",
      "Epoch: 0029 train_loss= 1.53339 train_acc= 0.80714 val_loss= 1.90828 val_acc= 0.37400 time= 0.02352\n",
      "Epoch: 0030 train_loss= 1.51447 train_acc= 0.82143 val_loss= 1.90666 val_acc= 0.37600 time= 0.02380\n",
      "Epoch: 0031 train_loss= 1.49550 train_acc= 0.83571 val_loss= 1.90492 val_acc= 0.38400 time= 0.02571\n",
      "Epoch: 0032 train_loss= 1.47647 train_acc= 0.85000 val_loss= 1.90302 val_acc= 0.39200 time= 0.02307\n",
      "Epoch: 0033 train_loss= 1.45741 train_acc= 0.86429 val_loss= 1.90085 val_acc= 0.40000 time= 0.02343\n",
      "Epoch: 0034 train_loss= 1.43834 train_acc= 0.87143 val_loss= 1.89849 val_acc= 0.40800 time= 0.02278\n",
      "Epoch: 0035 train_loss= 1.41926 train_acc= 0.87857 val_loss= 1.89597 val_acc= 0.41600 time= 0.02345\n",
      "Epoch: 0036 train_loss= 1.40026 train_acc= 0.87857 val_loss= 1.89325 val_acc= 0.42600 time= 0.02260\n",
      "Epoch: 0037 train_loss= 1.38128 train_acc= 0.88571 val_loss= 1.89029 val_acc= 0.43600 time= 0.02255\n",
      "Epoch: 0038 train_loss= 1.36236 train_acc= 0.88571 val_loss= 1.88715 val_acc= 0.44400 time= 0.02332\n",
      "Epoch: 0039 train_loss= 1.34354 train_acc= 0.88571 val_loss= 1.88372 val_acc= 0.45000 time= 0.02364\n",
      "Epoch: 0040 train_loss= 1.32484 train_acc= 0.88571 val_loss= 1.88001 val_acc= 0.45800 time= 0.02396\n",
      "Epoch: 0041 train_loss= 1.30624 train_acc= 0.89286 val_loss= 1.87603 val_acc= 0.46200 time= 0.02292\n",
      "Epoch: 0042 train_loss= 1.28780 train_acc= 0.91429 val_loss= 1.87197 val_acc= 0.47000 time= 0.02261\n",
      "Epoch: 0043 train_loss= 1.26952 train_acc= 0.91429 val_loss= 1.86779 val_acc= 0.47600 time= 0.02369\n",
      "Epoch: 0044 train_loss= 1.25140 train_acc= 0.91429 val_loss= 1.86329 val_acc= 0.49000 time= 0.02307\n",
      "Epoch: 0045 train_loss= 1.23348 train_acc= 0.92143 val_loss= 1.85859 val_acc= 0.49800 time= 0.02273\n",
      "Epoch: 0046 train_loss= 1.21582 train_acc= 0.92857 val_loss= 1.85351 val_acc= 0.50600 time= 0.02368\n",
      "Epoch: 0047 train_loss= 1.19836 train_acc= 0.94286 val_loss= 1.84846 val_acc= 0.51400 time= 0.02411\n",
      "Epoch: 0048 train_loss= 1.18113 train_acc= 0.95000 val_loss= 1.84326 val_acc= 0.52000 time= 0.02384\n",
      "Epoch: 0049 train_loss= 1.16415 train_acc= 0.95714 val_loss= 1.83787 val_acc= 0.53200 time= 0.02406\n",
      "Epoch: 0050 train_loss= 1.14744 train_acc= 0.95714 val_loss= 1.83225 val_acc= 0.53600 time= 0.02424\n",
      "Epoch: 0051 train_loss= 1.13101 train_acc= 0.95714 val_loss= 1.82646 val_acc= 0.54000 time= 0.02293\n",
      "Epoch: 0052 train_loss= 1.11487 train_acc= 0.95714 val_loss= 1.82062 val_acc= 0.54400 time= 0.02347\n",
      "Epoch: 0053 train_loss= 1.09902 train_acc= 0.95714 val_loss= 1.81471 val_acc= 0.55200 time= 0.02321\n",
      "Epoch: 0054 train_loss= 1.08344 train_acc= 0.97143 val_loss= 1.80860 val_acc= 0.55800 time= 0.02267\n",
      "Epoch: 0055 train_loss= 1.06815 train_acc= 0.97143 val_loss= 1.80229 val_acc= 0.56200 time= 0.02301\n",
      "Epoch: 0056 train_loss= 1.05321 train_acc= 0.97857 val_loss= 1.79588 val_acc= 0.56600 time= 0.02315\n",
      "Epoch: 0057 train_loss= 1.03853 train_acc= 0.97857 val_loss= 1.78932 val_acc= 0.57600 time= 0.02373\n",
      "Epoch: 0058 train_loss= 1.02415 train_acc= 0.98571 val_loss= 1.78277 val_acc= 0.58200 time= 0.02392\n",
      "Epoch: 0059 train_loss= 1.01009 train_acc= 0.98571 val_loss= 1.77613 val_acc= 0.59200 time= 0.02320\n",
      "Epoch: 0060 train_loss= 0.99637 train_acc= 0.99286 val_loss= 1.76930 val_acc= 0.59800 time= 0.02338\n",
      "Epoch: 0061 train_loss= 0.98292 train_acc= 0.99286 val_loss= 1.76230 val_acc= 0.60200 time= 0.02294\n",
      "Epoch: 0062 train_loss= 0.96976 train_acc= 0.99286 val_loss= 1.75548 val_acc= 0.61200 time= 0.02324\n",
      "Epoch: 0063 train_loss= 0.95690 train_acc= 0.99286 val_loss= 1.74848 val_acc= 0.62400 time= 0.02344\n",
      "Epoch: 0064 train_loss= 0.94437 train_acc= 0.99286 val_loss= 1.74132 val_acc= 0.63600 time= 0.02279\n",
      "Epoch: 0065 train_loss= 0.93212 train_acc= 0.99286 val_loss= 1.73401 val_acc= 0.64200 time= 0.02334\n",
      "Epoch: 0066 train_loss= 0.92015 train_acc= 0.99286 val_loss= 1.72678 val_acc= 0.64200 time= 0.02315\n",
      "Epoch: 0067 train_loss= 0.90846 train_acc= 0.99286 val_loss= 1.71952 val_acc= 0.64600 time= 0.02470\n",
      "Epoch: 0068 train_loss= 0.89705 train_acc= 0.99286 val_loss= 1.71220 val_acc= 0.65200 time= 0.02344\n",
      "Epoch: 0069 train_loss= 0.88590 train_acc= 0.99286 val_loss= 1.70468 val_acc= 0.65800 time= 0.02252\n",
      "Epoch: 0070 train_loss= 0.87502 train_acc= 0.99286 val_loss= 1.69717 val_acc= 0.66600 time= 0.02335\n",
      "Epoch: 0071 train_loss= 0.86446 train_acc= 1.00000 val_loss= 1.68987 val_acc= 0.66800 time= 0.02257\n",
      "Epoch: 0072 train_loss= 0.85411 train_acc= 1.00000 val_loss= 1.68265 val_acc= 0.67800 time= 0.02308\n",
      "Epoch: 0073 train_loss= 0.84403 train_acc= 1.00000 val_loss= 1.67533 val_acc= 0.68200 time= 0.02265\n",
      "Epoch: 0074 train_loss= 0.83418 train_acc= 1.00000 val_loss= 1.66793 val_acc= 0.68600 time= 0.02321\n",
      "Epoch: 0075 train_loss= 0.82458 train_acc= 1.00000 val_loss= 1.66067 val_acc= 0.68800 time= 0.02369\n",
      "Epoch: 0076 train_loss= 0.81521 train_acc= 1.00000 val_loss= 1.65335 val_acc= 0.69600 time= 0.02493\n",
      "Epoch: 0077 train_loss= 0.80607 train_acc= 1.00000 val_loss= 1.64627 val_acc= 0.70000 time= 0.02241\n",
      "Epoch: 0078 train_loss= 0.79718 train_acc= 1.00000 val_loss= 1.63938 val_acc= 0.70800 time= 0.02302\n",
      "Epoch: 0079 train_loss= 0.78851 train_acc= 1.00000 val_loss= 1.63234 val_acc= 0.71200 time= 0.02297\n",
      "Epoch: 0080 train_loss= 0.78004 train_acc= 1.00000 val_loss= 1.62540 val_acc= 0.71200 time= 0.02398\n",
      "Epoch: 0081 train_loss= 0.77180 train_acc= 1.00000 val_loss= 1.61868 val_acc= 0.71600 time= 0.02179\n",
      "Epoch: 0082 train_loss= 0.76374 train_acc= 1.00000 val_loss= 1.61222 val_acc= 0.71600 time= 0.02280\n",
      "Epoch: 0083 train_loss= 0.75589 train_acc= 1.00000 val_loss= 1.60575 val_acc= 0.71800 time= 0.02283\n",
      "Epoch: 0084 train_loss= 0.74821 train_acc= 1.00000 val_loss= 1.59914 val_acc= 0.71800 time= 0.02338\n",
      "Epoch: 0085 train_loss= 0.74073 train_acc= 1.00000 val_loss= 1.59259 val_acc= 0.71800 time= 0.02387\n",
      "Epoch: 0086 train_loss= 0.73345 train_acc= 1.00000 val_loss= 1.58613 val_acc= 0.72200 time= 0.02382\n",
      "Epoch: 0087 train_loss= 0.72631 train_acc= 1.00000 val_loss= 1.57994 val_acc= 0.72400 time= 0.02319\n",
      "Epoch: 0088 train_loss= 0.71935 train_acc= 1.00000 val_loss= 1.57407 val_acc= 0.72400 time= 0.02344\n",
      "Epoch: 0089 train_loss= 0.71257 train_acc= 1.00000 val_loss= 1.56828 val_acc= 0.72600 time= 0.02285\n",
      "Epoch: 0090 train_loss= 0.70596 train_acc= 1.00000 val_loss= 1.56249 val_acc= 0.72600 time= 0.02388\n",
      "Epoch: 0091 train_loss= 0.69948 train_acc= 1.00000 val_loss= 1.55676 val_acc= 0.73000 time= 0.02258\n",
      "Epoch: 0092 train_loss= 0.69317 train_acc= 1.00000 val_loss= 1.55100 val_acc= 0.73200 time= 0.02261\n",
      "Epoch: 0093 train_loss= 0.68700 train_acc= 1.00000 val_loss= 1.54526 val_acc= 0.73600 time= 0.02391\n",
      "Epoch: 0094 train_loss= 0.68098 train_acc= 1.00000 val_loss= 1.53968 val_acc= 0.73800 time= 0.02426\n",
      "Epoch: 0095 train_loss= 0.67507 train_acc= 1.00000 val_loss= 1.53427 val_acc= 0.73600 time= 0.02276\n",
      "Epoch: 0096 train_loss= 0.66931 train_acc= 1.00000 val_loss= 1.52894 val_acc= 0.73600 time= 0.02296\n",
      "Epoch: 0097 train_loss= 0.66367 train_acc= 1.00000 val_loss= 1.52357 val_acc= 0.73800 time= 0.02295\n",
      "Epoch: 0098 train_loss= 0.65816 train_acc= 1.00000 val_loss= 1.51829 val_acc= 0.73600 time= 0.02366\n",
      "Epoch: 0099 train_loss= 0.65277 train_acc= 1.00000 val_loss= 1.51311 val_acc= 0.73600 time= 0.02284\n",
      "Epoch: 0100 train_loss= 0.64750 train_acc= 1.00000 val_loss= 1.50803 val_acc= 0.73800 time= 0.02284\n",
      "Epoch: 0101 train_loss= 0.64233 train_acc= 1.00000 val_loss= 1.50296 val_acc= 0.74200 time= 0.02313\n",
      "Epoch: 0102 train_loss= 0.63725 train_acc= 1.00000 val_loss= 1.49804 val_acc= 0.74200 time= 0.02354\n",
      "Epoch: 0103 train_loss= 0.63231 train_acc= 1.00000 val_loss= 1.49334 val_acc= 0.74400 time= 0.02469\n",
      "Epoch: 0104 train_loss= 0.62748 train_acc= 1.00000 val_loss= 1.48870 val_acc= 0.74400 time= 0.02362\n",
      "Epoch: 0105 train_loss= 0.62274 train_acc= 1.00000 val_loss= 1.48401 val_acc= 0.74400 time= 0.02342\n",
      "Epoch: 0106 train_loss= 0.61808 train_acc= 1.00000 val_loss= 1.47944 val_acc= 0.74200 time= 0.02275\n",
      "Epoch: 0107 train_loss= 0.61351 train_acc= 1.00000 val_loss= 1.47497 val_acc= 0.74400 time= 0.02321\n",
      "Epoch: 0108 train_loss= 0.60903 train_acc= 1.00000 val_loss= 1.47044 val_acc= 0.74400 time= 0.02361\n",
      "Epoch: 0109 train_loss= 0.60465 train_acc= 1.00000 val_loss= 1.46590 val_acc= 0.74600 time= 0.02261\n",
      "Epoch: 0110 train_loss= 0.60036 train_acc= 1.00000 val_loss= 1.46137 val_acc= 0.74800 time= 0.02277\n",
      "Epoch: 0111 train_loss= 0.59616 train_acc= 1.00000 val_loss= 1.45715 val_acc= 0.74800 time= 0.02436\n",
      "Epoch: 0112 train_loss= 0.59202 train_acc= 1.00000 val_loss= 1.45286 val_acc= 0.74800 time= 0.05379\n",
      "Epoch: 0113 train_loss= 0.58795 train_acc= 1.00000 val_loss= 1.44858 val_acc= 0.74800 time= 0.02727\n",
      "Epoch: 0114 train_loss= 0.58398 train_acc= 1.00000 val_loss= 1.44445 val_acc= 0.75000 time= 0.02319\n",
      "Epoch: 0115 train_loss= 0.58008 train_acc= 1.00000 val_loss= 1.44043 val_acc= 0.75200 time= 0.02341\n",
      "Epoch: 0116 train_loss= 0.57624 train_acc= 1.00000 val_loss= 1.43633 val_acc= 0.75200 time= 0.02296\n",
      "Epoch: 0117 train_loss= 0.57246 train_acc= 1.00000 val_loss= 1.43231 val_acc= 0.75200 time= 0.02270\n",
      "Epoch: 0118 train_loss= 0.56875 train_acc= 1.00000 val_loss= 1.42847 val_acc= 0.75400 time= 0.02299\n",
      "Epoch: 0119 train_loss= 0.56512 train_acc= 1.00000 val_loss= 1.42462 val_acc= 0.75000 time= 0.02406\n",
      "Epoch: 0120 train_loss= 0.56154 train_acc= 1.00000 val_loss= 1.42071 val_acc= 0.75200 time= 0.02513\n",
      "Epoch: 0121 train_loss= 0.55804 train_acc= 1.00000 val_loss= 1.41695 val_acc= 0.75200 time= 0.02414\n",
      "Epoch: 0122 train_loss= 0.55461 train_acc= 1.00000 val_loss= 1.41337 val_acc= 0.75200 time= 0.02339\n",
      "Epoch: 0123 train_loss= 0.55122 train_acc= 1.00000 val_loss= 1.40982 val_acc= 0.75200 time= 0.02308\n",
      "Epoch: 0124 train_loss= 0.54787 train_acc= 1.00000 val_loss= 1.40617 val_acc= 0.75200 time= 0.02319\n",
      "Epoch: 0125 train_loss= 0.54458 train_acc= 1.00000 val_loss= 1.40247 val_acc= 0.75200 time= 0.02332\n",
      "Epoch: 0126 train_loss= 0.54138 train_acc= 1.00000 val_loss= 1.39909 val_acc= 0.75400 time= 0.02272\n",
      "Epoch: 0127 train_loss= 0.53820 train_acc= 1.00000 val_loss= 1.39583 val_acc= 0.75400 time= 0.02372\n",
      "Epoch: 0128 train_loss= 0.53507 train_acc= 1.00000 val_loss= 1.39244 val_acc= 0.75400 time= 0.02380\n",
      "Epoch: 0129 train_loss= 0.53199 train_acc= 1.00000 val_loss= 1.38901 val_acc= 0.75800 time= 0.02478\n",
      "Epoch: 0130 train_loss= 0.52896 train_acc= 1.00000 val_loss= 1.38574 val_acc= 0.75800 time= 0.02287\n",
      "Epoch: 0131 train_loss= 0.52598 train_acc= 1.00000 val_loss= 1.38252 val_acc= 0.75800 time= 0.02346\n",
      "Epoch: 0132 train_loss= 0.52304 train_acc= 1.00000 val_loss= 1.37921 val_acc= 0.75800 time= 0.02347\n",
      "Epoch: 0133 train_loss= 0.52014 train_acc= 1.00000 val_loss= 1.37598 val_acc= 0.75800 time= 0.02274\n",
      "Epoch: 0134 train_loss= 0.51727 train_acc= 1.00000 val_loss= 1.37283 val_acc= 0.75800 time= 0.02344\n",
      "Epoch: 0135 train_loss= 0.51448 train_acc= 1.00000 val_loss= 1.36976 val_acc= 0.76000 time= 0.02307\n",
      "Epoch: 0136 train_loss= 0.51172 train_acc= 1.00000 val_loss= 1.36674 val_acc= 0.76000 time= 0.02251\n",
      "Epoch: 0137 train_loss= 0.50900 train_acc= 1.00000 val_loss= 1.36363 val_acc= 0.76000 time= 0.02375\n",
      "Epoch: 0138 train_loss= 0.50631 train_acc= 1.00000 val_loss= 1.36065 val_acc= 0.76000 time= 0.02417\n",
      "Epoch: 0139 train_loss= 0.50367 train_acc= 1.00000 val_loss= 1.35773 val_acc= 0.76000 time= 0.02344\n",
      "Epoch: 0140 train_loss= 0.50106 train_acc= 1.00000 val_loss= 1.35473 val_acc= 0.76000 time= 0.02263\n",
      "Epoch: 0141 train_loss= 0.49849 train_acc= 1.00000 val_loss= 1.35173 val_acc= 0.76000 time= 0.02263\n",
      "Epoch: 0142 train_loss= 0.49597 train_acc= 1.00000 val_loss= 1.34881 val_acc= 0.76200 time= 0.02303\n",
      "Epoch: 0143 train_loss= 0.49347 train_acc= 1.00000 val_loss= 1.34603 val_acc= 0.76200 time= 0.02259\n",
      "Epoch: 0144 train_loss= 0.49100 train_acc= 1.00000 val_loss= 1.34340 val_acc= 0.76400 time= 0.02329\n",
      "Epoch: 0145 train_loss= 0.48856 train_acc= 1.00000 val_loss= 1.34070 val_acc= 0.76400 time= 0.02321\n",
      "Epoch: 0146 train_loss= 0.48617 train_acc= 1.00000 val_loss= 1.33786 val_acc= 0.76400 time= 0.02301\n",
      "Epoch: 0147 train_loss= 0.48381 train_acc= 1.00000 val_loss= 1.33504 val_acc= 0.76400 time= 0.02410\n",
      "Epoch: 0148 train_loss= 0.48146 train_acc= 1.00000 val_loss= 1.33242 val_acc= 0.76200 time= 0.02282\n",
      "Epoch: 0149 train_loss= 0.47915 train_acc= 1.00000 val_loss= 1.32985 val_acc= 0.76200 time= 0.02383\n",
      "Epoch: 0150 train_loss= 0.47688 train_acc= 1.00000 val_loss= 1.32732 val_acc= 0.76400 time= 0.02283\n",
      "Epoch: 0151 train_loss= 0.47464 train_acc= 1.00000 val_loss= 1.32478 val_acc= 0.76400 time= 0.02280\n",
      "Epoch: 0152 train_loss= 0.47241 train_acc= 1.00000 val_loss= 1.32221 val_acc= 0.76400 time= 0.02384\n",
      "Epoch: 0153 train_loss= 0.47023 train_acc= 1.00000 val_loss= 1.31968 val_acc= 0.76400 time= 0.02280\n",
      "Epoch: 0154 train_loss= 0.46807 train_acc= 1.00000 val_loss= 1.31733 val_acc= 0.76400 time= 0.02331\n",
      "Epoch: 0155 train_loss= 0.46595 train_acc= 1.00000 val_loss= 1.31487 val_acc= 0.76400 time= 0.02405\n",
      "Epoch: 0156 train_loss= 0.46383 train_acc= 1.00000 val_loss= 1.31235 val_acc= 0.76400 time= 0.02395\n",
      "Epoch: 0157 train_loss= 0.46175 train_acc= 1.00000 val_loss= 1.30988 val_acc= 0.76400 time= 0.02396\n",
      "Epoch: 0158 train_loss= 0.45970 train_acc= 1.00000 val_loss= 1.30748 val_acc= 0.76600 time= 0.02340\n",
      "Epoch: 0159 train_loss= 0.45767 train_acc= 1.00000 val_loss= 1.30509 val_acc= 0.76600 time= 0.02299\n",
      "Epoch: 0160 train_loss= 0.45566 train_acc= 1.00000 val_loss= 1.30291 val_acc= 0.76600 time= 0.02294\n",
      "Epoch: 0161 train_loss= 0.45367 train_acc= 1.00000 val_loss= 1.30073 val_acc= 0.76600 time= 0.02337\n",
      "Epoch: 0162 train_loss= 0.45172 train_acc= 1.00000 val_loss= 1.29836 val_acc= 0.76600 time= 0.02387\n",
      "Epoch: 0163 train_loss= 0.44978 train_acc= 1.00000 val_loss= 1.29612 val_acc= 0.76800 time= 0.02272\n",
      "Epoch: 0164 train_loss= 0.44786 train_acc= 1.00000 val_loss= 1.29418 val_acc= 0.76800 time= 0.02380\n",
      "Epoch: 0165 train_loss= 0.44597 train_acc= 1.00000 val_loss= 1.29217 val_acc= 0.76600 time= 0.02426\n",
      "Epoch: 0166 train_loss= 0.44410 train_acc= 1.00000 val_loss= 1.28991 val_acc= 0.76400 time= 0.02395\n",
      "Epoch: 0167 train_loss= 0.44225 train_acc= 1.00000 val_loss= 1.28765 val_acc= 0.76400 time= 0.02290\n",
      "Epoch: 0168 train_loss= 0.44042 train_acc= 1.00000 val_loss= 1.28549 val_acc= 0.76400 time= 0.02271\n",
      "Epoch: 0169 train_loss= 0.43860 train_acc= 1.00000 val_loss= 1.28346 val_acc= 0.76400 time= 0.02365\n",
      "Epoch: 0170 train_loss= 0.43682 train_acc= 1.00000 val_loss= 1.28144 val_acc= 0.76400 time= 0.02272\n",
      "Epoch: 0171 train_loss= 0.43506 train_acc= 1.00000 val_loss= 1.27936 val_acc= 0.76600 time= 0.02323\n",
      "Epoch: 0172 train_loss= 0.43331 train_acc= 1.00000 val_loss= 1.27727 val_acc= 0.76400 time= 0.02272\n",
      "Epoch: 0173 train_loss= 0.43157 train_acc= 1.00000 val_loss= 1.27544 val_acc= 0.76000 time= 0.02353\n",
      "Epoch: 0174 train_loss= 0.42986 train_acc= 1.00000 val_loss= 1.27366 val_acc= 0.76000 time= 0.02571\n",
      "Epoch: 0175 train_loss= 0.42817 train_acc= 1.00000 val_loss= 1.27191 val_acc= 0.76000 time= 0.02411\n",
      "Epoch: 0176 train_loss= 0.42649 train_acc= 1.00000 val_loss= 1.26987 val_acc= 0.76200 time= 0.02307\n",
      "Epoch: 0177 train_loss= 0.42484 train_acc= 1.00000 val_loss= 1.26767 val_acc= 0.76200 time= 0.02275\n",
      "Epoch: 0178 train_loss= 0.42320 train_acc= 1.00000 val_loss= 1.26570 val_acc= 0.76200 time= 0.02356\n",
      "Epoch: 0179 train_loss= 0.42157 train_acc= 1.00000 val_loss= 1.26372 val_acc= 0.76200 time= 0.02359\n",
      "Epoch: 0180 train_loss= 0.41998 train_acc= 1.00000 val_loss= 1.26196 val_acc= 0.76400 time= 0.02294\n",
      "Epoch: 0181 train_loss= 0.41838 train_acc= 1.00000 val_loss= 1.26022 val_acc= 0.76400 time= 0.02316\n",
      "Epoch: 0182 train_loss= 0.41681 train_acc= 1.00000 val_loss= 1.25851 val_acc= 0.76400 time= 0.02428\n",
      "Epoch: 0183 train_loss= 0.41526 train_acc= 1.00000 val_loss= 1.25683 val_acc= 0.76400 time= 0.02470\n",
      "Epoch: 0184 train_loss= 0.41371 train_acc= 1.00000 val_loss= 1.25512 val_acc= 0.76400 time= 0.02274\n",
      "Epoch: 0185 train_loss= 0.41218 train_acc= 1.00000 val_loss= 1.25318 val_acc= 0.76200 time= 0.02355\n",
      "Epoch: 0186 train_loss= 0.41067 train_acc= 1.00000 val_loss= 1.25115 val_acc= 0.76400 time= 0.02341\n",
      "Epoch: 0187 train_loss= 0.40917 train_acc= 1.00000 val_loss= 1.24940 val_acc= 0.76600 time= 0.02328\n",
      "Epoch: 0188 train_loss= 0.40770 train_acc= 1.00000 val_loss= 1.24771 val_acc= 0.76600 time= 0.02298\n",
      "Epoch: 0189 train_loss= 0.40623 train_acc= 1.00000 val_loss= 1.24608 val_acc= 0.76600 time= 0.02280\n",
      "Epoch: 0190 train_loss= 0.40477 train_acc= 1.00000 val_loss= 1.24455 val_acc= 0.76600 time= 0.02287\n",
      "Epoch: 0191 train_loss= 0.40332 train_acc= 1.00000 val_loss= 1.24288 val_acc= 0.76600 time= 0.02346\n",
      "Epoch: 0192 train_loss= 0.40190 train_acc= 1.00000 val_loss= 1.24105 val_acc= 0.76600 time= 0.02373\n",
      "Epoch: 0193 train_loss= 0.40049 train_acc= 1.00000 val_loss= 1.23940 val_acc= 0.76600 time= 0.02452\n",
      "Epoch: 0194 train_loss= 0.39909 train_acc= 1.00000 val_loss= 1.23796 val_acc= 0.76600 time= 0.02343\n",
      "Epoch: 0195 train_loss= 0.39770 train_acc= 1.00000 val_loss= 1.23631 val_acc= 0.76600 time= 0.02311\n",
      "Epoch: 0196 train_loss= 0.39634 train_acc= 1.00000 val_loss= 1.23452 val_acc= 0.76800 time= 0.02394\n",
      "Epoch: 0197 train_loss= 0.39498 train_acc= 1.00000 val_loss= 1.23279 val_acc= 0.76400 time= 0.02275\n",
      "Epoch: 0198 train_loss= 0.39363 train_acc= 1.00000 val_loss= 1.23121 val_acc= 0.76400 time= 0.02356\n",
      "Epoch: 0199 train_loss= 0.39228 train_acc= 1.00000 val_loss= 1.22967 val_acc= 0.76600 time= 0.02264\n",
      "Epoch: 0200 train_loss= 0.39096 train_acc= 1.00000 val_loss= 1.22826 val_acc= 0.76600 time= 0.02414\n",
      "Optimization Finished!\n",
      "total train time 4.88779\n",
      "Test set results: cost= 1.17233 accuracy= 0.78000 time= 0.01118\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(flags_dataset)\n",
    "\n",
    "# Some preprocessing\n",
    "features = preprocess_features(features)\n",
    "if flags_model == 'gcn':\n",
    "    support = [preprocess_adj(adj)]\n",
    "    num_supports = 1\n",
    "    model_func = GCN\n",
    "elif flags_model == 'gcn_cheby':\n",
    "    support = chebyshev_polynomials(adj, flags_max_degree)\n",
    "    num_supports = 1 + flags_max_degree\n",
    "    model_func = GCN\n",
    "elif flags_model == 'dense':\n",
    "    support = [preprocess_adj(adj)]  # Not used\n",
    "    num_supports = 1\n",
    "    model_func = MLP\n",
    "else:\n",
    "    raise ValueError('Invalid argument for model: ' + str(flags_model))\n",
    "\n",
    "# Define placeholders\n",
    "placeholders = {\n",
    "    'support': [tf.sparse_placeholder(tf.float32) for _ in range(num_supports)],\n",
    "    'features': tf.sparse_placeholder(tf.float32, shape=tf.constant(features[2], dtype=tf.int64)),\n",
    "    'labels': tf.placeholder(tf.float32, shape=(None, y_train.shape[1])),\n",
    "    'labels_mask': tf.placeholder(tf.int32),\n",
    "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "    'num_features_nonzero': tf.placeholder(tf.int32)  # helper variable for sparse dropout\n",
    "}\n",
    "\n",
    "# Create model\n",
    "model = model_func(placeholders, input_dim=features[2][1], logging=True)\n",
    "\n",
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "\n",
    "\n",
    "# Define model evaluation function\n",
    "def evaluate(features, support, labels, mask, placeholders):\n",
    "    t_test = time.time()\n",
    "    feed_dict_val = construct_feed_dict(features, support, labels, mask, placeholders)\n",
    "    outs_val = sess.run([model.loss, model.accuracy], feed_dict=feed_dict_val)\n",
    "    return outs_val[0], outs_val[1], (time.time() - t_test)\n",
    "\n",
    "\n",
    "# Init variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "cost_val = []\n",
    "\n",
    "t_begin = time.time()\n",
    "print(\"start training...\")\n",
    "# Train model\n",
    "for epoch in range(flags_epochs):\n",
    "\n",
    "    t = time.time()\n",
    "    # Construct feed dictionary\n",
    "    feed_dict = construct_feed_dict(features, support, y_train, train_mask, placeholders)\n",
    "    feed_dict.update({placeholders['dropout']: flags_dropout})\n",
    "\n",
    "    # Training step\n",
    "    outs = sess.run([model.opt_op, model.loss, model.accuracy], feed_dict=feed_dict)\n",
    "\n",
    "    # Validation\n",
    "    cost, acc, duration = evaluate(features, support, y_val, val_mask, placeholders)\n",
    "    cost_val.append(cost)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(outs[1]),\n",
    "          \"train_acc=\", \"{:.5f}\".format(outs[2]), \"val_loss=\", \"{:.5f}\".format(cost),\n",
    "          \"val_acc=\", \"{:.5f}\".format(acc), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "    if epoch > flags_early_stopping and cost_val[-1] > np.mean(cost_val[-(flags_early_stopping+1):-1]):\n",
    "        print(\"Early stopping...\")\n",
    "        break\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "print(\"total train time {:.5f}\".format(time.time() - t_begin))\n",
    "\n",
    "# Testing\n",
    "test_cost, test_acc, test_duration = evaluate(features, support, y_test, test_mask, placeholders)\n",
    "print(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost),\n",
    "      \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(test_duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
