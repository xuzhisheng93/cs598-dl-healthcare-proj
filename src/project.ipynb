{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter notebook is created to reproduce the results and experiment abalations of paper \"SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS\" by Thomas N. Kipf and Max Welling. It was published on \"Proceedings of the International Conference on Learning Representations (ICLR)\" and can be found at https://arxiv.org/pdf/1609.02907.pdf.\n",
    "\n",
    "The authors of the paper proposed a semi-supervised classification algorithm based on graph convolutional networks (GCNs). And the original code can be found at https://github.com/tkipf/gcn. In this notebook, we reused most of the code and made a few modifications. \n",
    "\n",
    "The main structure of this notebook is as follows:\n",
    "1. We modified the code so that it works in compatibility mode with tensorflow 2.12.0 and also added a few modifications so that we are able to explore different ablations.\n",
    "2. We put the code from original authors under `src/gcn` directory. And they were modified to be compatible with tensorflow 2.12.0.\n",
    "3. In the \"Methodology explanation and examples\" section, we explored all of our ablations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is included in the original GCN repository. Thus, there is no need to download it again. It can be found under `gcn/data` directory. The data are from article \"Collective classification in network data\" by Sen et al. (2008). Here is an overview of the datasets:\n",
    "\n",
    "| Dataset | Nodes   | Edges   | Features | Classes |\n",
    "|---------|---------|---------|----------|---------|\n",
    "| Citeseer| 3,327   | 4,732   | 3,703    | 6       |\n",
    "| Cora    | 2,708   | 5,429   | 1,433    | 7       |\n",
    "| Pubmed  | 19,717  | 44,338  | 500      | 3       |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducibility Summary\n",
    "\n",
    "By running this notebook, we are able to reproduce the results of GCN on all three datasets. In the \"Reproduce of Original Model\" section, it includes our reproduction of the original model. We achieved very similar results to original paper. Additionally, in the \"Methodology explanation and examples\" section, we explored all of our ablations. And results of these ablations can be found in the \"Results of Ablations\" section. \n",
    "- In terms of swapping activation functions, the majority of activation function (nn.tf.leaky_relu, nn.tf.tanh, nn.tf.elu) options resulted in very similar performance to the original results using tf.nn.relu. But the Sigmoid activation function yielded significantly worse results. \n",
    "- For the experiment with different optimizers, we achieved results close to the original findings using at least one combination of optimizers and learning rates.\n",
    "- For the experiment with different number of layers, by simply adding or removing the number of layers, the results becomes worse.\n",
    "\n",
    "<!-- TODO: A summary of the report and findings, about 200 words -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages and Set Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\xuzhi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from gcn.inits import *\n",
    "from gcn.utils import *\n",
    "from gcn.metrics import *\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers Class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global unique layer ID dictionary for layer name assignment\n",
    "_LAYER_UIDS = {}\n",
    "\n",
    "\n",
    "def get_layer_uid(layer_name=''):\n",
    "    \"\"\"Helper function, assigns unique layer IDs.\"\"\"\n",
    "    if layer_name not in _LAYER_UIDS:\n",
    "        _LAYER_UIDS[layer_name] = 1\n",
    "        return 1\n",
    "    else:\n",
    "        _LAYER_UIDS[layer_name] += 1\n",
    "        return _LAYER_UIDS[layer_name]\n",
    "\n",
    "\n",
    "def sparse_dropout(x, keep_prob, noise_shape):\n",
    "    \"\"\"Dropout for sparse tensors.\"\"\"\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += tf.random_uniform(noise_shape)\n",
    "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
    "    pre_out = tf.sparse_retain(x, dropout_mask)\n",
    "    return pre_out * (1./keep_prob)\n",
    "\n",
    "\n",
    "def dot(x, y, sparse=False):\n",
    "    \"\"\"Wrapper for tf.matmul (sparse vs dense).\"\"\"\n",
    "    if sparse:\n",
    "        res = tf.sparse_tensor_dense_matmul(x, y)\n",
    "    else:\n",
    "        res = tf.matmul(x, y)\n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    \"\"\"Base layer class. Defines basic API for all layer objects.\n",
    "    Implementation inspired by keras (http://keras.io).\n",
    "\n",
    "    # Properties\n",
    "        name: String, defines the variable scope of the layer.\n",
    "        logging: Boolean, switches Tensorflow histogram logging on/off\n",
    "\n",
    "    # Methods\n",
    "        _call(inputs): Defines computation graph of layer\n",
    "            (i.e. takes input, returns output)\n",
    "        __call__(inputs): Wrapper for _call()\n",
    "        _log_vars(): Log all variables\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        allowed_kwargs = {'name', 'logging'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            layer = self.__class__.__name__.lower()\n",
    "            name = layer + '_' + str(get_layer_uid(layer))\n",
    "        self.name = name\n",
    "        self.vars = {}\n",
    "        logging = kwargs.get('logging', False)\n",
    "        self.logging = logging\n",
    "        self.sparse_inputs = False\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        return inputs\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.name_scope(self.name):\n",
    "            if self.logging and not self.sparse_inputs:\n",
    "                tf.summary.histogram(self.name + '/inputs', inputs)\n",
    "            outputs = self._call(inputs)\n",
    "            if self.logging:\n",
    "                tf.summary.histogram(self.name + '/outputs', outputs)\n",
    "            return outputs\n",
    "\n",
    "    def _log_vars(self):\n",
    "        for var in self.vars:\n",
    "            tf.summary.histogram(self.name + '/vars/' + var, self.vars[var])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Convolutional Layer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(Layer):\n",
    "    \"\"\"Graph convolution layer.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, placeholders, dropout=0.,\n",
    "                 sparse_inputs=False, act=tf.nn.relu, bias=False,\n",
    "                 featureless=False, **kwargs):\n",
    "        super(GraphConvolution, self).__init__(**kwargs)\n",
    "\n",
    "        if dropout:\n",
    "            self.dropout = placeholders['dropout']\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "\n",
    "        self.act = act\n",
    "        self.support = placeholders['support']\n",
    "        self.sparse_inputs = sparse_inputs\n",
    "        self.featureless = featureless\n",
    "        self.bias = bias\n",
    "\n",
    "        # helper variable for sparse dropout\n",
    "        self.num_features_nonzero = placeholders['num_features_nonzero']\n",
    "\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            for i in range(len(self.support)):\n",
    "                self.vars['weights_' + str(i)] = glorot([input_dim, output_dim],\n",
    "                                                        name='weights_' + str(i))\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([output_dim], name='bias')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        # dropout\n",
    "        if self.sparse_inputs:\n",
    "            x = sparse_dropout(x, 1-self.dropout, self.num_features_nonzero)\n",
    "        else:\n",
    "            x = tf.nn.dropout(x, 1-self.dropout)\n",
    "\n",
    "        # convolve\n",
    "        supports = list()\n",
    "        for i in range(len(self.support)):\n",
    "            if not self.featureless:\n",
    "                pre_sup = dot(x, self.vars['weights_' + str(i)],\n",
    "                              sparse=self.sparse_inputs)\n",
    "            else:\n",
    "                pre_sup = self.vars['weights_' + str(i)]\n",
    "            support = dot(self.support[i], pre_sup, sparse=True)\n",
    "            supports.append(support)\n",
    "        output = tf.add_n(supports)\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "\n",
    "        return self.act(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        # print(\"new model, seed=\", tf.get_default_graph().seed)\n",
    "        allowed_kwargs = {'name', 'logging'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            name = self.__class__.__name__.lower()\n",
    "        self.name = name\n",
    "\n",
    "        logging = kwargs.get('logging', False)\n",
    "        self.logging = logging\n",
    "\n",
    "        self.vars = {}\n",
    "        self.placeholders = {}\n",
    "\n",
    "        self.layers = []\n",
    "        self.activations = []\n",
    "\n",
    "        self.inputs = None\n",
    "        self.outputs = None\n",
    "\n",
    "        self.loss = 0\n",
    "        self.accuracy = 0\n",
    "        self.optimizer = None\n",
    "        self.opt_op = None\n",
    "\n",
    "    def _build(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\" Wrapper for _build() \"\"\"\n",
    "        with tf.variable_scope(self.name):\n",
    "            self._build()\n",
    "\n",
    "        # Build sequential layer model\n",
    "        self.activations.append(self.inputs)\n",
    "        for layer in self.layers:\n",
    "            hidden = layer(self.activations[-1])\n",
    "            self.activations.append(hidden)\n",
    "        self.outputs = self.activations[-1]\n",
    "\n",
    "        # Store model variables for easy access\n",
    "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
    "        self.vars = {var.name: var for var in variables}\n",
    "\n",
    "        # Build metrics\n",
    "        self._loss()\n",
    "        self._accuracy()\n",
    "\n",
    "        self.opt_op = self.optimizer.minimize(self.loss)\n",
    "\n",
    "    def predict(self):\n",
    "        pass\n",
    "\n",
    "    def _loss(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _accuracy(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def save(self, sess=None):\n",
    "        if not sess:\n",
    "            raise AttributeError(\"TensorFlow session not provided.\")\n",
    "        saver = tf.train.Saver(self.vars)\n",
    "        save_path = saver.save(sess, \"tmp/%s.ckpt\" % self.name)\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "    def load(self, sess=None):\n",
    "        if not sess:\n",
    "            raise AttributeError(\"TensorFlow session not provided.\")\n",
    "        saver = tf.train.Saver(self.vars)\n",
    "        save_path = \"tmp/%s.ckpt\" % self.name\n",
    "        saver.restore(sess, save_path)\n",
    "        print(\"Model restored from file: %s\" % save_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(Model):\n",
    "    def __init__(self, placeholders, input_dim, **kwargs):\n",
    "        super(GCN, self).__init__(**kwargs)\n",
    "\n",
    "        # print(\"new GCN\")\n",
    "\n",
    "        self.inputs = placeholders['features']\n",
    "        self.input_dim = input_dim\n",
    "        # self.input_dim = self.inputs.get_shape().as_list()[1]  # To be supported in future Tensorflow versions\n",
    "        self.output_dim = placeholders['labels'].get_shape().as_list()[1]\n",
    "        self.placeholders = placeholders\n",
    "\n",
    "        self.optimizer = flags_optimizer(learning_rate=flags_learning_rate)\n",
    "\n",
    "        self.build()\n",
    "\n",
    "    def _loss(self):\n",
    "        # Weight decay loss\n",
    "        for var in self.layers[0].vars.values():\n",
    "            self.loss += flags_weight_decay * tf.nn.l2_loss(var)\n",
    "\n",
    "        # Cross entropy error\n",
    "        self.loss += masked_softmax_cross_entropy(self.outputs, self.placeholders['labels'],\n",
    "                                                  self.placeholders['labels_mask'])\n",
    "\n",
    "    def _accuracy(self):\n",
    "        self.accuracy = masked_accuracy(self.outputs, self.placeholders['labels'],\n",
    "                                        self.placeholders['labels_mask'])\n",
    "\n",
    "    def _build(self):\n",
    "\n",
    "        self.layers=[]\n",
    "\n",
    "        if flags_layers == 2:\n",
    "            # Paper layer configuration\n",
    "            self.layers.append(GraphConvolution(input_dim=self.input_dim,\n",
    "                                                output_dim=flags_hidden1,\n",
    "                                                placeholders=self.placeholders,\n",
    "                                                act=flags_act_func,\n",
    "                                                dropout=True,\n",
    "                                                sparse_inputs=True,\n",
    "                                                logging=self.logging))\n",
    "\n",
    "            self.layers.append(GraphConvolution(input_dim=flags_hidden1,\n",
    "                                                output_dim=self.output_dim,\n",
    "                                                placeholders=self.placeholders,\n",
    "                                                act=lambda x: x,\n",
    "                                                dropout=True,\n",
    "                                                logging=self.logging))\n",
    "        elif flags_layers == 1:\n",
    "            # Single layer configuration\n",
    "            self.layers.append(GraphConvolution(input_dim=self.input_dim,\n",
    "                                                output_dim=self.output_dim,\n",
    "                                                placeholders=self.placeholders,\n",
    "                                                act=flags_act_func,\n",
    "                                                dropout=True,\n",
    "                                                sparse_inputs=True,\n",
    "                                                logging=self.logging))\n",
    "        elif flags_layers == 3:\n",
    "            # Triple layer configuration\n",
    "            self.layers.append(GraphConvolution(input_dim=self.input_dim,\n",
    "                                                output_dim=64,\n",
    "                                                placeholders=self.placeholders,\n",
    "                                                act=flags_act_func,\n",
    "                                                dropout=True,\n",
    "                                                sparse_inputs=True,\n",
    "                                                logging=self.logging))\n",
    "\n",
    "            self.layers.append(GraphConvolution(input_dim=64,\n",
    "                                                output_dim=flags_hidden1,\n",
    "                                                placeholders=self.placeholders,\n",
    "                                                act=flags_act_func,\n",
    "                                                dropout=True,\n",
    "                                                logging=self.logging))\n",
    "\n",
    "            self.layers.append(GraphConvolution(input_dim=flags_hidden1,\n",
    "                                                output_dim=self.output_dim,\n",
    "                                                placeholders=self.placeholders,\n",
    "                                                act=lambda x: x,\n",
    "                                                dropout=True,\n",
    "                                                logging=self.logging))\n",
    "\n",
    "    def predict(self):\n",
    "        return tf.nn.softmax(self.outputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default Settings\n",
    "flags_dataset = 'cora'# , 'Dataset string.')  # 'cora', 'citeseer', 'pubmed'\n",
    "flags_model = 'gcn' #, 'Model string.')  # 'gcn', 'gcn_cheby', 'dense'\n",
    "flags_learning_rate = 0.01 #, 'Initial learning rate.')\n",
    "flags_epochs = 200 #, 'Number of epochs to train.')\n",
    "flags_hidden1 = 16 #, 'Number of units in hidden layer 1.')\n",
    "flags_dropout = 0.5 #, 'Dropout rate (1 - keep probability).')\n",
    "flags_weight_decay = 5e-4 #, 'Weight for L2 loss on embedding matrix.')\n",
    "flags_early_stopping = 10 #, 'Tolerance for early stopping (# of epochs).')\n",
    "flags_max_degree = 3 #, 'Maximum Chebyshev polynomial degree.')\n",
    "flags_act_func = tf.nn.relu # Activation function: tf.nn.relu, tf.nn.leaky_relu, tf.nn.sigmoid, tf.nn.tanh, tf.nn.elu\n",
    "flags_optimizer = tf.train.AdamOptimizer # Optimizer: tf.train.AdamOptimizer, tf.train.GradientDescentOptimizer, tf.train.AdadeltaOptimizer, tf.train.RMSPropOptimizer\n",
    "flags_layers = 2 # layers: 1, 2, 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to Train Model\n",
    "\n",
    "The following functions are used to train the model. It was a reuse of `train.py` provided by original authors. We've modified it to fit into the Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model evaluation function\n",
    "def evaluate(features, support, labels, mask, placeholders, sess, model):\n",
    "    t_test = time.time()\n",
    "    feed_dict_val = construct_feed_dict(features, support, labels, mask, placeholders)\n",
    "    outs_val = sess.run([model.loss, model.accuracy], feed_dict=feed_dict_val)\n",
    "    return outs_val[0], outs_val[1], (time.time() - t_test)\n",
    "\n",
    "def train(adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask):\n",
    "\n",
    "    \"\"\" reset global \"\"\"\n",
    "    _LAYER_UIDS = {}\n",
    "    \n",
    "    # Some preprocessing\n",
    "    features = preprocess_features(features)\n",
    "    support = [preprocess_adj(adj)]\n",
    "    num_supports = 1\n",
    "\n",
    "    # Define placeholders\n",
    "    placeholders = {\n",
    "        'support': [tf.sparse_placeholder(tf.float32) for _ in range(num_supports)],\n",
    "        'features': tf.sparse_placeholder(tf.float32, shape=tf.constant(features[2], dtype=tf.int64)),\n",
    "        'labels': tf.placeholder(tf.float32, shape=(None, y_train.shape[1])),\n",
    "        'labels_mask': tf.placeholder(tf.int32),\n",
    "        'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "        'num_features_nonzero': tf.placeholder(tf.int32)  # helper variable for sparse dropout\n",
    "    }\n",
    "\n",
    "    # Create model\n",
    "    model = GCN(placeholders, input_dim=features[2][1], logging=True)\n",
    "\n",
    "    # Initialize session\n",
    "    sess = tf.Session()\n",
    "    # Init variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    cost_val = []\n",
    "\n",
    "    t_begin = time.time()\n",
    "\n",
    "    # Train model\n",
    "    for epoch in range(flags_epochs):\n",
    "\n",
    "        t = time.time()\n",
    "        # Construct feed dictionary\n",
    "        feed_dict = construct_feed_dict(features, support, y_train, train_mask, placeholders)\n",
    "        feed_dict.update({placeholders['dropout']: flags_dropout})\n",
    "\n",
    "        # Training step\n",
    "        outs = sess.run([model.opt_op, model.loss, model.accuracy], feed_dict=feed_dict)\n",
    "\n",
    "        # Validation\n",
    "        cost, acc, duration = evaluate(features, support, y_val, val_mask, placeholders, sess, model)\n",
    "        cost_val.append(cost)\n",
    "\n",
    "        # Print results\n",
    "        # print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(outs[1]),\n",
    "        #     \"train_acc=\", \"{:.5f}\".format(outs[2]), \"val_loss=\", \"{:.5f}\".format(cost),\n",
    "        #     \"val_acc=\", \"{:.5f}\".format(acc), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "        if epoch % 5 == 0:\n",
    "            print(\".\", end=\"\")\n",
    "\n",
    "        if epoch > flags_early_stopping and cost_val[-1] > np.mean(cost_val[-(flags_early_stopping+1):-1]):\n",
    "            print(\"Early stopping...\")\n",
    "            break\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "    # print(\"total train time {:.5f}\".format(time.time() - t_begin))\n",
    "    duration = time.time() - t_begin\n",
    "\n",
    "    # Testing\n",
    "    test_cost, test_acc, test_duration = evaluate(features, support, y_test, test_mask, placeholders, sess, model)\n",
    "    sess.close()\n",
    "    return test_cost, test_acc, duration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduce of Original Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resetAllRandomSeeds():\n",
    "    seed = 123\n",
    "    np.random.seed(seed)\n",
    "    tf.reset_default_graph() # reset session\n",
    "    tf.set_random_seed(seed)\n",
    "    tf.config.set_soft_device_placement(True)\n",
    "\n",
    "def loadDataAndTrain():\n",
    "    adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(flags_dataset)\n",
    "    return train(adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\xuzhi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From c:\\Users\\xuzhi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\uiuc_mcs\\cs598-dl-healthcare-proj\\src\\gcn\\utils.py:70: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................................\n",
      "[cora] Test set results: cost=1.01651, accuracy=0.81700, time=2.48694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\uiuc_mcs\\cs598-dl-healthcare-proj\\src\\gcn\\utils.py:115: RuntimeWarning: divide by zero encountered in power\n",
      "  r_inv = np.power(rowsum, -1).flatten()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................................\n",
      "[citeseer] Test set results: cost=1.30973, accuracy=0.70600, time=2.82150\n",
      "...............................Early stopping...\n",
      "\n",
      "[pubmed] Test set results: cost=0.72828, accuracy=0.79400, time=9.65042\n"
     ]
    }
   ],
   "source": [
    "dataset_list = ['cora', 'citeseer', 'pubmed']\n",
    "\n",
    "for flags_dataset in dataset_list:\n",
    "     resetAllRandomSeeds()\n",
    "     test_cost, test_acc, test_duration = loadDataAndTrain()\n",
    "     print(\"[{}] Test set results: cost={cost:.5f}, accuracy={accuracy:.5f}, time={time:.5f}\".format(\n",
    "          flags_dataset, cost=test_cost, accuracy=test_acc, time=test_duration))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology explanation and examples\n",
    "\n",
    "Here we loop through the three datasets and train the model on each of them with different combination of optimizer, activation function and number of layers. It will call function `train()` defined in the above cell to perform the training. We keep all the other parameters the same as the original paper. The `train()` function creates the GCN model and train it on the dataset. It will return the accuracy on the test set which will be later recorded in the Result section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list = ['cora', 'citeseer', 'pubmed']\n",
    "optimizer_list = [tf.train.AdamOptimizer, tf.train.GradientDescentOptimizer, tf.train.AdadeltaOptimizer, tf.train.RMSPropOptimizer]\n",
    "activation_list = [tf.nn.relu, tf.nn.leaky_relu, tf.nn.sigmoid, tf.nn.tanh, tf.nn.elu]\n",
    "lr_list = [0.01, 0.99]\n",
    "layers_list = [2, 1, 3]\n",
    "result = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== cora begin ===========\n",
      "+ [cora] activation function trial begin\n",
      "........................................\n",
      "+ + [cora][relu] Test set results: cost=1.01649, accuracy=0.81700, time=1.77342\n",
      "........................................\n",
      "+ + [cora][leaky_relu] Test set results: cost=1.01414, accuracy=0.81300, time=1.73895\n",
      "...Early stopping...\n",
      "\n",
      "+ + [cora][sigmoid] Test set results: cost=1.95441, accuracy=0.09300, time=0.18706\n",
      "........................................\n",
      "+ + [cora][tanh] Test set results: cost=0.97253, accuracy=0.81600, time=1.83278\n",
      "........................................\n",
      "+ + [cora][elu] Test set results: cost=0.97768, accuracy=0.81700, time=1.80212\n",
      "+ [cora] optimizer trial begin\n",
      "........................................\n",
      "+ + [cora][AdamOptimizer][0.01] Test set results: cost=1.01651, accuracy=0.81700, time=2.03768\n",
      "......Early stopping...\n",
      "\n",
      "+ + [cora][AdamOptimizer][0.99] Test set results: cost=2.62066, accuracy=0.71800, time=0.31936\n",
      "........................................\n",
      "+ + [cora][GradientDescentOptimizer][0.01] Test set results: cost=1.95306, accuracy=0.14400, time=1.89835\n",
      "........................................\n",
      "+ + [cora][GradientDescentOptimizer][0.99] Test set results: cost=1.90279, accuracy=0.44400, time=2.14508\n",
      "........................................\n",
      "+ + [cora][AdadeltaOptimizer][0.01] Test set results: cost=1.95313, accuracy=0.14500, time=2.45829\n",
      "........................................\n",
      "+ + [cora][AdadeltaOptimizer][0.99] Test set results: cost=1.92976, accuracy=0.38800, time=2.90695\n",
      "WARNING:tensorflow:From c:\\Users\\xuzhi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\rmsprop.py:188: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "........................................\n",
      "+ + [cora][RMSPropOptimizer][0.01] Test set results: cost=1.41405, accuracy=0.76000, time=2.52545\n",
      "................Early stopping...\n",
      "\n",
      "+ + [cora][RMSPropOptimizer][0.99] Test set results: cost=1.05779, accuracy=0.73100, time=1.12679\n",
      "+ [cora] layers trial begin\n",
      "........................................\n",
      "+ + [cora][2] Test set results: cost=1.01651, accuracy=0.81700, time=2.86451\n",
      "................Early stopping...\n",
      "\n",
      "+ + [cora][1] Test set results: cost=1.90915, accuracy=0.74200, time=1.30080\n",
      "...............Early stopping...\n",
      "\n",
      "+ + [cora][3] Test set results: cost=0.84225, accuracy=0.79600, time=1.41737\n",
      "=========== cora end ===========\n",
      "=========== citeseer begin ===========\n",
      "+ [citeseer] activation function trial begin\n",
      "........................................\n",
      "+ + [citeseer][relu] Test set results: cost=1.30974, accuracy=0.70600, time=4.91659\n",
      "........................................\n",
      "+ + [citeseer][leaky_relu] Test set results: cost=1.30573, accuracy=0.70400, time=4.43419\n",
      "...Early stopping...\n",
      "\n",
      "+ + [citeseer][sigmoid] Test set results: cost=1.81644, accuracy=0.07700, time=0.26528\n",
      "........................................\n",
      "+ + [citeseer][tanh] Test set results: cost=1.26322, accuracy=0.70400, time=3.19977\n",
      "........................................\n",
      "+ + [citeseer][elu] Test set results: cost=1.26523, accuracy=0.70100, time=2.95690\n",
      "+ [citeseer] optimizer trial begin\n",
      "........................................\n",
      "+ + [citeseer][AdamOptimizer][0.01] Test set results: cost=1.30971, accuracy=0.70600, time=2.73628\n",
      "....Early stopping...\n",
      "\n",
      "+ + [citeseer][AdamOptimizer][0.99] Test set results: cost=4.29128, accuracy=0.67900, time=0.31558\n",
      "........................................\n",
      "+ + [citeseer][GradientDescentOptimizer][0.01] Test set results: cost=1.79932, accuracy=0.26900, time=2.46332\n",
      "........................................\n",
      "+ + [citeseer][GradientDescentOptimizer][0.99] Test set results: cost=1.78069, accuracy=0.44500, time=2.40614\n",
      "........................................\n",
      "+ + [citeseer][AdadeltaOptimizer][0.01] Test set results: cost=1.79933, accuracy=0.26800, time=2.39501\n",
      "........................................\n",
      "+ + [citeseer][AdadeltaOptimizer][0.99] Test set results: cost=1.78703, accuracy=0.43300, time=2.38563\n",
      "........................................\n",
      "+ + [citeseer][RMSPropOptimizer][0.01] Test set results: cost=1.55498, accuracy=0.66000, time=2.39031\n",
      ".................Early stopping...\n",
      "\n",
      "+ + [citeseer][RMSPropOptimizer][0.99] Test set results: cost=1.13253, accuracy=0.69700, time=1.12571\n",
      "+ [citeseer] layers trial begin\n",
      "........................................\n",
      "+ + [citeseer][2] Test set results: cost=1.30974, accuracy=0.70600, time=2.43051\n",
      "...Early stopping...\n",
      "\n",
      "+ + [citeseer][1] Test set results: cost=1.78711, accuracy=0.65200, time=0.23336\n",
      "...........Early stopping...\n",
      "\n",
      "+ + [citeseer][3] Test set results: cost=1.30615, accuracy=0.67000, time=0.71496\n",
      "=========== citeseer end ===========\n",
      "=========== pubmed begin ===========\n",
      "+ [pubmed] activation function trial begin\n",
      "...............................Early stopping...\n",
      "\n",
      "+ + [pubmed][relu] Test set results: cost=0.72828, accuracy=0.79400, time=9.74098\n",
      "................................Early stopping...\n",
      "\n",
      "+ + [pubmed][leaky_relu] Test set results: cost=0.72966, accuracy=0.78900, time=10.24529\n",
      ".....Early stopping...\n",
      "\n",
      "+ + [pubmed][sigmoid] Test set results: cost=1.10906, accuracy=0.21800, time=1.53105\n",
      "................................Early stopping...\n",
      "\n",
      "+ + [pubmed][tanh] Test set results: cost=0.70819, accuracy=0.79300, time=13.38469\n",
      "................................Early stopping...\n",
      "\n",
      "+ + [pubmed][elu] Test set results: cost=0.71049, accuracy=0.79100, time=18.21972\n",
      "+ [pubmed] optimizer trial begin\n",
      "...............................Early stopping...\n",
      "\n",
      "+ + [pubmed][AdamOptimizer][0.01] Test set results: cost=0.72828, accuracy=0.79400, time=15.27326\n",
      "...Early stopping...\n",
      "\n",
      "+ + [pubmed][AdamOptimizer][0.99] Test set results: cost=1.96890, accuracy=0.66600, time=1.02529\n",
      "........................................\n",
      "+ + [pubmed][GradientDescentOptimizer][0.01] Test set results: cost=1.10629, accuracy=0.32800, time=13.99222\n",
      "........................................\n",
      "+ + [pubmed][GradientDescentOptimizer][0.99] Test set results: cost=1.04026, accuracy=0.54700, time=12.90024\n",
      "........................................\n",
      "+ + [pubmed][AdadeltaOptimizer][0.01] Test set results: cost=1.10631, accuracy=0.32700, time=12.64044\n",
      "........................................\n",
      "+ + [pubmed][AdadeltaOptimizer][0.99] Test set results: cost=1.08722, accuracy=0.60800, time=12.76841\n",
      "........................................\n",
      "+ + [pubmed][RMSPropOptimizer][0.01] Test set results: cost=0.78005, accuracy=0.77400, time=13.07517\n",
      "...............Early stopping...\n",
      "\n",
      "+ + [pubmed][RMSPropOptimizer][0.99] Test set results: cost=0.69200, accuracy=0.76500, time=4.99707\n",
      "+ [pubmed] layers trial begin\n",
      "...............................Early stopping...\n",
      "\n",
      "+ + [pubmed][2] Test set results: cost=0.72828, accuracy=0.79400, time=9.93281\n",
      "...............Early stopping...\n",
      "\n",
      "+ + [pubmed][1] Test set results: cost=1.06566, accuracy=0.72400, time=4.66522\n",
      ".......Early stopping...\n",
      "\n",
      "+ + [pubmed][3] Test set results: cost=0.82386, accuracy=0.74700, time=2.34966\n",
      "=========== pubmed end ===========\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\" RESET GLOBALS \"\"\"\n",
    "_LAYER_UIDS = {}\n",
    "\n",
    "for flags_dataset in dataset_list:\n",
    "    print(\"=========== {} begin ===========\".format(flags_dataset))\n",
    "    if flags_dataset not in result.keys():\n",
    "        result[flags_dataset] = {}\n",
    "    print(\"+ [{}] activation function trial begin\".format(flags_dataset))\n",
    "    result[flags_dataset]['activation'] = {}\n",
    "    for flags_act_func in activation_list:\n",
    "        # adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(flags_dataset)\n",
    "        # test_cost, test_acc, test_duration = train(adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask)\n",
    "        resetAllRandomSeeds()\n",
    "        test_cost, test_acc, test_duration = loadDataAndTrain()\n",
    "        print(\"+ + [{}][{}] Test set results: cost={cost:.5f}, accuracy={accuracy:.5f}, time={time:.5f}\".format(\n",
    "            flags_dataset, flags_act_func.__name__, cost=test_cost, accuracy=test_acc, time=test_duration))\n",
    "        result[flags_dataset]['activation'][flags_act_func.__name__] = {\"cost\": test_cost, \"accuracy\": test_acc, \"time\": test_duration}\n",
    "    # reset activation function\n",
    "    flags_act_func = tf.nn.relu\n",
    "\n",
    "    \n",
    "    print(\"+ [{}] optimizer trial begin\".format(flags_dataset))\n",
    "    result[flags_dataset]['optimizer'] = {}\n",
    "    for flags_optimizer in optimizer_list:\n",
    "        result[flags_dataset]['optimizer'][flags_optimizer.__name__] = []\n",
    "        for flags_learning_rate in lr_list:\n",
    "            resetAllRandomSeeds()\n",
    "            test_cost, test_acc, test_duration = loadDataAndTrain()\n",
    "            print(\"+ + [{}][{}][{lr}] Test set results: cost={cost:.5f}, accuracy={accuracy:.5f}, time={time:.5f}\".format(\n",
    "                flags_dataset, flags_optimizer.__name__, cost=test_cost, accuracy=test_acc, time=test_duration, lr=flags_learning_rate))\n",
    "            result[flags_dataset]['optimizer'][flags_optimizer.__name__].append({\"cost\": test_cost, \"accuracy\": test_acc, \"time\": test_duration})\n",
    "    # reset optimizer\n",
    "    flags_optimizer = tf.train.AdamOptimizer\n",
    "    flags_learning_rate = 0.01\n",
    "    \n",
    "    print(\"+ [{}] layers trial begin\".format(flags_dataset))\n",
    "    result[flags_dataset]['layers'] = {}\n",
    "    for flags_layers in layers_list:\n",
    "        resetAllRandomSeeds()\n",
    "        test_cost, test_acc, test_duration = loadDataAndTrain()\n",
    "        print(\"+ + [{}][{}] Test set results: cost={cost:.5f}, accuracy={accuracy:.5f}, time={time:.5f}\".format(\n",
    "            flags_dataset, flags_layers, cost=test_cost, accuracy=test_acc, time=test_duration))\n",
    "        result[flags_dataset]['layers'][flags_layers] = {\"cost\": test_cost, \"accuracy\": test_acc, \"time\": test_duration}\n",
    "    # reset number of layers\n",
    "    flags_layers = 2\n",
    "\n",
    "    print(\"=========== {} end ===========\".format(flags_dataset))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of Ablations\n",
    "\n",
    "Here we produced all the results of our ablations. We loop through the three datasets and train the model on each of them with different combination of optimizer, activation function and number of layers. It will call function `train()` defined in the above cell to perform the training. We keep all the other parameters the same as the original paper. The `train()` function creates the GCN model and train it on the dataset. It will return the accuracy on the test set which is recorded in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cora\n",
      "+ activation\n",
      "+ + relu\n",
      "+ + + cost 1.0164862\n",
      "+ + + accuracy 0.8169999\n",
      "+ + + time 1.7734153270721436\n",
      "+ + leaky_relu\n",
      "+ + + cost 1.0141354\n",
      "+ + + accuracy 0.813\n",
      "+ + + time 1.738950490951538\n",
      "+ + sigmoid\n",
      "+ + + cost 1.9544058\n",
      "+ + + accuracy 0.092999995\n",
      "+ + + time 0.1870582103729248\n",
      "+ + tanh\n",
      "+ + + cost 0.97252643\n",
      "+ + + accuracy 0.816\n",
      "+ + + time 1.8327789306640625\n",
      "+ + elu\n",
      "+ + + cost 0.97768116\n",
      "+ + + accuracy 0.81700003\n",
      "+ + + time 1.8021161556243896\n",
      "+ optimizer\n",
      "+ + AdamOptimizer\n",
      "+ + + lr=0.01\n",
      "+ + + + cost 1.0165086\n",
      "+ + + + accuracy 0.8169999\n",
      "+ + + + time 2.0376811027526855\n",
      "+ + + lr=0.99\n",
      "+ + + + cost 2.6206574\n",
      "+ + + + accuracy 0.718\n",
      "+ + + + time 0.31935954093933105\n",
      "+ + GradientDescentOptimizer\n",
      "+ + + lr=0.01\n",
      "+ + + + cost 1.9530557\n",
      "+ + + + accuracy 0.144\n",
      "+ + + + time 1.898348093032837\n",
      "+ + + lr=0.99\n",
      "+ + + + cost 1.9027857\n",
      "+ + + + accuracy 0.44399998\n",
      "+ + + + time 2.1450817584991455\n",
      "+ + AdadeltaOptimizer\n",
      "+ + + lr=0.01\n",
      "+ + + + cost 1.9531313\n",
      "+ + + + accuracy 0.145\n",
      "+ + + + time 2.458294630050659\n",
      "+ + + lr=0.99\n",
      "+ + + + cost 1.9297608\n",
      "+ + + + accuracy 0.38799998\n",
      "+ + + + time 2.906947135925293\n",
      "+ + RMSPropOptimizer\n",
      "+ + + lr=0.01\n",
      "+ + + + cost 1.414048\n",
      "+ + + + accuracy 0.76000005\n",
      "+ + + + time 2.525453567504883\n",
      "+ + + lr=0.99\n",
      "+ + + + cost 1.0577899\n",
      "+ + + + accuracy 0.73099995\n",
      "+ + + + time 1.1267893314361572\n",
      "+ layers\n",
      "+ + 2\n",
      "+ + + cost 1.0165105\n",
      "+ + + accuracy 0.8169999\n",
      "+ + + time 2.864514112472534\n",
      "+ + 1\n",
      "+ + + cost 1.9091494\n",
      "+ + + accuracy 0.742\n",
      "+ + + time 1.3008029460906982\n",
      "+ + 3\n",
      "+ + + cost 0.8422531\n",
      "+ + + accuracy 0.79599994\n",
      "+ + + time 1.4173734188079834\n",
      "citeseer\n",
      "+ activation\n",
      "+ + relu\n",
      "+ + + cost 1.3097384\n",
      "+ + + accuracy 0.706\n",
      "+ + + time 4.9165942668914795\n",
      "+ + leaky_relu\n",
      "+ + + cost 1.3057289\n",
      "+ + + accuracy 0.70400006\n",
      "+ + + time 4.434187889099121\n",
      "+ + sigmoid\n",
      "+ + + cost 1.8164393\n",
      "+ + + accuracy 0.07700001\n",
      "+ + + time 0.26528143882751465\n",
      "+ + tanh\n",
      "+ + + cost 1.2632229\n",
      "+ + + accuracy 0.704\n",
      "+ + + time 3.1997663974761963\n",
      "+ + elu\n",
      "+ + + cost 1.2652334\n",
      "+ + + accuracy 0.70100003\n",
      "+ + + time 2.9569034576416016\n",
      "+ optimizer\n",
      "+ + AdamOptimizer\n",
      "+ + + lr=0.01\n",
      "+ + + + cost 1.3097079\n",
      "+ + + + accuracy 0.706\n",
      "+ + + + time 2.736284017562866\n",
      "+ + + lr=0.99\n",
      "+ + + + cost 4.291281\n",
      "+ + + + accuracy 0.6790001\n",
      "+ + + + time 0.3155796527862549\n",
      "+ + GradientDescentOptimizer\n",
      "+ + + lr=0.01\n",
      "+ + + + cost 1.7993151\n",
      "+ + + + accuracy 0.269\n",
      "+ + + + time 2.463320016860962\n",
      "+ + + lr=0.99\n",
      "+ + + + cost 1.7806909\n",
      "+ + + + accuracy 0.445\n",
      "+ + + + time 2.406144380569458\n",
      "+ + AdadeltaOptimizer\n",
      "+ + + lr=0.01\n",
      "+ + + + cost 1.7993264\n",
      "+ + + + accuracy 0.268\n",
      "+ + + + time 2.395012378692627\n",
      "+ + + lr=0.99\n",
      "+ + + + cost 1.7870291\n",
      "+ + + + accuracy 0.43299997\n",
      "+ + + + time 2.3856348991394043\n",
      "+ + RMSPropOptimizer\n",
      "+ + + lr=0.01\n",
      "+ + + + cost 1.5549824\n",
      "+ + + + accuracy 0.66\n",
      "+ + + + time 2.3903074264526367\n",
      "+ + + lr=0.99\n",
      "+ + + + cost 1.1325307\n",
      "+ + + + accuracy 0.69699997\n",
      "+ + + + time 1.1257073879241943\n",
      "+ layers\n",
      "+ + 2\n",
      "+ + + cost 1.3097384\n",
      "+ + + accuracy 0.706\n",
      "+ + + time 2.430513620376587\n",
      "+ + 1\n",
      "+ + + cost 1.787108\n",
      "+ + + accuracy 0.652\n",
      "+ + + time 0.23336243629455566\n",
      "+ + 3\n",
      "+ + + cost 1.3061514\n",
      "+ + + accuracy 0.67\n",
      "+ + + time 0.7149567604064941\n",
      "pubmed\n",
      "+ activation\n",
      "+ + relu\n",
      "+ + + cost 0.72828436\n",
      "+ + + accuracy 0.79399997\n",
      "+ + + time 9.740981817245483\n",
      "+ + leaky_relu\n",
      "+ + + cost 0.7296622\n",
      "+ + + accuracy 0.7889999\n",
      "+ + + time 10.245285511016846\n",
      "+ + sigmoid\n",
      "+ + + cost 1.1090643\n",
      "+ + + accuracy 0.21799998\n",
      "+ + + time 1.5310490131378174\n",
      "+ + tanh\n",
      "+ + + cost 0.7081878\n",
      "+ + + accuracy 0.793\n",
      "+ + + time 13.384689569473267\n",
      "+ + elu\n",
      "+ + + cost 0.71048594\n",
      "+ + + accuracy 0.79099995\n",
      "+ + + time 18.219722747802734\n",
      "+ optimizer\n",
      "+ + AdamOptimizer\n",
      "+ + + lr=0.01\n",
      "+ + + + cost 0.72828436\n",
      "+ + + + accuracy 0.79399997\n",
      "+ + + + time 15.273262023925781\n",
      "+ + + lr=0.99\n",
      "+ + + + cost 1.9689016\n",
      "+ + + + accuracy 0.66599995\n",
      "+ + + + time 1.025285243988037\n",
      "+ + GradientDescentOptimizer\n",
      "+ + + lr=0.01\n",
      "+ + + + cost 1.1062919\n",
      "+ + + + accuracy 0.32799998\n",
      "+ + + + time 13.992219686508179\n",
      "+ + + lr=0.99\n",
      "+ + + + cost 1.0402629\n",
      "+ + + + accuracy 0.547\n",
      "+ + + + time 12.900237321853638\n",
      "+ + AdadeltaOptimizer\n",
      "+ + + lr=0.01\n",
      "+ + + + cost 1.1063143\n",
      "+ + + + accuracy 0.327\n",
      "+ + + + time 12.640442848205566\n",
      "+ + + lr=0.99\n",
      "+ + + + cost 1.0872197\n",
      "+ + + + accuracy 0.608\n",
      "+ + + + time 12.768409252166748\n",
      "+ + RMSPropOptimizer\n",
      "+ + + lr=0.01\n",
      "+ + + + cost 0.7800458\n",
      "+ + + + accuracy 0.774\n",
      "+ + + + time 13.075170755386353\n",
      "+ + + lr=0.99\n",
      "+ + + + cost 0.6920026\n",
      "+ + + + accuracy 0.7649999\n",
      "+ + + + time 4.9970667362213135\n",
      "+ layers\n",
      "+ + 2\n",
      "+ + + cost 0.72828436\n",
      "+ + + accuracy 0.79399997\n",
      "+ + + time 9.932807683944702\n",
      "+ + 1\n",
      "+ + + cost 1.0656586\n",
      "+ + + accuracy 0.724\n",
      "+ + + time 4.665218114852905\n",
      "+ + 3\n",
      "+ + + cost 0.82385576\n",
      "+ + + accuracy 0.7469999\n",
      "+ + + time 2.3496575355529785\n"
     ]
    }
   ],
   "source": [
    "for dataset in result.keys():\n",
    "    print(dataset)\n",
    "    for ablation in result[dataset]:\n",
    "        print(\"+\", ablation)\n",
    "        if ablation == 'optimizer':\n",
    "            for opt in result[dataset][ablation]:\n",
    "                print(\"+\", \"+\", opt)\n",
    "                for lr_idx in range(len(result[dataset][ablation][opt])):\n",
    "                    print(\"+\", \"+\", \"+\", \"lr={}\".format(lr_list[lr_idx]))\n",
    "                    for key, val in result[dataset][ablation][opt][lr_idx].items():\n",
    "                        print(\"+\", \"+\", \"+\", \"+\", key, val)\n",
    "        if ablation == 'activation':\n",
    "            for act in result[dataset][ablation]:\n",
    "                print(\"+\", \"+\", act)\n",
    "                for key, val in result[dataset][ablation][act].items():\n",
    "                    print(\"+\", \"+\", \"+\", key, val)\n",
    "        if ablation == \"layers\":\n",
    "            for layer in result[dataset][ablation]:\n",
    "                print(\"+\", \"+\", layer)\n",
    "                for key, val in result[dataset][ablation][layer].items():\n",
    "                    print(\"+\", \"+\", \"+\", key, val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a conclusion of above results in a table format:\n",
    "\n",
    "Exploration with different activation functions:\n",
    "\n",
    "| Activation Function | Cora  | Citeseer | PubMed |\n",
    "|---------------------|-------|----------|--------|\n",
    "| ReLU                | 0.817 | 0.706    | 0.794  |\n",
    "| Sigmoid             | 0.092 | 0.077    | 0.218  |\n",
    "| Tanh                | 0.816 | 0.704    | 0.793  |\n",
    "| Leaky ReLU          | 0.813 | 0.704    | 0.789  |\n",
    "| ELU                 | 0.817 | 0.701    | 0.791  |\n",
    "\n",
    "Exploration with different optimizers and learning rates:\n",
    "\n",
    "| Optimizer (LR)      | Cora  | Citeseer | PubMed |\n",
    "|---------------------|-------|----------|--------|\n",
    "| Adam (0.01)         | 0.818 | 0.706    | 0.794  |\n",
    "| Adam (0.99)         | 0.718 | 0.679    | 0.666  |\n",
    "| Adadelta (0.01)     | 0.145 | 0.268    | 0.327  |\n",
    "| Adadelta (0.99)     | 0.388 | 0.433    | 0.608  |\n",
    "| RMSProp (0.01)      | 0.760 | 0.660    | 0.774  |\n",
    "| RMSProp (0.99)      | 0.729 | 0.697    | 0.765  |\n",
    "| Gradient Descent (0.01) | 0.144 | 0.269 | 0.328  |\n",
    "| Gradient Descent (0.99) | 0.444 | 0.445 | 0.547  |\n",
    "\n",
    "Exploration with different number of layers:\n",
    "\n",
    "| Number of Layers   | Cora  | Citeseer | PubMed |\n",
    "|--------------------|-------|----------|--------|\n",
    "| 2 (original setup) | 0.817 | 0.706    | 0.794  |\n",
    "| 1 (2nd layer removed) | 0.742 | 0.652 | 0.724  |\n",
    "| 3 (added ReLU layer) | 0.796 | 0.670 | 0.747  |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Kipf, T. N., & Welling, M. (2017). Semi-supervised classification with graph convolutional networks. In Proceedings of the International Conference on Learning Representations (ICLR).\n",
    "2. Kipf, T. (n.d.). gcn: Implementation of Graph Convolutional Networks in TensorFlow. GitHub. https://github.com/tkipf/gcn\n",
    "3. Sen, P., Namata, G., Bilgic, M., Getoor, L., Gallagher, B., & Eliassi-Rad, T. (2008). Collective classification in network data. AI Magazine, 29(3), 93."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
