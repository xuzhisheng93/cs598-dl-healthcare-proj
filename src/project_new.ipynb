{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-06 21:01:42.751941: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/goutham/anaconda3/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from gcn.inits import *\n",
    "from gcn.utils import *\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "# flags = tf.compat.v1.flags\n",
    "# FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Layers Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global unique layer ID dictionary for layer name assignment\n",
    "_LAYER_UIDS = {}\n",
    "\n",
    "\n",
    "def get_layer_uid(layer_name=''):\n",
    "    \"\"\"Helper function, assigns unique layer IDs.\"\"\"\n",
    "    if layer_name not in _LAYER_UIDS:\n",
    "        _LAYER_UIDS[layer_name] = 1\n",
    "        return 1\n",
    "    else:\n",
    "        _LAYER_UIDS[layer_name] += 1\n",
    "        return _LAYER_UIDS[layer_name]\n",
    "\n",
    "\n",
    "def sparse_dropout(x, keep_prob, noise_shape):\n",
    "    \"\"\"Dropout for sparse tensors.\"\"\"\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += tf.random_uniform(noise_shape)\n",
    "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
    "    pre_out = tf.sparse_retain(x, dropout_mask)\n",
    "    return pre_out * (1./keep_prob)\n",
    "\n",
    "\n",
    "def dot(x, y, sparse=False):\n",
    "    \"\"\"Wrapper for tf.matmul (sparse vs dense).\"\"\"\n",
    "    if sparse:\n",
    "        res = tf.sparse_tensor_dense_matmul(x, y)\n",
    "    else:\n",
    "        res = tf.matmul(x, y)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Layer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    \"\"\"Base layer class. Defines basic API for all layer objects.\n",
    "    Implementation inspired by keras (http://keras.io).\n",
    "\n",
    "    # Properties\n",
    "        name: String, defines the variable scope of the layer.\n",
    "        logging: Boolean, switches Tensorflow histogram logging on/off\n",
    "\n",
    "    # Methods\n",
    "        _call(inputs): Defines computation graph of layer\n",
    "            (i.e. takes input, returns output)\n",
    "        __call__(inputs): Wrapper for _call()\n",
    "        _log_vars(): Log all variables\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        allowed_kwargs = {'name', 'logging'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            layer = self.__class__.__name__.lower()\n",
    "            name = layer + '_' + str(get_layer_uid(layer))\n",
    "        self.name = name\n",
    "        self.vars = {}\n",
    "        logging = kwargs.get('logging', False)\n",
    "        self.logging = logging\n",
    "        self.sparse_inputs = False\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        return inputs\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.name_scope(self.name):\n",
    "            if self.logging and not self.sparse_inputs:\n",
    "                tf.summary.histogram(self.name + '/inputs', inputs)\n",
    "            outputs = self._call(inputs)\n",
    "            if self.logging:\n",
    "                tf.summary.histogram(self.name + '/outputs', outputs)\n",
    "            return outputs\n",
    "\n",
    "    def _log_vars(self):\n",
    "        for var in self.vars:\n",
    "            tf.summary.histogram(self.name + '/vars/' + var, self.vars[var])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    \"\"\"Dense layer.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, placeholders, dropout=0., sparse_inputs=False,\n",
    "                 act=tf.nn.relu, bias=False, featureless=False, **kwargs):\n",
    "        super(Dense, self).__init__(**kwargs)\n",
    "\n",
    "        if dropout:\n",
    "            self.dropout = placeholders['dropout']\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "\n",
    "        self.act = act\n",
    "        self.sparse_inputs = sparse_inputs\n",
    "        self.featureless = featureless\n",
    "        self.bias = bias\n",
    "\n",
    "        # helper variable for sparse dropout\n",
    "        self.num_features_nonzero = placeholders['num_features_nonzero']\n",
    "\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            self.vars['weights'] = glorot([input_dim, output_dim],\n",
    "                                          name='weights')\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([output_dim], name='bias')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        # dropout\n",
    "        if self.sparse_inputs:\n",
    "            x = sparse_dropout(x, 1-self.dropout, self.num_features_nonzero)\n",
    "        else:\n",
    "            x = tf.nn.dropout(x, 1-self.dropout)\n",
    "\n",
    "        # transform\n",
    "        output = dot(x, self.vars['weights'], sparse=self.sparse_inputs)\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "\n",
    "        return self.act(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Graph Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(Layer):\n",
    "    \"\"\"Graph convolution layer.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, placeholders, dropout=0.,\n",
    "                 sparse_inputs=False, act=tf.nn.relu, bias=False,\n",
    "                 featureless=False, **kwargs):\n",
    "        super(GraphConvolution, self).__init__(**kwargs)\n",
    "\n",
    "        if dropout:\n",
    "            self.dropout = placeholders['dropout']\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "\n",
    "        self.act = act\n",
    "        self.support = placeholders['support']\n",
    "        self.sparse_inputs = sparse_inputs\n",
    "        self.featureless = featureless\n",
    "        self.bias = bias\n",
    "\n",
    "        # helper variable for sparse dropout\n",
    "        self.num_features_nonzero = placeholders['num_features_nonzero']\n",
    "\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            for i in range(len(self.support)):\n",
    "                self.vars['weights_' + str(i)] = glorot([input_dim, output_dim],\n",
    "                                                        name='weights_' + str(i))\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([output_dim], name='bias')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        # dropout\n",
    "        if self.sparse_inputs:\n",
    "            x = sparse_dropout(x, 1-self.dropout, self.num_features_nonzero)\n",
    "        else:\n",
    "            x = tf.nn.dropout(x, 1-self.dropout)\n",
    "\n",
    "        # convolve\n",
    "        supports = list()\n",
    "        for i in range(len(self.support)):\n",
    "            if not self.featureless:\n",
    "                pre_sup = dot(x, self.vars['weights_' + str(i)],\n",
    "                              sparse=self.sparse_inputs)\n",
    "            else:\n",
    "                pre_sup = self.vars['weights_' + str(i)]\n",
    "            support = dot(self.support[i], pre_sup, sparse=True)\n",
    "            supports.append(support)\n",
    "        output = tf.add_n(supports)\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "\n",
    "        return self.act(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcn.metrics import *\n",
    "\n",
    "# flags = tf.compat.v1.flags\n",
    "# FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        allowed_kwargs = {'name', 'logging'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            name = self.__class__.__name__.lower()\n",
    "        self.name = name\n",
    "\n",
    "        logging = kwargs.get('logging', False)\n",
    "        self.logging = logging\n",
    "\n",
    "        self.vars = {}\n",
    "        self.placeholders = {}\n",
    "\n",
    "        self.layers = []\n",
    "        self.activations = []\n",
    "\n",
    "        self.inputs = None\n",
    "        self.outputs = None\n",
    "\n",
    "        self.loss = 0\n",
    "        self.accuracy = 0\n",
    "        self.optimizer = None\n",
    "        self.opt_op = None\n",
    "\n",
    "    def _build(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\" Wrapper for _build() \"\"\"\n",
    "        with tf.variable_scope(self.name):\n",
    "            self._build()\n",
    "\n",
    "        # Build sequential layer model\n",
    "        self.activations.append(self.inputs)\n",
    "        for layer in self.layers:\n",
    "            hidden = layer(self.activations[-1])\n",
    "            self.activations.append(hidden)\n",
    "        self.outputs = self.activations[-1]\n",
    "\n",
    "        # Store model variables for easy access\n",
    "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
    "        self.vars = {var.name: var for var in variables}\n",
    "\n",
    "        # Build metrics\n",
    "        self._loss()\n",
    "        self._accuracy()\n",
    "\n",
    "        self.opt_op = self.optimizer.minimize(self.loss)\n",
    "\n",
    "    def predict(self):\n",
    "        pass\n",
    "\n",
    "    def _loss(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _accuracy(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def save(self, sess=None):\n",
    "        if not sess:\n",
    "            raise AttributeError(\"TensorFlow session not provided.\")\n",
    "        saver = tf.train.Saver(self.vars)\n",
    "        save_path = saver.save(sess, \"tmp/%s.ckpt\" % self.name)\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "    def load(self, sess=None):\n",
    "        if not sess:\n",
    "            raise AttributeError(\"TensorFlow session not provided.\")\n",
    "        saver = tf.train.Saver(self.vars)\n",
    "        save_path = \"tmp/%s.ckpt\" % self.name\n",
    "        saver.restore(sess, save_path)\n",
    "        print(\"Model restored from file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(Model):\n",
    "    def __init__(self, placeholders, input_dim, **kwargs):\n",
    "        super(MLP, self).__init__(**kwargs)\n",
    "\n",
    "        self.inputs = placeholders['features']\n",
    "        self.input_dim = input_dim\n",
    "        # self.input_dim = self.inputs.get_shape().as_list()[1]  # To be supported in future Tensorflow versions\n",
    "        self.output_dim = placeholders['labels'].get_shape().as_list()[1]\n",
    "        self.placeholders = placeholders\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=flags_learning_rate)\n",
    "\n",
    "        self.build()\n",
    "\n",
    "    def _loss(self):\n",
    "        # Weight decay loss\n",
    "        for var in self.layers[0].vars.values():\n",
    "            self.loss += flags_weight_decay * tf.nn.l2_loss(var)\n",
    "\n",
    "        # Cross entropy error\n",
    "        self.loss += masked_softmax_cross_entropy(self.outputs, self.placeholders['labels'],\n",
    "                                                  self.placeholders['labels_mask'])\n",
    "\n",
    "    def _accuracy(self):\n",
    "        self.accuracy = masked_accuracy(self.outputs, self.placeholders['labels'],\n",
    "                                        self.placeholders['labels_mask'])\n",
    "\n",
    "    def _build(self):\n",
    "        self.layers.append(Dense(input_dim=self.input_dim,\n",
    "                                 output_dim=flags_hidden1,\n",
    "                                 placeholders=self.placeholders,\n",
    "                                 act=tf.nn.relu,\n",
    "                                 dropout=True,\n",
    "                                 sparse_inputs=True,\n",
    "                                 logging=self.logging))\n",
    "\n",
    "        self.layers.append(Dense(input_dim=flags_hidden1,\n",
    "                                 output_dim=self.output_dim,\n",
    "                                 placeholders=self.placeholders,\n",
    "                                 act=lambda x: x,\n",
    "                                 dropout=True,\n",
    "                                 logging=self.logging))\n",
    "\n",
    "    def predict(self):\n",
    "        return tf.nn.softmax(self.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(Model):\n",
    "    def __init__(self, placeholders, input_dim, **kwargs):\n",
    "        super(GCN, self).__init__(**kwargs)\n",
    "\n",
    "        self.inputs = placeholders['features']\n",
    "        self.input_dim = input_dim\n",
    "        # self.input_dim = self.inputs.get_shape().as_list()[1]  # To be supported in future Tensorflow versions\n",
    "        self.output_dim = placeholders['labels'].get_shape().as_list()[1]\n",
    "        self.placeholders = placeholders\n",
    "\n",
    "        self.optimizer = flags_optimizer(learning_rate=flags_learning_rate)\n",
    "\n",
    "        self.build()\n",
    "\n",
    "    def _loss(self):\n",
    "        # Weight decay loss\n",
    "        for var in self.layers[0].vars.values():\n",
    "            self.loss += flags_weight_decay * tf.nn.l2_loss(var)\n",
    "\n",
    "        # Cross entropy error\n",
    "        self.loss += masked_softmax_cross_entropy(self.outputs, self.placeholders['labels'],\n",
    "                                                  self.placeholders['labels_mask'])\n",
    "\n",
    "    def _accuracy(self):\n",
    "        self.accuracy = masked_accuracy(self.outputs, self.placeholders['labels'],\n",
    "                                        self.placeholders['labels_mask'])\n",
    "\n",
    "    def _build(self):\n",
    "\n",
    "#         Paper layer configuration\n",
    "        self.layers.append(GraphConvolution(input_dim=self.input_dim,\n",
    "                                            output_dim=flags_hidden1,\n",
    "                                            placeholders=self.placeholders,\n",
    "                                            act=flags_act_func,\n",
    "                                            dropout=True,\n",
    "                                            sparse_inputs=True,\n",
    "                                            logging=self.logging))\n",
    "\n",
    "        self.layers.append(GraphConvolution(input_dim=flags_hidden1,\n",
    "                                            output_dim=self.output_dim,\n",
    "                                            placeholders=self.placeholders,\n",
    "                                            act=lambda x: x,\n",
    "                                            dropout=True,\n",
    "                                            logging=self.logging))\n",
    "#         Single layer configuration\n",
    "#         self.layers.append(GraphConvolution(input_dim=self.input_dim,\n",
    "#                                             output_dim=self.output_dim,\n",
    "#                                             placeholders=self.placeholders,\n",
    "#                                             act=flags_act_func,\n",
    "#                                             dropout=True,\n",
    "#                                             sparse_inputs=True,\n",
    "#                                             logging=self.logging))\n",
    "#         Triple layer configuration\n",
    "#         self.layers.append(GraphConvolution(input_dim=self.input_dim,\n",
    "#                                             output_dim=64,\n",
    "#                                             placeholders=self.placeholders,\n",
    "#                                             act=flags_act_func,\n",
    "#                                             dropout=True,\n",
    "#                                             sparse_inputs=True,\n",
    "#                                             logging=self.logging))\n",
    "\n",
    "#         self.layers.append(GraphConvolution(input_dim=64,\n",
    "#                                             output_dim=flags_hidden1,\n",
    "#                                             placeholders=self.placeholders,\n",
    "#                                             act=flags_act_func,\n",
    "#                                             dropout=True,\n",
    "#                                             logging=self.logging))\n",
    "\n",
    "#         self.layers.append(GraphConvolution(input_dim=flags_hidden1,\n",
    "#                                             output_dim=self.output_dim,\n",
    "#                                             placeholders=self.placeholders,\n",
    "#                                             act=lambda x: x,\n",
    "#                                             dropout=True,\n",
    "#                                             logging=self.logging))\n",
    "\n",
    "    def predict(self):\n",
    "        return tf.nn.softmax(self.outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "# import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# Set random seed\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "# flags = tf.compat.v1.flags\n",
    "# FLAGS = flags.FLAGS\n",
    "flags_dataset = 'cora'# , 'Dataset string.')  # 'cora', 'citeseer', 'pubmed'\n",
    "flags_model = 'gcn' #, 'Model string.')  # 'gcn', 'gcn_cheby', 'dense'\n",
    "flags_learning_rate = 0.01 #, 'Initial learning rate.')\n",
    "flags_epochs = 200 #, 'Number of epochs to train.')\n",
    "flags_hidden1 = 16 #, 'Number of units in hidden layer 1.')\n",
    "flags_dropout = 0.5 #, 'Dropout rate (1 - keep probability).')\n",
    "flags_weight_decay = 5e-4 #, 'Weight for L2 loss on embedding matrix.')\n",
    "flags_early_stopping = 10 #, 'Tolerance for early stopping (# of epochs).')\n",
    "flags_max_degree = 3 #, 'Maximum Chebyshev polynomial degree.')\n",
    "flags_act_func = tf.nn.relu # Activation function: tf.nn.relu, tf.nn.leaky_relu, tf.nn.sigmoid, tf.nn.tanh, tf.nn.elu\n",
    "flags_optimizer = tf.train.AdamOptimizer # Optimizer: tf.train.AdamOptimizer, tf.train.GradientDescentOptimizer, tf.train.AdadeltaOptimizer, tf.train.RMSPropOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Load Data and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/goutham/anaconda3/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Users/goutham/anaconda3/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/goutham/Downloads/cs598-dl-healthcare-proj-mainzhisheng/src/gcn/utils.py:70: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
      "2023-05-06 21:01:49.513476: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder_7' with dtype int32\n",
      "\t [[{{node Placeholder_7}}]]\n",
      "2023-05-06 21:01:49.513594: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder_7' with dtype int32\n",
      "\t [[{{node Placeholder_7}}]]\n",
      "2023-05-06 21:01:49.522493: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype int64 and shape [?]\n",
      "\t [[{{node Placeholder}}]]\n",
      "2023-05-06 21:01:49.522584: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype int64 and shape [?]\n",
      "\t [[{{node Placeholder}}]]\n",
      "2023-05-06 21:01:49.533585: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype int64 and shape [?]\n",
      "\t [[{{node Placeholder}}]]\n",
      "2023-05-06 21:01:49.533688: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype int64 and shape [?]\n",
      "\t [[{{node Placeholder}}]]\n",
      "2023-05-06 21:01:49.635515: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype int64 and shape [?]\n",
      "\t [[{{node Placeholder}}]]\n",
      "2023-05-06 21:01:49.635633: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype int64 and shape [?]\n",
      "\t [[{{node Placeholder}}]]\n",
      "2023-05-06 21:01:49.668784: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype int64 and shape [?]\n",
      "\t [[{{node Placeholder}}]]\n",
      "2023-05-06 21:01:49.668882: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype int64 and shape [?]\n",
      "\t [[{{node Placeholder}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "Epoch: 0001 train_loss= 1.95398 train_acc= 0.14286 val_loss= 1.94916 val_acc= 0.19200 time= 0.13065\n",
      "Epoch: 0002 train_loss= 1.94717 train_acc= 0.24286 val_loss= 1.94463 val_acc= 0.25400 time= 0.01129\n",
      "Epoch: 0003 train_loss= 1.93999 train_acc= 0.40000 val_loss= 1.93989 val_acc= 0.36000 time= 0.01132\n",
      "Epoch: 0004 train_loss= 1.92993 train_acc= 0.60714 val_loss= 1.93522 val_acc= 0.38400 time= 0.01166\n",
      "Epoch: 0005 train_loss= 1.92239 train_acc= 0.62143 val_loss= 1.93089 val_acc= 0.38400 time= 0.01158\n",
      "Epoch: 0006 train_loss= 1.91587 train_acc= 0.62143 val_loss= 1.92688 val_acc= 0.38800 time= 0.01148\n",
      "Epoch: 0007 train_loss= 1.90422 train_acc= 0.65714 val_loss= 1.92295 val_acc= 0.38800 time= 0.01170\n",
      "Epoch: 0008 train_loss= 1.90035 train_acc= 0.60714 val_loss= 1.91907 val_acc= 0.37200 time= 0.01237\n",
      "Epoch: 0009 train_loss= 1.88918 train_acc= 0.61429 val_loss= 1.91513 val_acc= 0.37200 time= 0.01292\n",
      "Epoch: 0010 train_loss= 1.88187 train_acc= 0.62143 val_loss= 1.91109 val_acc= 0.36800 time= 0.01355\n",
      "Epoch: 0011 train_loss= 1.87274 train_acc= 0.64286 val_loss= 1.90697 val_acc= 0.36800 time= 0.01563\n",
      "Epoch: 0012 train_loss= 1.86119 train_acc= 0.64286 val_loss= 1.90276 val_acc= 0.37000 time= 0.01396\n",
      "Epoch: 0013 train_loss= 1.85539 train_acc= 0.61429 val_loss= 1.89846 val_acc= 0.36400 time= 0.01421\n",
      "Epoch: 0014 train_loss= 1.84110 train_acc= 0.72857 val_loss= 1.89412 val_acc= 0.36200 time= 0.01284\n",
      "Epoch: 0015 train_loss= 1.83454 train_acc= 0.62143 val_loss= 1.88983 val_acc= 0.37800 time= 0.01262\n",
      "Epoch: 0016 train_loss= 1.82084 train_acc= 0.70714 val_loss= 1.88550 val_acc= 0.39200 time= 0.01266\n",
      "Epoch: 0017 train_loss= 1.81845 train_acc= 0.64286 val_loss= 1.88113 val_acc= 0.40000 time= 0.01577\n",
      "Epoch: 0018 train_loss= 1.80250 train_acc= 0.68571 val_loss= 1.87666 val_acc= 0.40200 time= 0.01776\n",
      "Epoch: 0019 train_loss= 1.78322 train_acc= 0.72143 val_loss= 1.87212 val_acc= 0.40600 time= 0.01725\n",
      "Epoch: 0020 train_loss= 1.77089 train_acc= 0.70714 val_loss= 1.86747 val_acc= 0.40800 time= 0.01979\n",
      "Epoch: 0021 train_loss= 1.75951 train_acc= 0.74286 val_loss= 1.86282 val_acc= 0.41200 time= 0.02055\n",
      "Epoch: 0022 train_loss= 1.74195 train_acc= 0.68571 val_loss= 1.85822 val_acc= 0.41600 time= 0.02115\n",
      "Epoch: 0023 train_loss= 1.74272 train_acc= 0.68571 val_loss= 1.85360 val_acc= 0.42400 time= 0.02791\n",
      "Epoch: 0024 train_loss= 1.73830 train_acc= 0.68571 val_loss= 1.84884 val_acc= 0.43000 time= 0.01745\n",
      "Epoch: 0025 train_loss= 1.72337 train_acc= 0.70000 val_loss= 1.84389 val_acc= 0.43400 time= 0.01688\n",
      "Epoch: 0026 train_loss= 1.72588 train_acc= 0.69286 val_loss= 1.83870 val_acc= 0.44600 time= 0.01774\n",
      "Epoch: 0027 train_loss= 1.68546 train_acc= 0.77143 val_loss= 1.83333 val_acc= 0.45000 time= 0.01462\n",
      "Epoch: 0028 train_loss= 1.68721 train_acc= 0.68571 val_loss= 1.82794 val_acc= 0.46200 time= 0.01397\n",
      "Epoch: 0029 train_loss= 1.66977 train_acc= 0.75000 val_loss= 1.82235 val_acc= 0.46400 time= 0.01392\n",
      "Epoch: 0030 train_loss= 1.63515 train_acc= 0.77143 val_loss= 1.81673 val_acc= 0.47000 time= 0.01349\n",
      "Epoch: 0031 train_loss= 1.64691 train_acc= 0.76429 val_loss= 1.81108 val_acc= 0.48400 time= 0.01378\n",
      "Epoch: 0032 train_loss= 1.63115 train_acc= 0.73571 val_loss= 1.80523 val_acc= 0.49800 time= 0.01397\n",
      "Epoch: 0033 train_loss= 1.61448 train_acc= 0.76429 val_loss= 1.79926 val_acc= 0.50800 time= 0.01363\n",
      "Epoch: 0034 train_loss= 1.60044 train_acc= 0.82143 val_loss= 1.79301 val_acc= 0.51400 time= 0.01318\n",
      "Epoch: 0035 train_loss= 1.57955 train_acc= 0.80000 val_loss= 1.78670 val_acc= 0.52400 time= 0.01386\n",
      "Epoch: 0036 train_loss= 1.58841 train_acc= 0.78571 val_loss= 1.78011 val_acc= 0.53400 time= 0.01583\n",
      "Epoch: 0037 train_loss= 1.54710 train_acc= 0.80714 val_loss= 1.77347 val_acc= 0.54400 time= 0.02146\n",
      "Epoch: 0038 train_loss= 1.55767 train_acc= 0.81429 val_loss= 1.76672 val_acc= 0.55600 time= 0.01673\n",
      "Epoch: 0039 train_loss= 1.51676 train_acc= 0.82857 val_loss= 1.76003 val_acc= 0.56800 time= 0.01612\n",
      "Epoch: 0040 train_loss= 1.52725 train_acc= 0.75714 val_loss= 1.75313 val_acc= 0.58400 time= 0.01432\n",
      "Epoch: 0041 train_loss= 1.48726 train_acc= 0.87857 val_loss= 1.74616 val_acc= 0.59000 time= 0.01392\n",
      "Epoch: 0042 train_loss= 1.50069 train_acc= 0.83571 val_loss= 1.73898 val_acc= 0.59800 time= 0.01384\n",
      "Epoch: 0043 train_loss= 1.44731 train_acc= 0.83571 val_loss= 1.73153 val_acc= 0.61600 time= 0.01366\n",
      "Epoch: 0044 train_loss= 1.43205 train_acc= 0.84286 val_loss= 1.72379 val_acc= 0.64400 time= 0.01391\n",
      "Epoch: 0045 train_loss= 1.46252 train_acc= 0.85714 val_loss= 1.71587 val_acc= 0.65600 time= 0.01386\n",
      "Epoch: 0046 train_loss= 1.44155 train_acc= 0.85000 val_loss= 1.70774 val_acc= 0.67000 time= 0.01359\n",
      "Epoch: 0047 train_loss= 1.40873 train_acc= 0.86429 val_loss= 1.69973 val_acc= 0.68000 time= 0.01355\n",
      "Epoch: 0048 train_loss= 1.39569 train_acc= 0.85714 val_loss= 1.69173 val_acc= 0.68200 time= 0.01282\n",
      "Epoch: 0049 train_loss= 1.38352 train_acc= 0.88571 val_loss= 1.68383 val_acc= 0.68800 time= 0.01314\n",
      "Epoch: 0050 train_loss= 1.37835 train_acc= 0.85000 val_loss= 1.67588 val_acc= 0.68800 time= 0.01342\n",
      "Epoch: 0051 train_loss= 1.34691 train_acc= 0.87143 val_loss= 1.66812 val_acc= 0.69600 time= 0.01380\n",
      "Epoch: 0052 train_loss= 1.31211 train_acc= 0.91429 val_loss= 1.66018 val_acc= 0.70400 time= 0.01720\n",
      "Epoch: 0053 train_loss= 1.33207 train_acc= 0.90000 val_loss= 1.65215 val_acc= 0.71400 time= 0.01906\n",
      "Epoch: 0054 train_loss= 1.26994 train_acc= 0.88571 val_loss= 1.64410 val_acc= 0.71600 time= 0.01828\n",
      "Epoch: 0055 train_loss= 1.30618 train_acc= 0.89286 val_loss= 1.63633 val_acc= 0.71800 time= 0.01652\n",
      "Epoch: 0056 train_loss= 1.26758 train_acc= 0.90714 val_loss= 1.62807 val_acc= 0.72200 time= 0.01447\n",
      "Epoch: 0057 train_loss= 1.24267 train_acc= 0.89286 val_loss= 1.61949 val_acc= 0.72400 time= 0.01379\n",
      "Epoch: 0058 train_loss= 1.25285 train_acc= 0.89286 val_loss= 1.61089 val_acc= 0.73000 time= 0.01403\n",
      "Epoch: 0059 train_loss= 1.27309 train_acc= 0.91429 val_loss= 1.60223 val_acc= 0.73800 time= 0.01325\n",
      "Epoch: 0060 train_loss= 1.23682 train_acc= 0.91429 val_loss= 1.59340 val_acc= 0.73600 time= 0.01521\n",
      "Epoch: 0061 train_loss= 1.19454 train_acc= 0.91429 val_loss= 1.58448 val_acc= 0.73600 time= 0.01282\n",
      "Epoch: 0062 train_loss= 1.17074 train_acc= 0.92143 val_loss= 1.57549 val_acc= 0.73800 time= 0.01252\n",
      "Epoch: 0063 train_loss= 1.16221 train_acc= 0.93571 val_loss= 1.56648 val_acc= 0.74200 time= 0.01246\n",
      "Epoch: 0064 train_loss= 1.15121 train_acc= 0.92143 val_loss= 1.55735 val_acc= 0.74800 time= 0.01347\n",
      "Epoch: 0065 train_loss= 1.16159 train_acc= 0.88571 val_loss= 1.54846 val_acc= 0.75400 time= 0.01229\n",
      "Epoch: 0066 train_loss= 1.18634 train_acc= 0.91429 val_loss= 1.53992 val_acc= 0.75400 time= 0.01331\n",
      "Epoch: 0067 train_loss= 1.12456 train_acc= 0.90000 val_loss= 1.53127 val_acc= 0.75600 time= 0.01153\n",
      "Epoch: 0068 train_loss= 1.12594 train_acc= 0.93571 val_loss= 1.52269 val_acc= 0.76000 time= 0.01142\n",
      "Epoch: 0069 train_loss= 1.12804 train_acc= 0.87143 val_loss= 1.51446 val_acc= 0.76000 time= 0.01165\n",
      "Epoch: 0070 train_loss= 1.07804 train_acc= 0.92857 val_loss= 1.50658 val_acc= 0.76000 time= 0.01149\n",
      "Epoch: 0071 train_loss= 1.05328 train_acc= 0.95000 val_loss= 1.49881 val_acc= 0.76200 time= 0.01132\n",
      "Epoch: 0072 train_loss= 1.09813 train_acc= 0.93571 val_loss= 1.49116 val_acc= 0.76400 time= 0.01115\n",
      "Epoch: 0073 train_loss= 1.02339 train_acc= 0.94286 val_loss= 1.48338 val_acc= 0.76000 time= 0.01111\n",
      "Epoch: 0074 train_loss= 1.02378 train_acc= 0.95714 val_loss= 1.47552 val_acc= 0.75800 time= 0.01114\n",
      "Epoch: 0075 train_loss= 1.01290 train_acc= 0.95000 val_loss= 1.46789 val_acc= 0.76000 time= 0.01121\n",
      "Epoch: 0076 train_loss= 1.06556 train_acc= 0.89286 val_loss= 1.46047 val_acc= 0.75800 time= 0.01130\n",
      "Epoch: 0077 train_loss= 1.01221 train_acc= 0.91429 val_loss= 1.45304 val_acc= 0.76200 time= 0.01158\n",
      "Epoch: 0078 train_loss= 1.00247 train_acc= 0.95000 val_loss= 1.44549 val_acc= 0.76200 time= 0.01112\n",
      "Epoch: 0079 train_loss= 1.04554 train_acc= 0.92857 val_loss= 1.43810 val_acc= 0.76200 time= 0.01116\n",
      "Epoch: 0080 train_loss= 1.04430 train_acc= 0.91429 val_loss= 1.43045 val_acc= 0.76200 time= 0.01147\n",
      "Epoch: 0081 train_loss= 0.97607 train_acc= 0.97143 val_loss= 1.42323 val_acc= 0.76400 time= 0.01148\n",
      "Epoch: 0082 train_loss= 0.99655 train_acc= 0.92143 val_loss= 1.41607 val_acc= 0.76600 time= 0.01149\n",
      "Epoch: 0083 train_loss= 0.97819 train_acc= 0.91429 val_loss= 1.40884 val_acc= 0.77200 time= 0.01204\n",
      "Epoch: 0084 train_loss= 0.96876 train_acc= 0.95000 val_loss= 1.40193 val_acc= 0.77600 time= 0.01174\n",
      "Epoch: 0085 train_loss= 0.93855 train_acc= 0.94286 val_loss= 1.39511 val_acc= 0.78200 time= 0.01268\n",
      "Epoch: 0086 train_loss= 0.96155 train_acc= 0.92857 val_loss= 1.38860 val_acc= 0.78200 time= 0.01178\n",
      "Epoch: 0087 train_loss= 0.96553 train_acc= 0.95714 val_loss= 1.38206 val_acc= 0.78200 time= 0.01194\n",
      "Epoch: 0088 train_loss= 0.93760 train_acc= 0.92143 val_loss= 1.37563 val_acc= 0.78200 time= 0.01180\n",
      "Epoch: 0089 train_loss= 0.91658 train_acc= 0.94286 val_loss= 1.36967 val_acc= 0.78200 time= 0.01236\n",
      "Epoch: 0090 train_loss= 0.94048 train_acc= 0.94286 val_loss= 1.36404 val_acc= 0.78200 time= 0.01173\n",
      "Epoch: 0091 train_loss= 0.89515 train_acc= 0.95714 val_loss= 1.35863 val_acc= 0.78200 time= 0.01158\n",
      "Epoch: 0092 train_loss= 0.93473 train_acc= 0.94286 val_loss= 1.35296 val_acc= 0.78400 time= 0.01133\n",
      "Epoch: 0093 train_loss= 0.90961 train_acc= 0.94286 val_loss= 1.34768 val_acc= 0.78200 time= 0.01140\n",
      "Epoch: 0094 train_loss= 0.91874 train_acc= 0.91429 val_loss= 1.34313 val_acc= 0.78200 time= 0.01126\n",
      "Epoch: 0095 train_loss= 0.93025 train_acc= 0.91429 val_loss= 1.33903 val_acc= 0.78200 time= 0.01125\n",
      "Epoch: 0096 train_loss= 0.92147 train_acc= 0.95000 val_loss= 1.33505 val_acc= 0.78400 time= 0.01101\n",
      "Epoch: 0097 train_loss= 0.87581 train_acc= 0.93571 val_loss= 1.33095 val_acc= 0.78400 time= 0.01099\n",
      "Epoch: 0098 train_loss= 0.89958 train_acc= 0.96429 val_loss= 1.32702 val_acc= 0.78600 time= 0.01112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0099 train_loss= 0.89164 train_acc= 0.95000 val_loss= 1.32282 val_acc= 0.78600 time= 0.01169\n",
      "Epoch: 0100 train_loss= 0.83943 train_acc= 0.97143 val_loss= 1.31836 val_acc= 0.78400 time= 0.01159\n",
      "Epoch: 0101 train_loss= 0.89123 train_acc= 0.92857 val_loss= 1.31348 val_acc= 0.78000 time= 0.01207\n",
      "Epoch: 0102 train_loss= 0.84613 train_acc= 0.92857 val_loss= 1.30786 val_acc= 0.78200 time= 0.01316\n",
      "Epoch: 0103 train_loss= 0.85106 train_acc= 0.94286 val_loss= 1.30189 val_acc= 0.78200 time= 0.01274\n",
      "Epoch: 0104 train_loss= 0.83774 train_acc= 0.92857 val_loss= 1.29585 val_acc= 0.78600 time= 0.01252\n",
      "Epoch: 0105 train_loss= 0.82232 train_acc= 0.96429 val_loss= 1.28971 val_acc= 0.78600 time= 0.01130\n",
      "Epoch: 0106 train_loss= 0.84253 train_acc= 0.95714 val_loss= 1.28336 val_acc= 0.78600 time= 0.01124\n",
      "Epoch: 0107 train_loss= 0.83385 train_acc= 0.95000 val_loss= 1.27727 val_acc= 0.78600 time= 0.01170\n",
      "Epoch: 0108 train_loss= 0.83776 train_acc= 0.96429 val_loss= 1.27111 val_acc= 0.78600 time= 0.01130\n",
      "Epoch: 0109 train_loss= 0.83914 train_acc= 0.95000 val_loss= 1.26563 val_acc= 0.79000 time= 0.01131\n",
      "Epoch: 0110 train_loss= 0.81960 train_acc= 0.95000 val_loss= 1.26053 val_acc= 0.79000 time= 0.01132\n",
      "Epoch: 0111 train_loss= 0.80221 train_acc= 0.94286 val_loss= 1.25579 val_acc= 0.79000 time= 0.01133\n",
      "Epoch: 0112 train_loss= 0.84440 train_acc= 0.93571 val_loss= 1.25109 val_acc= 0.79000 time= 0.01115\n",
      "Epoch: 0113 train_loss= 0.82257 train_acc= 0.95714 val_loss= 1.24688 val_acc= 0.79000 time= 0.01131\n",
      "Epoch: 0114 train_loss= 0.81499 train_acc= 0.94286 val_loss= 1.24287 val_acc= 0.79000 time= 0.01149\n",
      "Epoch: 0115 train_loss= 0.83107 train_acc= 0.94286 val_loss= 1.23924 val_acc= 0.79000 time= 0.01178\n",
      "Epoch: 0116 train_loss= 0.80484 train_acc= 0.92857 val_loss= 1.23566 val_acc= 0.78800 time= 0.01115\n",
      "Epoch: 0117 train_loss= 0.78614 train_acc= 0.95714 val_loss= 1.23240 val_acc= 0.78600 time= 0.01233\n",
      "Epoch: 0118 train_loss= 0.77831 train_acc= 0.94286 val_loss= 1.22966 val_acc= 0.78600 time= 0.01416\n",
      "Epoch: 0119 train_loss= 0.77073 train_acc= 0.95000 val_loss= 1.22724 val_acc= 0.78800 time= 0.01399\n",
      "Epoch: 0120 train_loss= 0.77642 train_acc= 0.96429 val_loss= 1.22486 val_acc= 0.78800 time= 0.01344\n",
      "Epoch: 0121 train_loss= 0.76526 train_acc= 0.93571 val_loss= 1.22220 val_acc= 0.78800 time= 0.01314\n",
      "Epoch: 0122 train_loss= 0.75809 train_acc= 0.96429 val_loss= 1.21980 val_acc= 0.78800 time= 0.01322\n",
      "Epoch: 0123 train_loss= 0.74995 train_acc= 0.97143 val_loss= 1.21724 val_acc= 0.78800 time= 0.01323\n",
      "Epoch: 0124 train_loss= 0.77459 train_acc= 0.93571 val_loss= 1.21434 val_acc= 0.79000 time= 0.01311\n",
      "Epoch: 0125 train_loss= 0.77623 train_acc= 0.97143 val_loss= 1.21180 val_acc= 0.79000 time= 0.01317\n",
      "Epoch: 0126 train_loss= 0.72542 train_acc= 0.95714 val_loss= 1.20932 val_acc= 0.79200 time= 0.01268\n",
      "Epoch: 0127 train_loss= 0.74450 train_acc= 0.93571 val_loss= 1.20658 val_acc= 0.79200 time= 0.01143\n",
      "Epoch: 0128 train_loss= 0.73371 train_acc= 0.97857 val_loss= 1.20363 val_acc= 0.79200 time= 0.01141\n",
      "Epoch: 0129 train_loss= 0.71551 train_acc= 0.96429 val_loss= 1.20069 val_acc= 0.79200 time= 0.01211\n",
      "Epoch: 0130 train_loss= 0.77533 train_acc= 0.94286 val_loss= 1.19739 val_acc= 0.79200 time= 0.01342\n",
      "Epoch: 0131 train_loss= 0.72124 train_acc= 0.96429 val_loss= 1.19431 val_acc= 0.79200 time= 0.01162\n",
      "Epoch: 0132 train_loss= 0.78295 train_acc= 0.95000 val_loss= 1.19175 val_acc= 0.79200 time= 0.01148\n",
      "Epoch: 0133 train_loss= 0.71783 train_acc= 0.95000 val_loss= 1.18917 val_acc= 0.79000 time= 0.01233\n",
      "Epoch: 0134 train_loss= 0.76042 train_acc= 0.95000 val_loss= 1.18628 val_acc= 0.79200 time= 0.01398\n",
      "Epoch: 0135 train_loss= 0.69706 train_acc= 0.97143 val_loss= 1.18309 val_acc= 0.79000 time= 0.01228\n",
      "Epoch: 0136 train_loss= 0.74125 train_acc= 0.95714 val_loss= 1.18018 val_acc= 0.79000 time= 0.01201\n",
      "Epoch: 0137 train_loss= 0.74695 train_acc= 0.95714 val_loss= 1.17701 val_acc= 0.79000 time= 0.01252\n",
      "Epoch: 0138 train_loss= 0.70791 train_acc= 0.94286 val_loss= 1.17432 val_acc= 0.78800 time= 0.01303\n",
      "Epoch: 0139 train_loss= 0.73143 train_acc= 0.94286 val_loss= 1.17187 val_acc= 0.79000 time= 0.01359\n",
      "Epoch: 0140 train_loss= 0.69934 train_acc= 0.97143 val_loss= 1.16926 val_acc= 0.78800 time= 0.01401\n",
      "Epoch: 0141 train_loss= 0.71071 train_acc= 0.94286 val_loss= 1.16684 val_acc= 0.78800 time= 0.01298\n",
      "Epoch: 0142 train_loss= 0.67117 train_acc= 0.96429 val_loss= 1.16472 val_acc= 0.78800 time= 0.01271\n",
      "Epoch: 0143 train_loss= 0.67728 train_acc= 0.97143 val_loss= 1.16291 val_acc= 0.78800 time= 0.01258\n",
      "Epoch: 0144 train_loss= 0.72865 train_acc= 0.95714 val_loss= 1.16087 val_acc= 0.78800 time= 0.01236\n",
      "Epoch: 0145 train_loss= 0.70414 train_acc= 0.95714 val_loss= 1.15891 val_acc= 0.78400 time= 0.01218\n",
      "Epoch: 0146 train_loss= 0.71027 train_acc= 0.95000 val_loss= 1.15753 val_acc= 0.78400 time= 0.01191\n",
      "Epoch: 0147 train_loss= 0.70071 train_acc= 0.97857 val_loss= 1.15546 val_acc= 0.78600 time= 0.01188\n",
      "Epoch: 0148 train_loss= 0.70648 train_acc= 0.94286 val_loss= 1.15337 val_acc= 0.78600 time= 0.01167\n",
      "Epoch: 0149 train_loss= 0.66662 train_acc= 0.97143 val_loss= 1.15110 val_acc= 0.78600 time= 0.01272\n",
      "Epoch: 0150 train_loss= 0.68066 train_acc= 0.99286 val_loss= 1.14923 val_acc= 0.78600 time= 0.01818\n",
      "Epoch: 0151 train_loss= 0.70275 train_acc= 0.96429 val_loss= 1.14772 val_acc= 0.78600 time= 0.01364\n",
      "Epoch: 0152 train_loss= 0.70096 train_acc= 0.95714 val_loss= 1.14628 val_acc= 0.78600 time= 0.01570\n",
      "Epoch: 0153 train_loss= 0.68893 train_acc= 0.95714 val_loss= 1.14501 val_acc= 0.78600 time= 0.01354\n",
      "Epoch: 0154 train_loss= 0.68354 train_acc= 0.96429 val_loss= 1.14304 val_acc= 0.78600 time= 0.01332\n",
      "Epoch: 0155 train_loss= 0.67374 train_acc= 0.97143 val_loss= 1.14123 val_acc= 0.78600 time= 0.01318\n",
      "Epoch: 0156 train_loss= 0.63494 train_acc= 0.99286 val_loss= 1.13956 val_acc= 0.78800 time= 0.01340\n",
      "Epoch: 0157 train_loss= 0.65656 train_acc= 0.96429 val_loss= 1.13783 val_acc= 0.79000 time= 0.01178\n",
      "Epoch: 0158 train_loss= 0.65648 train_acc= 0.95714 val_loss= 1.13580 val_acc= 0.79200 time= 0.01154\n",
      "Epoch: 0159 train_loss= 0.71239 train_acc= 0.93571 val_loss= 1.13380 val_acc= 0.79200 time= 0.01144\n",
      "Epoch: 0160 train_loss= 0.68017 train_acc= 0.94286 val_loss= 1.13143 val_acc= 0.79200 time= 0.01167\n",
      "Epoch: 0161 train_loss= 0.67913 train_acc= 0.95714 val_loss= 1.12902 val_acc= 0.79200 time= 0.01159\n",
      "Epoch: 0162 train_loss= 0.68679 train_acc= 0.92857 val_loss= 1.12701 val_acc= 0.79000 time= 0.01198\n",
      "Epoch: 0163 train_loss= 0.68635 train_acc= 0.95714 val_loss= 1.12556 val_acc= 0.79000 time= 0.01244\n",
      "Epoch: 0164 train_loss= 0.63512 train_acc= 0.98571 val_loss= 1.12359 val_acc= 0.78800 time= 0.01384\n",
      "Epoch: 0165 train_loss= 0.67694 train_acc= 0.95000 val_loss= 1.12229 val_acc= 0.78800 time= 0.01425\n",
      "Epoch: 0166 train_loss= 0.67551 train_acc= 0.96429 val_loss= 1.12105 val_acc= 0.78400 time= 0.01488\n",
      "Epoch: 0167 train_loss= 0.69947 train_acc= 0.95000 val_loss= 1.11963 val_acc= 0.78400 time= 0.01312\n",
      "Epoch: 0168 train_loss= 0.66728 train_acc= 0.97143 val_loss= 1.11815 val_acc= 0.78400 time= 0.01266\n",
      "Epoch: 0169 train_loss= 0.66971 train_acc= 0.97143 val_loss= 1.11681 val_acc= 0.78400 time= 0.01192\n",
      "Epoch: 0170 train_loss= 0.62243 train_acc= 0.98571 val_loss= 1.11476 val_acc= 0.78400 time= 0.01187\n",
      "Epoch: 0171 train_loss= 0.66034 train_acc= 0.96429 val_loss= 1.11329 val_acc= 0.78400 time= 0.01368\n",
      "Epoch: 0172 train_loss= 0.70384 train_acc= 0.95000 val_loss= 1.11188 val_acc= 0.78400 time= 0.01346\n",
      "Epoch: 0173 train_loss= 0.63969 train_acc= 0.97143 val_loss= 1.10982 val_acc= 0.78400 time= 0.01208\n",
      "Epoch: 0174 train_loss= 0.68714 train_acc= 0.95714 val_loss= 1.10740 val_acc= 0.78800 time= 0.01192\n",
      "Epoch: 0175 train_loss= 0.68592 train_acc= 0.96429 val_loss= 1.10473 val_acc= 0.78800 time= 0.01160\n",
      "Epoch: 0176 train_loss= 0.64401 train_acc= 0.97143 val_loss= 1.10238 val_acc= 0.78800 time= 0.01315\n",
      "Epoch: 0177 train_loss= 0.65554 train_acc= 0.96429 val_loss= 1.10005 val_acc= 0.79200 time= 0.01491\n",
      "Epoch: 0178 train_loss= 0.64684 train_acc= 0.94286 val_loss= 1.09770 val_acc= 0.79200 time= 0.01900\n",
      "Epoch: 0179 train_loss= 0.62039 train_acc= 0.94286 val_loss= 1.09502 val_acc= 0.79200 time= 0.01203\n",
      "Epoch: 0180 train_loss= 0.64569 train_acc= 0.93571 val_loss= 1.09291 val_acc= 0.79200 time= 0.01162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0181 train_loss= 0.62059 train_acc= 0.95000 val_loss= 1.09067 val_acc= 0.79200 time= 0.03352\n",
      "Epoch: 0182 train_loss= 0.62461 train_acc= 0.96429 val_loss= 1.08900 val_acc= 0.79000 time= 0.06612\n",
      "Epoch: 0183 train_loss= 0.62334 train_acc= 0.96429 val_loss= 1.08723 val_acc= 0.79000 time= 0.04023\n",
      "Epoch: 0184 train_loss= 0.65683 train_acc= 0.95000 val_loss= 1.08566 val_acc= 0.79000 time= 0.01302\n",
      "Epoch: 0185 train_loss= 0.66517 train_acc= 0.96429 val_loss= 1.08404 val_acc= 0.79000 time= 0.01589\n",
      "Epoch: 0186 train_loss= 0.68030 train_acc= 0.94286 val_loss= 1.08288 val_acc= 0.79000 time= 0.02886\n",
      "Epoch: 0187 train_loss= 0.60279 train_acc= 0.95000 val_loss= 1.08196 val_acc= 0.79000 time= 0.01805\n",
      "Epoch: 0188 train_loss= 0.60231 train_acc= 0.97857 val_loss= 1.08095 val_acc= 0.79000 time= 0.01425\n",
      "Epoch: 0189 train_loss= 0.61390 train_acc= 0.96429 val_loss= 1.07952 val_acc= 0.79000 time= 0.01721\n",
      "Epoch: 0190 train_loss= 0.59581 train_acc= 0.96429 val_loss= 1.07784 val_acc= 0.79000 time= 0.02521\n",
      "Epoch: 0191 train_loss= 0.60097 train_acc= 0.97857 val_loss= 1.07597 val_acc= 0.79200 time= 0.01343\n",
      "Epoch: 0192 train_loss= 0.61073 train_acc= 0.90714 val_loss= 1.07484 val_acc= 0.79400 time= 0.01315\n",
      "Epoch: 0193 train_loss= 0.61191 train_acc= 0.95714 val_loss= 1.07386 val_acc= 0.79600 time= 0.01276\n",
      "Epoch: 0194 train_loss= 0.63342 train_acc= 0.95714 val_loss= 1.07295 val_acc= 0.79600 time= 0.01228\n",
      "Epoch: 0195 train_loss= 0.67858 train_acc= 0.92857 val_loss= 1.07154 val_acc= 0.79600 time= 0.01261\n",
      "Epoch: 0196 train_loss= 0.60298 train_acc= 0.97857 val_loss= 1.06956 val_acc= 0.79400 time= 0.01214\n",
      "Epoch: 0197 train_loss= 0.64595 train_acc= 0.95714 val_loss= 1.06777 val_acc= 0.79200 time= 0.01299\n",
      "Epoch: 0198 train_loss= 0.62150 train_acc= 0.97143 val_loss= 1.06582 val_acc= 0.79400 time= 0.01260\n",
      "Epoch: 0199 train_loss= 0.62767 train_acc= 0.94286 val_loss= 1.06380 val_acc= 0.79400 time= 0.01197\n",
      "Epoch: 0200 train_loss= 0.60381 train_acc= 0.93571 val_loss= 1.06281 val_acc= 0.79400 time= 0.01132\n",
      "Optimization Finished!\n",
      "total train time 2.92232\n",
      "Test set results: cost= 1.01652 accuracy= 0.81800 time= 0.00585\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(flags_dataset)\n",
    "\n",
    "# Some preprocessing\n",
    "features = preprocess_features(features)\n",
    "if flags_model == 'gcn':\n",
    "    support = [preprocess_adj(adj)]\n",
    "    num_supports = 1\n",
    "    model_func = GCN\n",
    "elif flags_model == 'gcn_cheby':\n",
    "    support = chebyshev_polynomials(adj, flags_max_degree)\n",
    "    num_supports = 1 + flags_max_degree\n",
    "    model_func = GCN\n",
    "elif flags_model == 'dense':\n",
    "    support = [preprocess_adj(adj)]  # Not used\n",
    "    num_supports = 1\n",
    "    model_func = MLP\n",
    "else:\n",
    "    raise ValueError('Invalid argument for model: ' + str(flags_model))\n",
    "\n",
    "# Define placeholders\n",
    "placeholders = {\n",
    "    'support': [tf.sparse_placeholder(tf.float32) for _ in range(num_supports)],\n",
    "    'features': tf.sparse_placeholder(tf.float32, shape=tf.constant(features[2], dtype=tf.int64)),\n",
    "    'labels': tf.placeholder(tf.float32, shape=(None, y_train.shape[1])),\n",
    "    'labels_mask': tf.placeholder(tf.int32),\n",
    "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "    'num_features_nonzero': tf.placeholder(tf.int32)  # helper variable for sparse dropout\n",
    "}\n",
    "\n",
    "# Create model\n",
    "model = model_func(placeholders, input_dim=features[2][1], logging=True)\n",
    "\n",
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "\n",
    "\n",
    "# Define model evaluation function\n",
    "def evaluate(features, support, labels, mask, placeholders):\n",
    "    t_test = time.time()\n",
    "    feed_dict_val = construct_feed_dict(features, support, labels, mask, placeholders)\n",
    "    outs_val = sess.run([model.loss, model.accuracy], feed_dict=feed_dict_val)\n",
    "    return outs_val[0], outs_val[1], (time.time() - t_test)\n",
    "\n",
    "\n",
    "# Init variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "cost_val = []\n",
    "\n",
    "t_begin = time.time()\n",
    "print(\"start training...\")\n",
    "# Train model\n",
    "for epoch in range(flags_epochs):\n",
    "\n",
    "    t = time.time()\n",
    "    # Construct feed dictionary\n",
    "    feed_dict = construct_feed_dict(features, support, y_train, train_mask, placeholders)\n",
    "    feed_dict.update({placeholders['dropout']: flags_dropout})\n",
    "\n",
    "    # Training step\n",
    "    outs = sess.run([model.opt_op, model.loss, model.accuracy], feed_dict=feed_dict)\n",
    "\n",
    "    # Validation\n",
    "    cost, acc, duration = evaluate(features, support, y_val, val_mask, placeholders)\n",
    "    cost_val.append(cost)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(outs[1]),\n",
    "          \"train_acc=\", \"{:.5f}\".format(outs[2]), \"val_loss=\", \"{:.5f}\".format(cost),\n",
    "          \"val_acc=\", \"{:.5f}\".format(acc), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "    if epoch > flags_early_stopping and cost_val[-1] > np.mean(cost_val[-(flags_early_stopping+1):-1]):\n",
    "        print(\"Early stopping...\")\n",
    "        break\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "print(\"total train time {:.5f}\".format(time.time() - t_begin))\n",
    "\n",
    "# Testing\n",
    "test_cost, test_acc, test_duration = evaluate(features, support, y_test, test_mask, placeholders)\n",
    "print(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost),\n",
    "      \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(test_duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
